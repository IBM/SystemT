{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SystemT and AQL model development SystemT provides: - AQL (Annotation Query Language): a declarative rule language for expressing transparent, explainable NLP algorithms on top of NLP Primitives and Operators. AQL enables visual tooling for rule-based NLP pipeline, and provides feature extraction for Machine Learning models. - A compiler, optimizer and runtime engine that executes AQL models. SystemT was originally developed in IBM Research - Almaden and is now owned and maintained by a team with 15+ years of experience in NLP. Features AQL (Annotation Query Language) AQL is a declarative rule language, on top of NLP Primitives (e.g., tokens, parts of speech, shallow semantic parsing) and NLP Operators (e.g., dictionaries, regular expressions, span operators, etc.). AQL is a pure declarative language: it separates semantics (the \u201cwhat\u201d) from execution strategy (the \u201chow\u201d). This enables an entire array of automatic optimizations. SystemT Optimizer The Optimizer compiles AQL code to an optimized algebraic execution plan. The design of the SystemT optimizer is similar to SQL optimizers in Relational Database Systems, but with text-centric optimizations. Example optimizations include Shared Dictionary Matching, Shared Regex Matching, Regex Strength Reduction, Conditional Evaluation, and many others. These optimizations achieve orders of magnitude speed-up compared to hand-tuned execution plans. They enable NLP engineers focus on the semantics of the NLP model, without worrying about hand-tuning the execution. SystemT Runtime The SystemT runtime engine is a light-weight, high speed, low memory footprint runtime engine for executing compiled AQL code. Getting Started 1. Add SystemT to Your Project Learning AQL 10 Minutes to SystemT (Java API)","title":"Home"},{"location":"#systemt-and-aql-model-development","text":"SystemT provides: - AQL (Annotation Query Language): a declarative rule language for expressing transparent, explainable NLP algorithms on top of NLP Primitives and Operators. AQL enables visual tooling for rule-based NLP pipeline, and provides feature extraction for Machine Learning models. - A compiler, optimizer and runtime engine that executes AQL models. SystemT was originally developed in IBM Research - Almaden and is now owned and maintained by a team with 15+ years of experience in NLP.","title":"SystemT and AQL model development"},{"location":"#features","text":"","title":"Features"},{"location":"#aql-annotation-query-language","text":"AQL is a declarative rule language, on top of NLP Primitives (e.g., tokens, parts of speech, shallow semantic parsing) and NLP Operators (e.g., dictionaries, regular expressions, span operators, etc.). AQL is a pure declarative language: it separates semantics (the \u201cwhat\u201d) from execution strategy (the \u201chow\u201d). This enables an entire array of automatic optimizations.","title":"AQL (Annotation Query Language)"},{"location":"#systemt-optimizer","text":"The Optimizer compiles AQL code to an optimized algebraic execution plan. The design of the SystemT optimizer is similar to SQL optimizers in Relational Database Systems, but with text-centric optimizations. Example optimizations include Shared Dictionary Matching, Shared Regex Matching, Regex Strength Reduction, Conditional Evaluation, and many others. These optimizations achieve orders of magnitude speed-up compared to hand-tuned execution plans. They enable NLP engineers focus on the semantics of the NLP model, without worrying about hand-tuning the execution.","title":"SystemT Optimizer"},{"location":"#systemt-runtime","text":"The SystemT runtime engine is a light-weight, high speed, low memory footprint runtime engine for executing compiled AQL code.","title":"SystemT Runtime"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#1-add-systemt-to-your-project","text":"Learning AQL 10 Minutes to SystemT (Java API)","title":"1. Add SystemT to Your Project"},{"location":"API-Reference/","text":"API Reference Latest Java Docs for the low-level Java API Latest Java Docs for the high-level Java API","title":"API Reference"},{"location":"API-Reference/#api-reference","text":"Latest Java Docs for the low-level Java API Latest Java Docs for the high-level Java API","title":"API Reference"},{"location":"FAQ/","text":"FAQ General What do SystemT and AQL stand for? SystemT is short of SystemText . It was originally invented in IBM Research - Almaden. The name pays homage to SystemR , short of SystemRelational , which was also invented in IBM Research - Almaden, in the 1970s. SystemR was the the first implementation of SQL ( Structured Query Language ), which has since become the standard relational data query language. The pioneering work done in SystemR laid the foundation of today's relational database systems such as IBM DB2, Oracle, Microsoft SQLServer. AQL stands for Annotation Query Language , and yes, you've guessed, both its name and syntax are inspired by SQL . Are there overview scientific materials? Yes, there are. - An overview of SystemT's design at NAACL 2018 Industry Track - Formal semantics of AQL in PODS 2013 and PODS 2014 - Declarative Information Extraction, or why AQL is so much faster and consumes less memory compared to cascading grammars and finite state transducers, in ACL 2010 - AQL Algebra and the design of the SystemT Optimizer ICDE 2008 Troubleshooting Coming soon!","title":"FAQ"},{"location":"FAQ/#faq","text":"","title":"FAQ"},{"location":"FAQ/#general","text":"","title":"General"},{"location":"FAQ/#what-do-systemt-and-aql-stand-for","text":"SystemT is short of SystemText . It was originally invented in IBM Research - Almaden. The name pays homage to SystemR , short of SystemRelational , which was also invented in IBM Research - Almaden, in the 1970s. SystemR was the the first implementation of SQL ( Structured Query Language ), which has since become the standard relational data query language. The pioneering work done in SystemR laid the foundation of today's relational database systems such as IBM DB2, Oracle, Microsoft SQLServer. AQL stands for Annotation Query Language , and yes, you've guessed, both its name and syntax are inspired by SQL .","title":"What do SystemT and AQL stand for?"},{"location":"FAQ/#are-there-overview-scientific-materials","text":"Yes, there are. - An overview of SystemT's design at NAACL 2018 Industry Track - Formal semantics of AQL in PODS 2013 and PODS 2014 - Declarative Information Extraction, or why AQL is so much faster and consumes less memory compared to cascading grammars and finite state transducers, in ACL 2010 - AQL Algebra and the design of the SystemT Optimizer ICDE 2008","title":"Are there overview scientific materials?"},{"location":"FAQ/#troubleshooting","text":"Coming soon!","title":"Troubleshooting"},{"location":"Learning-AQL/","text":"Learning AQL AQL (Annotation Query Language) is the language for expressing NLP rules in Watson NLP. To understand AQL and create AQL models, refer to the following resources: Table of Contents {:toc} AQL Reference See AQL concepts to understand the main concepts in AQL. See Guideline for writing AQL for examples of most commonly used AQL statements. The Complete AQL Reference Manual contains the syntax and examples for all AQL statements. Creating AQL Models You create AQL models either by: Directly editing files on your local system Using the open source Elyra-based NLP visual editor, downloadable from: github.com/CODAIT/nlp-editor .","title":"Learning AQL"},{"location":"Learning-AQL/#learning-aql","text":"AQL (Annotation Query Language) is the language for expressing NLP rules in Watson NLP. To understand AQL and create AQL models, refer to the following resources: Table of Contents {:toc}","title":"Learning AQL"},{"location":"Learning-AQL/#aql-reference","text":"See AQL concepts to understand the main concepts in AQL. See Guideline for writing AQL for examples of most commonly used AQL statements. The Complete AQL Reference Manual contains the syntax and examples for all AQL statements.","title":"AQL Reference"},{"location":"Learning-AQL/#creating-aql-models","text":"You create AQL models either by: Directly editing files on your local system Using the open source Elyra-based NLP visual editor, downloadable from: github.com/CODAIT/nlp-editor .","title":"Creating AQL Models"},{"location":"Specification-of-manifest.json/","text":"Specification of manifest.json Table of Contents {:toc} Overview manifest.json is a json-format configuration file for SystemT High-level Java API . This format specifies all the artifacts that make up a SystemT extractor including: Location of compiled TAM files to execute, or optionally, location of AQL source modules to compile (if any) and then execute Location of external dictionaries and tables required by the extractor Modules, Input and Output views expected by the extractor Other metadata including format for serializing output spans and user-specified metadata for the extractor Example manifest.json can be found in Create configuration . Format The format of the manifest.json is as follows: { \"annotator\": { \"version\": JSON string, \"key\": JSON string }, \"annotatorRuntime\": \"SystemT\", \"version\": \"1.0\", \"acceptedContentTypes\": JSON array of string, \"serializeAnnotatorInfo\": JSON boolean, \"location\": JSON string, \"serializeSpan\": JSON string, \"tokenizer\": JSON string, \"modulePath\": JSON array of string, \"moduleNames\": JSON array of string, \"sourceModules\": JSON array of string, \"inputTypes\": JSON array of string, \"outputTypes\": JSON array of string, \"externalDictionaries\": JSON record of string key/value pairs, \"externalTables\": JSON record of string key/value pairs } The semantics of each field, whether it is mandatory, and the allowed values are explained below. Field Name Type Mandatory Description version JSON string Y The version of the configuration format. Supported values: 1.0 . annotator JSON record Y User-specified annotator key and version, for informational purposes only; the format is a record with two fields: key and version . annotator.key JSON string Y User-specified key for the annotator, for informational purposes only; recommend assignment using name::guid format, but you could use any human-readable description that describes your extractor. For example, \"Extractor for rule-based implementation of PII entity types SSN, BankAccountNumber and PhoneNumber\" . annotator.version JSON string Y User-specified version for the annotator, for informational purposes only; recommend CCYY-MM-DDThh:mm:ssZ format, but you can use any other version that is suitable for your workflow. For example, you could use the version number of the project where you develop the extractor. annotatorRuntime JSON string Y Name of the field in the Annotator Module Configuration JSON record that indicates the text analytics runtime for executing this annotator; supported values: SystemT . acceptedContentTypes JSON array of string Y Name of the field in the Annotator Module Configuration JSON record that indicates the types of input text accepted by this annotator; possible values are: text/plain if the annotator supports only plain text, and text/html if the annotator supports only HTML text; specify both text/plain and text/html if the annotator supports both kinds of document text. inputTypes JSON array of string N Name of the field in the Annotator Module Configuration JSON record that indicates the input types expected by this annotator; null if no input type is expected. These are used to populate external view constructs in AQL. outputTypes JSON Array of String Y Name of the field in the Annotator Module Configuration JSON record that indicates the output types produced by this annotator. Names should coincide with the names exposed by the output view AQL statements. If the extractor outputs view X from module M using AQL statement output view X; then output type name M.X should be used. If output view X as 'Y'; is used, output type name Y should be used. serializeAnnotatorInfo JSON boolean N Whether to serialize the annotator info (including the key and the version) inside each output annotation; if true, the value of the annotator parameter is copied to each output annotation; if not specified, the default is false (no serialization). location JSON string Y Absolute path (on local file system, or a distributed file system) where all artifacts required by the annotator (including compiled modules, external dictionary files, and external table files) are located; relative paths are also supported, and they are resolved with respect to the root directory where the SystemT Java API or Python binding is executed from. serializeSpan JSON string N How to serialize the AQL Span type. Possible values: simple and locationAndText . If unspecified, the default is simple . In simple mode, spans are serialized in the format {\"begin\": Integer, \"end\": Integer} . In locationAndText mode, spans are serialized in the format {\"text\": String, \"location\": {\"begin\": Integer, \"end\": Integer}} . This latter mode coincides with the way spans are represented in Watson NLU and Discovery APIs. Here, begin and end are character offsets of the span in the input document (if the input was text/html the spans are over the input html text, and not the plain text that may have been computed internally by the extractor). Whereas text is the text covered by the span. If the input text is text/plain , the covered text is akin to inputText.substring(begin, end) , whereas if the input text is text/html , the computation of the covered text depends on how the developer computed the span in AQL: if the span was over the detagged document (after using the AQL detag statement) and the developer used the AQL Remap() function, the covered text is a substring of the input html text. If the developer did not use the Remap() function, the covered text is over the detagged text, so not a substring of the input html text, although the begin and end character offsets are always over the input text, in this case, the html text. moduleNames JSON array of string Y The names of modules to instantiate for this annotator as a JSON Array of Strings, where each string is a module name. You can specify only the top-level modules you want to run; any other dependent modules are automatically searched in the modulePath . modulePath JSON array of string N(*1) The list of the module path for this annotator as a JSON array of String values, where each String indicates a path relative to the value of location of a directory or jar/zip archive containing compiled SystemT modules (.tam files). This field enables you to execute pre-compiled models ( .tam files). If you want to run AQL models, please configure the sourceModules field. sourceModules JSON array of string N(*1) The list of AQL source modules for this annotator as a JSON array of String values, where each String points to a source AQL module directory, the path being relative to the value of location of a directory containing aql files. This field enables you to execute AQL models ( .aql files). If you'd like to run pre-compiled models, please configure the modulePath field. externalDictionaries JSON record of string key/value pairs N The external dictionaries ( create external dictionary AQL statement) used by this annotator, as a JSON array of key value pairs, where the key is the AQL external dictionary name and the value is the path to the dictionary file, relative to the value of location . externalTables JSON record of string key/value pairs N The external tables ( create external dictionary AQL statement) used by this annotator, as a JSON array of key value pairs, where the key is the AQL external table name and the value is the path to the CSV file with the table entries, relative to the value of location . tokenizer JSON string Y The type of tokenizer used by the SystemT annotator; possible values: standard to use the whitespace and punctuation-based tokenizer *1) Either modulePath or sourceModules field is required.","title":"Specification of manifest.json"},{"location":"Specification-of-manifest.json/#specification-of-manifestjson","text":"Table of Contents {:toc}","title":"Specification of manifest.json"},{"location":"Specification-of-manifest.json/#overview","text":"manifest.json is a json-format configuration file for SystemT High-level Java API . This format specifies all the artifacts that make up a SystemT extractor including: Location of compiled TAM files to execute, or optionally, location of AQL source modules to compile (if any) and then execute Location of external dictionaries and tables required by the extractor Modules, Input and Output views expected by the extractor Other metadata including format for serializing output spans and user-specified metadata for the extractor Example manifest.json can be found in Create configuration .","title":"Overview"},{"location":"Specification-of-manifest.json/#format","text":"The format of the manifest.json is as follows: { \"annotator\": { \"version\": JSON string, \"key\": JSON string }, \"annotatorRuntime\": \"SystemT\", \"version\": \"1.0\", \"acceptedContentTypes\": JSON array of string, \"serializeAnnotatorInfo\": JSON boolean, \"location\": JSON string, \"serializeSpan\": JSON string, \"tokenizer\": JSON string, \"modulePath\": JSON array of string, \"moduleNames\": JSON array of string, \"sourceModules\": JSON array of string, \"inputTypes\": JSON array of string, \"outputTypes\": JSON array of string, \"externalDictionaries\": JSON record of string key/value pairs, \"externalTables\": JSON record of string key/value pairs } The semantics of each field, whether it is mandatory, and the allowed values are explained below. Field Name Type Mandatory Description version JSON string Y The version of the configuration format. Supported values: 1.0 . annotator JSON record Y User-specified annotator key and version, for informational purposes only; the format is a record with two fields: key and version . annotator.key JSON string Y User-specified key for the annotator, for informational purposes only; recommend assignment using name::guid format, but you could use any human-readable description that describes your extractor. For example, \"Extractor for rule-based implementation of PII entity types SSN, BankAccountNumber and PhoneNumber\" . annotator.version JSON string Y User-specified version for the annotator, for informational purposes only; recommend CCYY-MM-DDThh:mm:ssZ format, but you can use any other version that is suitable for your workflow. For example, you could use the version number of the project where you develop the extractor. annotatorRuntime JSON string Y Name of the field in the Annotator Module Configuration JSON record that indicates the text analytics runtime for executing this annotator; supported values: SystemT . acceptedContentTypes JSON array of string Y Name of the field in the Annotator Module Configuration JSON record that indicates the types of input text accepted by this annotator; possible values are: text/plain if the annotator supports only plain text, and text/html if the annotator supports only HTML text; specify both text/plain and text/html if the annotator supports both kinds of document text. inputTypes JSON array of string N Name of the field in the Annotator Module Configuration JSON record that indicates the input types expected by this annotator; null if no input type is expected. These are used to populate external view constructs in AQL. outputTypes JSON Array of String Y Name of the field in the Annotator Module Configuration JSON record that indicates the output types produced by this annotator. Names should coincide with the names exposed by the output view AQL statements. If the extractor outputs view X from module M using AQL statement output view X; then output type name M.X should be used. If output view X as 'Y'; is used, output type name Y should be used. serializeAnnotatorInfo JSON boolean N Whether to serialize the annotator info (including the key and the version) inside each output annotation; if true, the value of the annotator parameter is copied to each output annotation; if not specified, the default is false (no serialization). location JSON string Y Absolute path (on local file system, or a distributed file system) where all artifacts required by the annotator (including compiled modules, external dictionary files, and external table files) are located; relative paths are also supported, and they are resolved with respect to the root directory where the SystemT Java API or Python binding is executed from. serializeSpan JSON string N How to serialize the AQL Span type. Possible values: simple and locationAndText . If unspecified, the default is simple . In simple mode, spans are serialized in the format {\"begin\": Integer, \"end\": Integer} . In locationAndText mode, spans are serialized in the format {\"text\": String, \"location\": {\"begin\": Integer, \"end\": Integer}} . This latter mode coincides with the way spans are represented in Watson NLU and Discovery APIs. Here, begin and end are character offsets of the span in the input document (if the input was text/html the spans are over the input html text, and not the plain text that may have been computed internally by the extractor). Whereas text is the text covered by the span. If the input text is text/plain , the covered text is akin to inputText.substring(begin, end) , whereas if the input text is text/html , the computation of the covered text depends on how the developer computed the span in AQL: if the span was over the detagged document (after using the AQL detag statement) and the developer used the AQL Remap() function, the covered text is a substring of the input html text. If the developer did not use the Remap() function, the covered text is over the detagged text, so not a substring of the input html text, although the begin and end character offsets are always over the input text, in this case, the html text. moduleNames JSON array of string Y The names of modules to instantiate for this annotator as a JSON Array of Strings, where each string is a module name. You can specify only the top-level modules you want to run; any other dependent modules are automatically searched in the modulePath . modulePath JSON array of string N(*1) The list of the module path for this annotator as a JSON array of String values, where each String indicates a path relative to the value of location of a directory or jar/zip archive containing compiled SystemT modules (.tam files). This field enables you to execute pre-compiled models ( .tam files). If you want to run AQL models, please configure the sourceModules field. sourceModules JSON array of string N(*1) The list of AQL source modules for this annotator as a JSON array of String values, where each String points to a source AQL module directory, the path being relative to the value of location of a directory containing aql files. This field enables you to execute AQL models ( .aql files). If you'd like to run pre-compiled models, please configure the modulePath field. externalDictionaries JSON record of string key/value pairs N The external dictionaries ( create external dictionary AQL statement) used by this annotator, as a JSON array of key value pairs, where the key is the AQL external dictionary name and the value is the path to the dictionary file, relative to the value of location . externalTables JSON record of string key/value pairs N The external tables ( create external dictionary AQL statement) used by this annotator, as a JSON array of key value pairs, where the key is the AQL external table name and the value is the path to the CSV file with the table entries, relative to the value of location . tokenizer JSON string Y The type of tokenizer used by the SystemT annotator; possible values: standard to use the whitespace and punctuation-based tokenizer *1) Either modulePath or sourceModules field is required.","title":"Format"},{"location":"ana_txtan_extract-perform/","text":"Improving the performance of an extractor Table of Contents {:toc} Overview of AQL runtime performance best practices Since you can usually rely on the SystemT Optimizer to choose an efficient execution plan, writing high-performance extraction rules with SystemT is relatively simple. But there are a few best practices that are important to follow to maximize the effectiveness of your extractor. Step 1: Know your performance requirements To begin, you must understand what level of throughput is acceptable for your application and what kinds of documents the extractor will examine. Step 2: Follow the optimization guidelines for writing AQL These guidelines describe the ways that you can write your AQL code so that the Optimizer has the most latitude in choosing efficient execution plans. Step 3: Focus on writing AQL instead of tuning performance You can usually rely on the Optimizer to choose an efficient plan, so do not tune performance while you are writing AQL. Step 4: Use the AQL Profiler to find problems Sometimes there is not an efficient plan for the particular semantics of the AQL expressions. The AQL Profiler that is included in the public API is designed to help you quickly zero in on these problem cases. Step 5: Tune the most expensive view first Focus your performance tuning on views that are heavily represented in the AQL Profiler output. Examples: Solving common performance problems The main goal of tuning an AQL view is to identify the reason that the view is running slowly, and then fix that problem. Step 1: Know your performance requirements To begin, you must understand what level of throughput is acceptable for your application and what kinds of documents the extractor will examine. These requirements determine how precisely you need to follow the other best practices. Different text applications have vastly different performance requirements. For example, if you are conducting a sentiment analysis on a small, fixed set of call-center records, you will probably require a throughput of only kilobytes per second. However, if you are analyzing over 10% of a Twitter feed, a throughput of megabytes per second might be needed. Performance tuning takes time and makes the code more difficult to maintain. Performance tuning is expensive and takes much time and effort. Changing the structure of the rules for performance adds extra effort later because it makes the AQL rules harder to maintain. If possible, try to design the solution so that high throughput is not needed. The best way to tune for performance is to make tuning unnecessary. In many cases, you can achieve this goal just by looking closely at the true requirements of the application. Evaluate the goal of your extractor, and try to determine the most efficient method by answering a few important questions: At what rate is the new data coming in? Is it possible to increase the level of parallelism (for example, by increasing the number of mappers per node, or the size of the cluster) instead of tuning the single-threaded performance? Can the results of an extraction be saved for later? Step 2: Follow the optimization guidelines for writing AQL These guidelines describe the ways that you can write your AQL code so that the Optimizer has the most latitude in choosing efficient execution plans. These optimization guidelines can help you to reduce AQL code errors, and improve the performance of your extractor. Follow the guidelines for regular expressions Write SimpleRegex compatible expressions and use token constraints in your regular expression extraction specifications whenever possible. Write regular expressions that are simple and easy to understand, and do not use regular expressions as a substitute for dictionaries. Write SimpleRegex compatible expressions There are two regular expression engines that are used by the SystemT runtime component, JavaRegex and SimpleRegex. The JavaRegex engine is slower but allows for more advanced processing. The SimpleRegex engine is a fast regular expression evaluation engine, but it supports a slightly limited class of expressions. Since the SimpleRegex engine is more scalable than the Java\u2122 engine, a good practice is to try to write SimpleRegex compatible expressions whenever possible. The SystemT engine automatically chooses the suitable Regex engine for running a given regular expression. Most expressions are SimpleRegex compatible. Supported SimpleRegex constructs are: Alternation Sequence Parentheses Basic quantifiers such as +, *, ?, {m,n} Unicode character classes Standard built-ins such as \\d, \\s A regular expression is not SimpleRegex compatible if at least one of the following is true: Uses one of these flags: CANON_EQ, COMMENTS, LITERAL, UNICODE, UNIX_LINES Contains look-ahead or look-behind constructs such as \\A, \\Z, \\b, ^, $, (?=X), (?!X), (?<=X), or (?<!X) Contains capturing groups (such as, return group 1 as \u2026) The AQL compiler flags a warning for all of the regular expressions that are not SimpleRegex compatible. If you encounter such a warning, try to determine if you can simplify your regular expression to make it SimpleRegex compatible while you preserve the semantics of your extractor. As an example, consider this statement for extracting numbers with optional decimals. The statement uses the \\b regex character class to indicate that the regular expression matching must be done on tokens considering word boundaries. create view NumberUsingJavaRegex as extract regex /\\b\\d+(\\.\\d+)?\\b/ on D.text as match from Document D; This statement matches stand-alone numbers such as 10.99 in the phrase \u201cI purchased something for 10.99 USD\u201d, but it does not match numbers that are part of a larger word such as \u201820\u2019 in the phrase \u201cI just bought something for USD20\u201d. Alternatively, you can rewrite the regular expression to use SimpleRegex: create view NumberUsingSimpleRegex as extract regex /\\d+(\\.\\d+)?/ on D.text as match from Document D; In this example, the word boundary \\b characters were removed. As a result, the regular expression is compatible with the SimpleRegex engine, but the semantics are slightly different from the original. The new expression \\d+(\\.\\d+)? matches both the numbers '10.99\u2019 and \u201820\u2019 from the phrases \u201cI purchased something for 10.99 USD\u201d, and \"I just bought something for USD20\u201d. However, the transformation might be acceptable if the number \u201820\u2019 is not needed in the final result (perhaps due to other constructs used downstream in parts of your AQL code). If it is important to preserve the semantics of the original regular expression while using a regular expression that conforms to SimpleRegex, consider specifying token constraints in the extract regex statement. Use token constraints in your regular expression extraction specifications The Optimizer performs Shared Regular Expression Matching (SRM), an optimization in which regular expressions that are both on token boundaries and are SimpleRegex compatible are grouped together and evaluated in one pass over the input text. Generally, this means that all regular expressions in the group are evaluated in approximately the same amount of time that it takes to individually evaluate the slowest expression in the group. When possible, write your regular expression to take advantage of the SRM optimization. The regular expression should be SimpleRegex compatible and have token constraints. Consider the same AQL example that uses a regular expression with word boundary character class \\b , and without token specifications: create view Number as extract regex /\\b\\d+(\\.\\d+)?\\b/ on D.text as match from Document D; Here is how you would rewrite it to both make it SimpleRegex compatible (by removing the \\b ) and amenable to SRM (by introducing the token constraints): create view NumberSRM as extract regex /\\d+(\\.\\d+)?/ on between 1 and 3 tokens in D.text as match from Document D; Use dictionaries instead of regular expressions An AQL regular expression that matches a list of terms is usually more expensive to evaluate than the equivalent extract dictionary statement. The dictionary evaluator in SystemT is specialized for the particular case of matching large dictionaries of strings on token boundaries, and the SystemT Optimizer generates plans that evaluate multiple extract dictionary statements in a single pass. Therefore, when you are looking for mentions of a set of words or phrases, avoid writing your extractor as a regular expression consisting of an alternation of terms and use a dictionary instead. For example, if you are looking for mentions of currencies, do not write your statement using a regular expression: -- Not recommended; long alternation of terms should be a dictionary create view Currency as extract regex /dollar|sterling pound|cent|euro|rupee|yen|peso|rouble/ with flags 'CASE_INSENSITIVE' on D.text as match from Document D; In addition to the performance-related reasons for using a dictionary, this example also illustrates some additional common mistakes associated with misusing regular expressions: The extract regex statement will output matches that start or end in the middle of a word. For example, the statement will match the first four letters of the word \u201c cent er\u201d. The expression will match the phrase sterling pound only if there is exactly one space character between the two words. If the words sterling and pound appear in a document separated by a carriage return or by two spaces, the regular expression will not produce a match. Instead, this rewrite uses a dictionary of terms. Using a dictionary increases performance, makes the AQL code easier to maintain and understand, and the code will be less prone to bugs: -- Use a dictionary to match against same list of terms create dictionary CurrencyDict as('dollar', 'sterling pound', 'cent', 'euro', 'rupee', 'yen', 'peso', 'rouble'); create view CurrencyAlternative as extract dictionary 'CurrencyDict' on D.text as match from Document D; Write short and simple regular expressions Avoid writing long and complex regular expressions. In general, long and complex regular expressions make AQL code harder to understand and maintain. This type of regular expressions can be expensive to evaluate. Consider breaking a complex regular expression into smaller expressions, and combining the results of these expressions by using other AQL constructs. In the following example, a regular expression is used to identify one to three capitalized words. The words are separated by white space. -- Not recommended; long, complex regular expressions are difficult to maintain create view OneToThreeCapitalizedWords as extract regex /[A-Z][a-z]+(\\s+[A-Z][a-z]+){0,2}/ on D.text as match from Document D; The regular expression becomes harder to maintain as the subexpression for the capitalized word becomes more complex; for example, if it was to support Unicode characters. For ease of maintenance and understanding, you can break down the expression into smaller components. For example, you can use one regular expression with token constraints to find a single capitalized word, and the extract blocks statement to find blocks of one to three capitalized words: -- Same extraction in an efficient, simple manner -- A single capitalized word create view CapitalizedWords as extract regex /[A-Z][a-z]+/ on 1 token in D.text as word from Document D; -- One to three capitalized words - rewritten avoiding use of long, complex regular expression create view OneToThreeCapitalizedWordsBetter as extract blocks with count between 1 and 3 and separation 0 tokens on CW.word as capswords from CapitalizedWords CW consolidate on capswords; Alternatively, you can also use extract pattern to find one to three capitalized words to make it even simpler and more efficient: -- Same extraction using the 'extract pattern' statement -- One to three capitalized words create view OneToThreeCapitalizedWordsBest as extract pattern <CW.word>{1,3} as capswords from CapitalizedWords CW consolidate on capswords; In both cases, since the extract blocks and extract pattern statements return all matches, including the matches that overlap, the consolidate clause was used to remove the matches that overlap. Watch for regular expressions that match an empty string A match on an empty string can create a serious performance problem. These matches can lead to unexpected results from your downstream AQL code, as well as performance bottlenecks (as many spans and tuples need to be created to hold the result). Here is an example of a regular expression that involves an alternation of two optional subexpressions, one of which is an empty string: -- Not recommended; the regular expression matches the empty string create view OptionalCurrency as extract regex /(\\$)?|(US dollar)?/ on D.text as match from Document D; In general, regular expressions that match the empty string are programming errors. The longer the regular expression, the easier it is to make such a mistake. Try to keep your regular expressions as simple as permitted by the semantics of your extractor by following the other regular expression guidelines. Use the and keyword instead of the And() built-in predicate The Optimizer does not attempt to optimize the order of evaluation of the arguments to the And() predicate. A rule that uses the And() built-in predicate such as: select ... from ... where And(predicate1, predicate2); often runs considerably slower than the same rule in the form of: select ... from ... where predicate1 and predicate2; When possible, use the SQL-style and keyword instead of the And() predicate. Avoid Cartesian products Most often, the result of a Cartesian product is very large and takes a long time to compute. Cartesian products occur in statements where the from clause contains two or more views or tables, but some of them are not connected through a join predicate (in the where clause). The presence of a Cartesian product usually indicates that the join predicate was omitted by mistake. This is a problem that can be easily corrected. In the most simple case, a Cartesian product occurs when there are two views in the from clause, but there is no join predicate relating these two views in the where clause, as in this example: create dictionary PersonDict as ('Bill Clinton', 'Barack Obama', 'George Bush'); create view Person as extract dictionary 'PersonDict' on D.text as name from Document D; -- Extract U.S. phone numbers of the form '123-456-7890' create view Phone as extract regex /\\p{Nd}{3}\\-\\p{Nd}{3}\\-\\p{Nd}{4}/ on between 1 and 10 tokens in D.text as number from Document D; --Cartesian product between Person and Phone create view PersonPhone as select P.name, Ph.number from Person P, Phone Ph; The semantics of this AQL rule is to compute the set of all pairs of Person.name and Phone.number in the input document. If the input contains 1000 persons and 1000 phone numbers, the result consists of 1,000,000 pairs. However, it is unlikely that the intent was to compute all of these pairs. The intent likely was to find pairs of person and phone in close proximity of each other. This goal can be easily accomplished by adding a Follows or FollowsTok join predicate to the where clause: --With a join predicate create view PersonPhoneJoinWithPredicate as select P.name, Ph.number from Person P, Phone Ph where FollowsTok(P.name, Ph.number, 0, 5); Cartesian products can also occur in more subtle cases. In this example, the from clause consists of four views. However, only A and B, and respectively C and D, are connected by a join predicate. Hence, a Cartesian product occurs between the result of joining A and B and the result of joining C and D. create dictionary FirstNamesUSA as ('Bill', 'George', 'Barack'); create dictionary FirstNamesUK as ('William', 'George', 'Stewart'); create dictionary LastNamesUSA as ('Clinton', 'Bush', 'Obama'); create dictionary LastNamesUK as ('Clinton', 'Johnson', 'Vaughan'); create view A as extract dictionary 'FirstNamesUSA' on D.text as match from Document D; create view B as extract dictionary 'FirstNamesUK' on D.text as match from Document D; create view C as extract dictionary 'LastNamesUSA' on D.text as match from Document D; create view D as extract dictionary 'LastNamesUK' on D.text as match from Document D; --Cartesian product between A joined with B and C joined with D create view ABCD as select A.* from A, B, C, D where Equals(A.match ,B.match) and Equals(C.match ,D.match); To avoid Cartesian products, every view in the from clause must be connected to each of the other views by a single join predicate, or a chain of join predicates. These are some cases when Cartesian products between two views do not introduce a performance bottleneck. There are cases when the result of the Cartesian product is small, regardless of the documents on which you are using your extractor. An example is when both views contain at most a few tuples, or when one of the views has exactly one tuple. For example, the view Document always has a single tuple; therefore, AQL rules that involve a Cartesian product with the view Document (for example, to add additional metadata to an output view) do not introduce performance bottlenecks. create dictionary MergerDict as ('merged', 'Merged', 'merger', 'Merger', 'merging', 'Merging', 'M&A'); create view Merger as extract dictionary 'MergerDict' on D.text as mergeMention from Document D; --Cartesian product with the view Document is OK assuming the view Document has a single tuple or small number of tuples of dates create view DocumentMergerDate as select M.*, D.text as mergerDate from Document D, Merger M; Avoid UDFs as join predicates When the Optimizer is not aware of the semantics of the UDF implementation, it must generate a Nested Loops Join plan that examines every input span. If possible, express the semantics of the UDF in a way that allows the Optimizer to suggest performance improvements. A user-defined function (UDF) can appear in the where clause of an AQL select statement as a join predicate. A join predicate is a predicate that determines whether a given pair of tuples from the from clause appears in the output of the view. Consider the following example, which identifies matching pairs of capitalized and lowercase names: create dictionary CapsNameDict as ('GEORGE', 'SAMUEL', 'PETER', 'HENRY'); create dictionary LowNameDict as ('samuel', 'johnson', 'david', 'george'); create function equalsNormalized (p1 Span, p2 Span) return Boolean external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!equalsNormalized' language java deterministic return null on null input; create function normalize (p1 String) return String external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!normalize' language java deterministic return null on null input; create view CapsName as extract dictionary 'CapsNameDict' on D.text as match from Document D; create view LowName as extract dictionary 'LowNameDict' on D.text as match from Document D; In this case, equalsNormalized() is a UDF function that takes as input two spans, and checks if their normalized values are equal. -- 'equalsNormalized' is a UDF that checks normalized values create view EqualNormalizedName as select C.match as caps, L.match as low from CapsName C, LowName L where equalsNormalized(L.match, C.match); When a select statement uses a UDF in this way, the Optimizer has limited options for compiling the statement. Since the Optimizer is not aware of the semantics of the UDF implementation, it must generate a Nested Loops Join plan that examines every pair of CapsName and LowName . As both of these views grow larger, this execution plan can become very expensive. In some cases, it might be possible to express the same semantics in a different way. In the case of the current example, the semantics of this rule can be obtained by using a slightly different UDF, called normalize(), that takes as input a span and outputs the normalized value of the span, together with the built-in predicate Equals() to compare the two normalized values: -- Avoid UDF as the join predicate and use built-in function to establish equality -- 'normalize', however is a UDF itself, although not used as the join predicate create view EqualNormalizedNameNoUDF as select C.match as caps, L.match as low from CapsName C, LowName L where Equals(normalize(GetString(L.match)), normalize(GetString(C.match))); In this case, the Optimizer can choose the Hash Join algorithm to implement the semantics of the Equals() predicate, which is usually much faster than using a Nested Loops Join operator. Be careful when using Or() as a join predicate The Or() built-in predicate is run with the Nested Loops Join operator. In most cases, if the inputs are supported by faster join operators, it is worthwhile to split the statement into a union of multiple smaller statements. For example, consider this sample where Or() is run with the Nested Loops Join operator: -- not recommended create view CandidatesAll as select C.* from A, C where Or( FollowsTok(A.match, C.match, 0, 5), FollowsTok(C.match, A.match, 0, 5) ); The view can be rewritten into the union of two statements, which gives the Optimizer the opportunity to use faster join algorithms such as Adjacent Join or Sort Merge Join to implement the FollowsTok() predicates in the two select statements to allow the run time to employ faster join algorithms internally:: -- above view, rewritten to use union all statement create view CandidatesAllBetter as ( select C.* from A, C where FollowsTok(A.match, C.match, 0, 5) ) union all ( select C.* from A, C where FollowsTok(C.match, A.match, 0, 5) ); Use the consolidate clause wisely The consolidate clause is designed to be used at specific points in the extractor where overlapping matches are no longer useful downstream and can be removed. Do not use the consolidate clause more often than necessary, but take advantage of its functionality to filter your data. If you use the consolidate too often your AQL code can be harder to maintain since each and every statement has an extra consolidate clause. You can also introduce redundant sorting operations, although this is not usually a performance bottleneck. More importantly, if you do not use the consolidate clause enough, a large number of intermediate results can be carried downstream from one view to another, past the point where they are technically required by the semantics of the extractor. A large amount of unnecessary intermediate results can increase the memory footprint and slow extractor throughput. In this example that illustrates this point, the extractor is looking for mentions of department names, followed within a single token by a phone number. -- A single capitalized word create view UppercaseWord as extract regex /[A-Z]+/ on 1 token in D.text as word from Document D; -- Find organization names as one to two capitalized words create view Department as extract blocks with count between 1 and 2 and separation 0 tokens on UW.word as name from UppercaseWord UW; -- Find phone numbers of the form xxx-xxxx create view Phone as extract regex /d{3}-\\d{4}/ on 3 tokens in D.text as number from Document D; -- Find candidate pairs of department and phone, where the department is followed within one token by the phone create view DepartmentPhoneAll as select D.name, P.number, CombineSpans(D.name, P.number) as deptWithPhone from Department D, Phone P where FollowsTok(D.name, P.number, 1, 1); -- Consolidate to remove overlapping mentions followed within one token by the phone create view DepartmentPhone as select DP.* from DepartmentPhoneAll DP consolidate on DP.deptWithPhone; An extremely simplistic Department extractor of a single uppercase word or two uppercase words can help illustrate the issues. Assume the following sample is the input text: CS DEPARTMENT 555-1234 REGISTRAR 555-5678 The semantics of the extract block statement are to return all mentions, including those mentions that overlap. Therefore, the view Department contains the following spans: CS DEPARTMENT CS DEPARTMENT REGISTRAR The result of DepartmentPhoneAll is: CS, 555-1234 DEPARTMENT, 555-1234 CS DEPARTMENT, 555-1234 REGISTRAR, 555-5678 Notice that DepartmentPhoneAll contains overlapping matches that are unnecessary at this point in the extractor. Those overlapping matches are removed by the consolidate clause in the final view DepartmentPhone , to return this desired result: CS DEPARTMENT, 555-1234 REGISTRAR, 555-5678 However, the overlapping Department spans are probably not needed when you are looking for corresponding phones. The spans result in extra tuples that are created in DepartmentPhoneAll and carried downstream to DepartmentPhone , only to be removed later on by the consolidate clause. Instead, the unnecessary Department partial spans can be removed earlier, in the Department view: -- Find organization names as one to two capitalized words -- Use the consolidate clause to remove unnecessary overlapping mentions create view DepartmentBetter as extract blocks with count between 1 and 2 and separation 0 tokens on UW.word as name from UppercaseWord UW consolidate on name; Define common subpatterns explicitly Define a view that computes common subpatterns so that the Optimizer does not need to evaluate each pattern. The Optimizer does not currently perform common subexpression elimination for sequence pattern expressions. Therefore, subpatterns that appear in multiple pattern expressions are evaluated multiple times. In the following code snippet, the common subexpression <A.match> <B.match> is computed twice; once for Pattern1 , and again for Pattern2 : -- subexpression <A.match> and <B.match> computed twice; once each for Pattern1 and Pattern2 create view Pattern1 as extract pattern <A.match> <B.match> <C.match> as match from A A, B B, C C; create view Pattern2 as extract pattern <A.match> <B.match> <D.match> as match from A A, B B, D D; If you find that multiples of your pattern expressions use a common subpattern, it is a good idea to explicitly define a view that computes the subpattern. You can reuse that view when you define the larger patterns. In this example, the view SubPattern explicitly defines <A.match> <B.match> . This is then reused in the views Pattern1 and Pattern2 : -- Define the common subpattern in its own view so not computed unnecessarily create view SubPattern as extract pattern <A.match> <B.match> as match from A A, B B; create view Pattern3 as extract pattern <AB.match> <C.match> as match from SubPattern AB, C C; create view Pattern4 as extract pattern <AB.match> <D.match> as match from SubPattern AB, D D; Step 3: Focus on writing AQL instead of tuning performance You can usually rely on the Optimizer to choose an efficient plan, so do not tune performance while you are writing AQL. The SystemT engine has a sophisticated cost-based Optimizer, so you can concentrate on writing AQL rules that are easy to understand and easy to maintain. Tuning your AQL to force a particular plan can be counter-productive The best execution plan is often not obvious, especially when you are considering an extractor that might consist of multiple different modules written by different developers. Tuning AQL rules to produce a particular execution plan that seems optimal can be counter-productive. This tuning always makes the extractor harder to maintain, and it often makes the extractor slower. Write AQL for ease of maintenance and understanding Aside from following the basic AQL guidelines, in most cases you will have better performance, as well as improved accuracy and maintainability, if you write AQL rules for ease of maintenance and understanding. Let the Optimizer do its job, then check to make sure that the Optimizer chose the most efficient plan by using the AQL Profiler. Step 4: Use the AQL Profiler to find problems Sometimes there is not an efficient plan for the particular semantics of the AQL expressions. The AQL Profiler is designed to help you quickly zero in on these problem cases. The AQL Profiler is included in the SystemT low-level Java API (class com.ibm.avatar.api.AQLProfiler) and is accessible in the AQL Developer Tooling (Eclipse-based) by right clicking on your AQL project > Profile as > Text Analytics Profiler Configuration . The AQL Profiler is a sampling profiler for SystemT operator graphs. While one thread runs the extractor over a collection of documents, a second thread periodically samples the state of the first thread, or what the operator is running at that moment. After the extraction thread is done running, the AQL Profiler sums up all of these samples to produce a breakdown of what views and operators account for what percentage of overall run time. This approach gives an accurate picture of the run time of various views and operators, as long as the AQL Profiler is allowed to run for a few tens of seconds. The SystemT low-level Java APIs describe how to start the AQL Profiler: You provide a collection of example documents to use as a target of extraction during profiling. The AQL Profiler runs on the set of documents that you specified. The documents should represent the kinds of documents that will be used in the end-to-end application, but you do not need a large document set. The AQL Profiler loops over the document collection until it has run for the specified minimum number of seconds. It is recommended to run the AQL Profiler for at least 30 seconds, to ensure the JVM has sufficient time to optimize byte code. This ensures the profiler results are realistic and as expected in production mode when usually an extractor is loaded once, and subsequently used to process many documents over a long period of time. Hot Views Hot views are views within the compiled plans that are responsible for the largest fraction of execution time. The views are sorted from least to most expensive. The view at the lower portion of the table is the most expensive and it is the best target for any hand-tuning efforts. Sequence patterns and subqueries Sequence patterns are implemented via an AQL to AQL rewrite. Each sequence pattern turns into one or more AQL views. Each extract pattern statement is turned into a collection of internal views that are based on select and extract statements before they are fed to the Optimizer. You will see these view names in the operator graph plan and the AQL Profiler output. The naming convention for sequence patterns is: ```bash _[original view name]_TmpView_[pattern]__[number] ``` For example: ``` _AmountWithUnit_TmpView_<N.match> <U.match>__3 ``` A similar approach applies to subqueries in the from clause of a `select` statement. Hot Documents Hot documents are documents that took unusually long amounts of time to process during the profiling run. The Optimizer attempts to produce plans that are not sensitive to the input documents, but sometimes such plans do not exist. For example, there might be a regular expression that happens to run slowly on particular input strings, or an AQL view can require the extractor to output a view whose size is very large for certain documents. If you find a problem document, you can create a document collection with just that document and run the Profiler again. The first document in the collection often comes to the top of this list, because the \u201cjust in time\u201d Java\u2122 compiler might be generating object code at the same time that it is processing this document. Step 5: Tune the most expensive view first Focus your performance tuning on views that are heavily represented in the AQL Profiler output. A standard performance tuning practice is that if a view is taking up 10% of the execution time, even if you drop its running time to zero, the overall extractor will only get 10% faster. When you tune for performance it impacts readability and ease of maintenance, so be selective. Use Amdahl's law Amdahl\u2019s Law states that the maximum possible speed enhancement from tuning a view is inversely proportional to what fraction of the execution time that the view represents. If a view takes 5% of the execution time and you make it 100 times faster, the extractor becomes about 1.05 times faster. If a view takes 90% of the execution time and you make it 100 times faster, the extractor becomes about 9.2 times faster. Concentrate efforts on the most expensive view Concentrate any tuning efforts on the most expensive view. Each time you make a change to improve performance, it is important to rerun the AQL Profiler and to check whether the view that you are working on is still the most expensive. If another view moves to the top of the list, switch to working on that view. After each round of tuning, rerun the AQL Profiler and repeat This process should continue until either the throughput goals of the application are met or all views take approximately the same amount of time to process. Examples: Solving common performance problems The main goal of tuning an AQL view is to identify the reason that the view is running slowly, and then fix that problem. Once you determine which view to tune for performance, you can focus on tuning that view. These scenarios illustrate regular causes of performance problems, from most common to least common, and explain ways to fix these performance problems. Did not follow an AQL guideline: Correct the AQL The most common performance tuning scenario is the need to correct a case where the basic AQL writing guidelines were not followed. In this example, word boundary checks ( \\b ) in the regular expression prevent Regular Expression Strength Reduction and Shared Regular Expression Matching (RSR/SRM) optimization. These are the most powerful regular expression optimizations. create view Number as extract regex /\\b\\d+\\b/ on D.text as match from Document D; This problem is easily corrected by modifying the view so that it conforms to the guideline. In this case, you can rewrite the extract regex statement to use the on 1 token clause in place of the word boundary checks. This change enables both RSR/SRM to be applied, without changing the results of the view. You can convert the regular expression to an RSR/SRM compatible view that produces the same result. create view Number as extract regex /\\d+/ on 1 token in D.text as match from Document D; Optimizer could not apply an optimization: Refine the view definition so that optimization applies Another common scenario occurs when the Optimizer is unable to apply an optimization due to unnecessary constraints built into the structure of the AQL rules. In this example, the extractor identifies expressions of sentiment that apply to particular IBM\u00ae products. The rules are composed of two parts: a very complex and expensive sentiment extractor, and a simple dictionary evaluation that looks for product names: -- Table containing <name, sentiment> pairs of well-known software products create table SentimentTable (name Text, sentiment Text) as values ('Cognos', 'positive'), ('QuickBooks', 'neutral'), ('Office 365', 'positive'), ('Lotus Notes', 'neutral'), ('DB2', 'positive'), ('SQL Server', 'neutral'), ('BigInsights', 'positive'), ('Photoshop', 'positive'), ('Watson', 'positive'); create dictionary IBMProductNameDict as ( 'DB2', 'BigInsights', 'Watson'); -- Classify mentions of products as positive or negative create view ProductPairs as select S.name as name, S.sentiment as sentiment from SentimentTable S where MatchesDict('IBMProductNameDict', S.name); The Optimizer cannot apply the most productive optimizations because of the way the AQL was written. This issue often happens because there are optimizations that occur behind the scenes that conflict with an attempt to hand-tune the AQL for performance while the rules are being written. In this case, the constraint is expressed that only sentiments about IBM products are relevant as an explicit filtering condition over the output tuples of the Sentiment view. There is only one plan that the Optimizer can generate that completely implements the semantics of the view; the Optimizer must evaluate the entire sentiment extractor all of the time, then run the results through a Select operator that filters the sentiments that do not apply to the target products. This plan leads to wasted work, particularly if most of the documents do not contain a product name. You can rewrite this view to give the Optimizer more leeway to choose an efficient plan. Take that dictionary match that was expressed as a filtering condition (a MatchesDict() predicate), and rewrite it into a join operator with the results of an extract dictionary statement. The result is a set of rules that has almost the same low-level semantics, and from the perspective of the application the semantics are exactly the same. But the Optimizer can now apply Conditional Evaluation to skip sentiment analysis entirely on documents that do not contain a product name. -- Table containing <name, sentiment> pairs of well-known software products create table SentimentTable (name Text, sentiment Text) as values ('Cognos', 'positive'), ('QuickBooks', 'neutral'), ('Office 365', 'positive'), ('Lotus Notes', 'neutral'), ('DB2', 'positive'), ('SQL Server', 'neutral'), ('BigInsights', 'positive'), ('Photoshop', 'positive'), ('Watson', 'positive'); create dictionary IBMProductNameDict as ( 'DB2', 'BigInsights', 'Watson'); create view IBMProductName as extract dictionary 'IBMProductNameDict' on D.text as name from Document D; -- Classify mentions of products as positive or negative create view ProductPairs as select S.name as name, S.sentiment as sentiment from SentimentTable S, IBMProductName I where Equals(S.name, I.name); Optimizer made a mistake: Rework the view definition to force the correct plan Occasionally, the Optimizer makes a mistake. The Optimizer chooses its plan that is based on a cost model, and this model is not perfect. For example, the cost model currently assumes that all dictionary evaluations are faster than all regular expressions. But in some cases, such as the following example, this assumption is not true. create dictionary TheDict as ('the'); create view The as extract dictionary 'TheDict' on D.text as match from Document D; create view Nth as extract regex /\\d+th/ on 1 token in D.text as match from Document D; -- Find phrases like \"the 10th\" create view TheNth as select CombineSpans(T.match, N.match) as match from The T, Nth N where FollowsTok(T.match, N.match,0,0); In this case, evaluating a dictionary that contains the common word the is fairly expensive because that word produces a large number of matches, and it takes time to construct a tuple for each match. The regular expression in this example is small and straightforward. It produces very few matches on most documents, so evaluating this regular expression is likely to be less expensive than running the TheDict dictionary. For the overall extractor, which looks for phrases like the 10th , the Optimizer always incorrectly chooses to evaluate the dictionary and to use conditional evaluation or Restricted Span Evaluation (RSE) to reduce the time that is spent on the regular expression. The alternate plan, to evaluate the regular expression first, is actually the one that the Optimizer should choose. This mistake can be corrected by rewriting the AQL so that the Optimizer has no choice but to choose the \u201ccorrect\u201d plan. Basically, the modification is the inverse of the previous example. Take the join between the views The and NTh and replace that join with a filtering condition on the contents of the Nth view. This rewrite forces the compiler to generate an execution plan that uses RSE to run the TheDict dictionary only on selected portions of each document. create dictionary TheDict as ('the'); create view Nth as extract regex /\\d+th/ on between 1 and 1 tokens in D.text as match from Document D; -- Find phrases like \"the 10th\" create view TheNth as select CombineSpans ( LeftContextTok(N.match, 1), N.match ) as match from Nth N where MatchesDict('TheDict', LeftContextTok(N.match, 1)); Important: These rewrites harm maintainability and can make the extractor slower if you are not careful. Rework views in this way only if the Profiler indicates that there is a serious performance problem. No efficient plan exists: Reevaluate the semantics of the extractor A relatively uncommon performance tuning scenario is when there is no possible efficient plan for a given set of AQL rules. Here is an example of how this kind of scenario can occur: create view SentenceBoundary as extract D.text as text, regex /(([\\.\\?!]+\\s)|(\\n\\s*\\n))/ on D.text as boundary from Document D; create view Sentence as extract split using S.boundary retain right split point on S.text as sentence from SentenceBoundary S; create dictionary ProductNameDict as ( 'DB2', 'BigInsights', 'Watson'); create view ProductName as extract dictionary 'ProductNameDict' on D.text as match from Document D; -- Find pairs of product names that occurred in the same sentence create view ProductPairs as select N1.match as name1, N2.match as name2 from ProductName N1, ProductName N2, Sentence S where FollowsTok(N1.match, N2.match, 0, 10) and Contains(S.sentence, N1.match) and Contains(S.sentence, N2.match) and Not(Equals(N1.match, N2.match)); In this example, the view Sentence splits the contents of the document into individual sentences. Imagine that the SentenceBoundary view is expensive because it contains an expensive regular expression. Another view, ProductPairs , looks for sentences that contain mentions of two different product names. Since the Sentence view uses the extract split statement, the join in the ProductPairs view must take the entire collection of sentences as input and identifying sentence boundaries can be expensive. The Optimizer can improve throughput by leveraging Conditional Evaluation to ensure that the Sentence view is only evaluated when the document contains at least one pair of product names. However, that entire view must still run even if the document contains only one product name pair. Basically, even the best possible plan is not very good. This problem can be corrected by rethinking the semantics of the extractor. Instead of looking for two product mentions within the same sentence, you can find two product names that do not have a sentence boundary between them. Convert the expensive regular expression for sentence boundaries into a filtering condition (the Not(MatchesRegex())), and the Optimizer can access an execution plan that avoids evaluating the expensive regular expression over most of the document. create dictionary ProductNameDict as ( 'DB2', 'BigInsights', 'Watson'); create view ProductName as extract dictionary 'ProductNameDict' on D.text as match from Document D; -- Find pairs of product names that occurred in the same sentence without splitting sentences create view ProductPairs as select N1.match as name1, N2.match as name2 from ProductName N1, ProductName N2 where FollowsTok(N1.match, N2.match,0,10) and Not(MatchesRegex(/(([\\.\\?!]+\\s)|(\\n\\s*\\n))/, SpanBetween(N1.match, N2.match)));s","title":"Improving AQL performance"},{"location":"ana_txtan_extract-perform/#improving-the-performance-of-an-extractor","text":"Table of Contents {:toc}","title":"Improving the performance of an extractor"},{"location":"ana_txtan_extract-perform/#overview-of-aql-runtime-performance-best-practices","text":"Since you can usually rely on the SystemT Optimizer to choose an efficient execution plan, writing high-performance extraction rules with SystemT is relatively simple. But there are a few best practices that are important to follow to maximize the effectiveness of your extractor. Step 1: Know your performance requirements To begin, you must understand what level of throughput is acceptable for your application and what kinds of documents the extractor will examine. Step 2: Follow the optimization guidelines for writing AQL These guidelines describe the ways that you can write your AQL code so that the Optimizer has the most latitude in choosing efficient execution plans. Step 3: Focus on writing AQL instead of tuning performance You can usually rely on the Optimizer to choose an efficient plan, so do not tune performance while you are writing AQL. Step 4: Use the AQL Profiler to find problems Sometimes there is not an efficient plan for the particular semantics of the AQL expressions. The AQL Profiler that is included in the public API is designed to help you quickly zero in on these problem cases. Step 5: Tune the most expensive view first Focus your performance tuning on views that are heavily represented in the AQL Profiler output. Examples: Solving common performance problems The main goal of tuning an AQL view is to identify the reason that the view is running slowly, and then fix that problem.","title":"Overview of AQL runtime performance best practices"},{"location":"ana_txtan_extract-perform/#step-1-know-your-performance-requirements","text":"To begin, you must understand what level of throughput is acceptable for your application and what kinds of documents the extractor will examine. These requirements determine how precisely you need to follow the other best practices. Different text applications have vastly different performance requirements. For example, if you are conducting a sentiment analysis on a small, fixed set of call-center records, you will probably require a throughput of only kilobytes per second. However, if you are analyzing over 10% of a Twitter feed, a throughput of megabytes per second might be needed. Performance tuning takes time and makes the code more difficult to maintain. Performance tuning is expensive and takes much time and effort. Changing the structure of the rules for performance adds extra effort later because it makes the AQL rules harder to maintain. If possible, try to design the solution so that high throughput is not needed. The best way to tune for performance is to make tuning unnecessary. In many cases, you can achieve this goal just by looking closely at the true requirements of the application. Evaluate the goal of your extractor, and try to determine the most efficient method by answering a few important questions: At what rate is the new data coming in? Is it possible to increase the level of parallelism (for example, by increasing the number of mappers per node, or the size of the cluster) instead of tuning the single-threaded performance? Can the results of an extraction be saved for later?","title":"Step 1: Know your performance requirements"},{"location":"ana_txtan_extract-perform/#step-2-follow-the-optimization-guidelines-for-writing-aql","text":"These guidelines describe the ways that you can write your AQL code so that the Optimizer has the most latitude in choosing efficient execution plans. These optimization guidelines can help you to reduce AQL code errors, and improve the performance of your extractor.","title":"Step 2: Follow the optimization guidelines for writing AQL"},{"location":"ana_txtan_extract-perform/#follow-the-guidelines-for-regular-expressions","text":"Write SimpleRegex compatible expressions and use token constraints in your regular expression extraction specifications whenever possible. Write regular expressions that are simple and easy to understand, and do not use regular expressions as a substitute for dictionaries.","title":"Follow the guidelines for regular expressions"},{"location":"ana_txtan_extract-perform/#write-simpleregex-compatible-expressions","text":"There are two regular expression engines that are used by the SystemT runtime component, JavaRegex and SimpleRegex. The JavaRegex engine is slower but allows for more advanced processing. The SimpleRegex engine is a fast regular expression evaluation engine, but it supports a slightly limited class of expressions. Since the SimpleRegex engine is more scalable than the Java\u2122 engine, a good practice is to try to write SimpleRegex compatible expressions whenever possible. The SystemT engine automatically chooses the suitable Regex engine for running a given regular expression. Most expressions are SimpleRegex compatible. Supported SimpleRegex constructs are: Alternation Sequence Parentheses Basic quantifiers such as +, *, ?, {m,n} Unicode character classes Standard built-ins such as \\d, \\s A regular expression is not SimpleRegex compatible if at least one of the following is true: Uses one of these flags: CANON_EQ, COMMENTS, LITERAL, UNICODE, UNIX_LINES Contains look-ahead or look-behind constructs such as \\A, \\Z, \\b, ^, $, (?=X), (?!X), (?<=X), or (?<!X) Contains capturing groups (such as, return group 1 as \u2026) The AQL compiler flags a warning for all of the regular expressions that are not SimpleRegex compatible. If you encounter such a warning, try to determine if you can simplify your regular expression to make it SimpleRegex compatible while you preserve the semantics of your extractor. As an example, consider this statement for extracting numbers with optional decimals. The statement uses the \\b regex character class to indicate that the regular expression matching must be done on tokens considering word boundaries. create view NumberUsingJavaRegex as extract regex /\\b\\d+(\\.\\d+)?\\b/ on D.text as match from Document D; This statement matches stand-alone numbers such as 10.99 in the phrase \u201cI purchased something for 10.99 USD\u201d, but it does not match numbers that are part of a larger word such as \u201820\u2019 in the phrase \u201cI just bought something for USD20\u201d. Alternatively, you can rewrite the regular expression to use SimpleRegex: create view NumberUsingSimpleRegex as extract regex /\\d+(\\.\\d+)?/ on D.text as match from Document D; In this example, the word boundary \\b characters were removed. As a result, the regular expression is compatible with the SimpleRegex engine, but the semantics are slightly different from the original. The new expression \\d+(\\.\\d+)? matches both the numbers '10.99\u2019 and \u201820\u2019 from the phrases \u201cI purchased something for 10.99 USD\u201d, and \"I just bought something for USD20\u201d. However, the transformation might be acceptable if the number \u201820\u2019 is not needed in the final result (perhaps due to other constructs used downstream in parts of your AQL code). If it is important to preserve the semantics of the original regular expression while using a regular expression that conforms to SimpleRegex, consider specifying token constraints in the extract regex statement.","title":"Write SimpleRegex compatible expressions"},{"location":"ana_txtan_extract-perform/#use-token-constraints-in-your-regular-expression-extraction-specifications","text":"The Optimizer performs Shared Regular Expression Matching (SRM), an optimization in which regular expressions that are both on token boundaries and are SimpleRegex compatible are grouped together and evaluated in one pass over the input text. Generally, this means that all regular expressions in the group are evaluated in approximately the same amount of time that it takes to individually evaluate the slowest expression in the group. When possible, write your regular expression to take advantage of the SRM optimization. The regular expression should be SimpleRegex compatible and have token constraints. Consider the same AQL example that uses a regular expression with word boundary character class \\b , and without token specifications: create view Number as extract regex /\\b\\d+(\\.\\d+)?\\b/ on D.text as match from Document D; Here is how you would rewrite it to both make it SimpleRegex compatible (by removing the \\b ) and amenable to SRM (by introducing the token constraints): create view NumberSRM as extract regex /\\d+(\\.\\d+)?/ on between 1 and 3 tokens in D.text as match from Document D;","title":"Use token constraints in your regular expression extraction specifications"},{"location":"ana_txtan_extract-perform/#use-dictionaries-instead-of-regular-expressions","text":"An AQL regular expression that matches a list of terms is usually more expensive to evaluate than the equivalent extract dictionary statement. The dictionary evaluator in SystemT is specialized for the particular case of matching large dictionaries of strings on token boundaries, and the SystemT Optimizer generates plans that evaluate multiple extract dictionary statements in a single pass. Therefore, when you are looking for mentions of a set of words or phrases, avoid writing your extractor as a regular expression consisting of an alternation of terms and use a dictionary instead. For example, if you are looking for mentions of currencies, do not write your statement using a regular expression: -- Not recommended; long alternation of terms should be a dictionary create view Currency as extract regex /dollar|sterling pound|cent|euro|rupee|yen|peso|rouble/ with flags 'CASE_INSENSITIVE' on D.text as match from Document D; In addition to the performance-related reasons for using a dictionary, this example also illustrates some additional common mistakes associated with misusing regular expressions: The extract regex statement will output matches that start or end in the middle of a word. For example, the statement will match the first four letters of the word \u201c cent er\u201d. The expression will match the phrase sterling pound only if there is exactly one space character between the two words. If the words sterling and pound appear in a document separated by a carriage return or by two spaces, the regular expression will not produce a match. Instead, this rewrite uses a dictionary of terms. Using a dictionary increases performance, makes the AQL code easier to maintain and understand, and the code will be less prone to bugs: -- Use a dictionary to match against same list of terms create dictionary CurrencyDict as('dollar', 'sterling pound', 'cent', 'euro', 'rupee', 'yen', 'peso', 'rouble'); create view CurrencyAlternative as extract dictionary 'CurrencyDict' on D.text as match from Document D;","title":"Use dictionaries instead of regular expressions"},{"location":"ana_txtan_extract-perform/#write-short-and-simple-regular-expressions","text":"Avoid writing long and complex regular expressions. In general, long and complex regular expressions make AQL code harder to understand and maintain. This type of regular expressions can be expensive to evaluate. Consider breaking a complex regular expression into smaller expressions, and combining the results of these expressions by using other AQL constructs. In the following example, a regular expression is used to identify one to three capitalized words. The words are separated by white space. -- Not recommended; long, complex regular expressions are difficult to maintain create view OneToThreeCapitalizedWords as extract regex /[A-Z][a-z]+(\\s+[A-Z][a-z]+){0,2}/ on D.text as match from Document D; The regular expression becomes harder to maintain as the subexpression for the capitalized word becomes more complex; for example, if it was to support Unicode characters. For ease of maintenance and understanding, you can break down the expression into smaller components. For example, you can use one regular expression with token constraints to find a single capitalized word, and the extract blocks statement to find blocks of one to three capitalized words: -- Same extraction in an efficient, simple manner -- A single capitalized word create view CapitalizedWords as extract regex /[A-Z][a-z]+/ on 1 token in D.text as word from Document D; -- One to three capitalized words - rewritten avoiding use of long, complex regular expression create view OneToThreeCapitalizedWordsBetter as extract blocks with count between 1 and 3 and separation 0 tokens on CW.word as capswords from CapitalizedWords CW consolidate on capswords; Alternatively, you can also use extract pattern to find one to three capitalized words to make it even simpler and more efficient: -- Same extraction using the 'extract pattern' statement -- One to three capitalized words create view OneToThreeCapitalizedWordsBest as extract pattern <CW.word>{1,3} as capswords from CapitalizedWords CW consolidate on capswords; In both cases, since the extract blocks and extract pattern statements return all matches, including the matches that overlap, the consolidate clause was used to remove the matches that overlap.","title":"Write short and simple regular expressions"},{"location":"ana_txtan_extract-perform/#watch-for-regular-expressions-that-match-an-empty-string","text":"A match on an empty string can create a serious performance problem. These matches can lead to unexpected results from your downstream AQL code, as well as performance bottlenecks (as many spans and tuples need to be created to hold the result). Here is an example of a regular expression that involves an alternation of two optional subexpressions, one of which is an empty string: -- Not recommended; the regular expression matches the empty string create view OptionalCurrency as extract regex /(\\$)?|(US dollar)?/ on D.text as match from Document D; In general, regular expressions that match the empty string are programming errors. The longer the regular expression, the easier it is to make such a mistake. Try to keep your regular expressions as simple as permitted by the semantics of your extractor by following the other regular expression guidelines.","title":"Watch for regular expressions that match an empty string"},{"location":"ana_txtan_extract-perform/#use-the-and-keyword-instead-of-the-and-built-in-predicate","text":"The Optimizer does not attempt to optimize the order of evaluation of the arguments to the And() predicate. A rule that uses the And() built-in predicate such as: select ... from ... where And(predicate1, predicate2); often runs considerably slower than the same rule in the form of: select ... from ... where predicate1 and predicate2; When possible, use the SQL-style and keyword instead of the And() predicate.","title":"Use the and keyword instead of the And() built-in predicate"},{"location":"ana_txtan_extract-perform/#avoid-cartesian-products","text":"Most often, the result of a Cartesian product is very large and takes a long time to compute. Cartesian products occur in statements where the from clause contains two or more views or tables, but some of them are not connected through a join predicate (in the where clause). The presence of a Cartesian product usually indicates that the join predicate was omitted by mistake. This is a problem that can be easily corrected. In the most simple case, a Cartesian product occurs when there are two views in the from clause, but there is no join predicate relating these two views in the where clause, as in this example: create dictionary PersonDict as ('Bill Clinton', 'Barack Obama', 'George Bush'); create view Person as extract dictionary 'PersonDict' on D.text as name from Document D; -- Extract U.S. phone numbers of the form '123-456-7890' create view Phone as extract regex /\\p{Nd}{3}\\-\\p{Nd}{3}\\-\\p{Nd}{4}/ on between 1 and 10 tokens in D.text as number from Document D; --Cartesian product between Person and Phone create view PersonPhone as select P.name, Ph.number from Person P, Phone Ph; The semantics of this AQL rule is to compute the set of all pairs of Person.name and Phone.number in the input document. If the input contains 1000 persons and 1000 phone numbers, the result consists of 1,000,000 pairs. However, it is unlikely that the intent was to compute all of these pairs. The intent likely was to find pairs of person and phone in close proximity of each other. This goal can be easily accomplished by adding a Follows or FollowsTok join predicate to the where clause: --With a join predicate create view PersonPhoneJoinWithPredicate as select P.name, Ph.number from Person P, Phone Ph where FollowsTok(P.name, Ph.number, 0, 5); Cartesian products can also occur in more subtle cases. In this example, the from clause consists of four views. However, only A and B, and respectively C and D, are connected by a join predicate. Hence, a Cartesian product occurs between the result of joining A and B and the result of joining C and D. create dictionary FirstNamesUSA as ('Bill', 'George', 'Barack'); create dictionary FirstNamesUK as ('William', 'George', 'Stewart'); create dictionary LastNamesUSA as ('Clinton', 'Bush', 'Obama'); create dictionary LastNamesUK as ('Clinton', 'Johnson', 'Vaughan'); create view A as extract dictionary 'FirstNamesUSA' on D.text as match from Document D; create view B as extract dictionary 'FirstNamesUK' on D.text as match from Document D; create view C as extract dictionary 'LastNamesUSA' on D.text as match from Document D; create view D as extract dictionary 'LastNamesUK' on D.text as match from Document D; --Cartesian product between A joined with B and C joined with D create view ABCD as select A.* from A, B, C, D where Equals(A.match ,B.match) and Equals(C.match ,D.match); To avoid Cartesian products, every view in the from clause must be connected to each of the other views by a single join predicate, or a chain of join predicates. These are some cases when Cartesian products between two views do not introduce a performance bottleneck. There are cases when the result of the Cartesian product is small, regardless of the documents on which you are using your extractor. An example is when both views contain at most a few tuples, or when one of the views has exactly one tuple. For example, the view Document always has a single tuple; therefore, AQL rules that involve a Cartesian product with the view Document (for example, to add additional metadata to an output view) do not introduce performance bottlenecks. create dictionary MergerDict as ('merged', 'Merged', 'merger', 'Merger', 'merging', 'Merging', 'M&A'); create view Merger as extract dictionary 'MergerDict' on D.text as mergeMention from Document D; --Cartesian product with the view Document is OK assuming the view Document has a single tuple or small number of tuples of dates create view DocumentMergerDate as select M.*, D.text as mergerDate from Document D, Merger M;","title":"Avoid Cartesian products"},{"location":"ana_txtan_extract-perform/#avoid-udfs-as-join-predicates","text":"When the Optimizer is not aware of the semantics of the UDF implementation, it must generate a Nested Loops Join plan that examines every input span. If possible, express the semantics of the UDF in a way that allows the Optimizer to suggest performance improvements. A user-defined function (UDF) can appear in the where clause of an AQL select statement as a join predicate. A join predicate is a predicate that determines whether a given pair of tuples from the from clause appears in the output of the view. Consider the following example, which identifies matching pairs of capitalized and lowercase names: create dictionary CapsNameDict as ('GEORGE', 'SAMUEL', 'PETER', 'HENRY'); create dictionary LowNameDict as ('samuel', 'johnson', 'david', 'george'); create function equalsNormalized (p1 Span, p2 Span) return Boolean external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!equalsNormalized' language java deterministic return null on null input; create function normalize (p1 String) return String external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!normalize' language java deterministic return null on null input; create view CapsName as extract dictionary 'CapsNameDict' on D.text as match from Document D; create view LowName as extract dictionary 'LowNameDict' on D.text as match from Document D; In this case, equalsNormalized() is a UDF function that takes as input two spans, and checks if their normalized values are equal. -- 'equalsNormalized' is a UDF that checks normalized values create view EqualNormalizedName as select C.match as caps, L.match as low from CapsName C, LowName L where equalsNormalized(L.match, C.match); When a select statement uses a UDF in this way, the Optimizer has limited options for compiling the statement. Since the Optimizer is not aware of the semantics of the UDF implementation, it must generate a Nested Loops Join plan that examines every pair of CapsName and LowName . As both of these views grow larger, this execution plan can become very expensive. In some cases, it might be possible to express the same semantics in a different way. In the case of the current example, the semantics of this rule can be obtained by using a slightly different UDF, called normalize(), that takes as input a span and outputs the normalized value of the span, together with the built-in predicate Equals() to compare the two normalized values: -- Avoid UDF as the join predicate and use built-in function to establish equality -- 'normalize', however is a UDF itself, although not used as the join predicate create view EqualNormalizedNameNoUDF as select C.match as caps, L.match as low from CapsName C, LowName L where Equals(normalize(GetString(L.match)), normalize(GetString(C.match))); In this case, the Optimizer can choose the Hash Join algorithm to implement the semantics of the Equals() predicate, which is usually much faster than using a Nested Loops Join operator.","title":"Avoid UDFs as join predicates"},{"location":"ana_txtan_extract-perform/#be-careful-when-using-or-as-a-join-predicate","text":"The Or() built-in predicate is run with the Nested Loops Join operator. In most cases, if the inputs are supported by faster join operators, it is worthwhile to split the statement into a union of multiple smaller statements. For example, consider this sample where Or() is run with the Nested Loops Join operator: -- not recommended create view CandidatesAll as select C.* from A, C where Or( FollowsTok(A.match, C.match, 0, 5), FollowsTok(C.match, A.match, 0, 5) ); The view can be rewritten into the union of two statements, which gives the Optimizer the opportunity to use faster join algorithms such as Adjacent Join or Sort Merge Join to implement the FollowsTok() predicates in the two select statements to allow the run time to employ faster join algorithms internally:: -- above view, rewritten to use union all statement create view CandidatesAllBetter as ( select C.* from A, C where FollowsTok(A.match, C.match, 0, 5) ) union all ( select C.* from A, C where FollowsTok(C.match, A.match, 0, 5) );","title":"Be careful when using Or() as a join predicate"},{"location":"ana_txtan_extract-perform/#use-the-consolidate-clause-wisely","text":"The consolidate clause is designed to be used at specific points in the extractor where overlapping matches are no longer useful downstream and can be removed. Do not use the consolidate clause more often than necessary, but take advantage of its functionality to filter your data. If you use the consolidate too often your AQL code can be harder to maintain since each and every statement has an extra consolidate clause. You can also introduce redundant sorting operations, although this is not usually a performance bottleneck. More importantly, if you do not use the consolidate clause enough, a large number of intermediate results can be carried downstream from one view to another, past the point where they are technically required by the semantics of the extractor. A large amount of unnecessary intermediate results can increase the memory footprint and slow extractor throughput. In this example that illustrates this point, the extractor is looking for mentions of department names, followed within a single token by a phone number. -- A single capitalized word create view UppercaseWord as extract regex /[A-Z]+/ on 1 token in D.text as word from Document D; -- Find organization names as one to two capitalized words create view Department as extract blocks with count between 1 and 2 and separation 0 tokens on UW.word as name from UppercaseWord UW; -- Find phone numbers of the form xxx-xxxx create view Phone as extract regex /d{3}-\\d{4}/ on 3 tokens in D.text as number from Document D; -- Find candidate pairs of department and phone, where the department is followed within one token by the phone create view DepartmentPhoneAll as select D.name, P.number, CombineSpans(D.name, P.number) as deptWithPhone from Department D, Phone P where FollowsTok(D.name, P.number, 1, 1); -- Consolidate to remove overlapping mentions followed within one token by the phone create view DepartmentPhone as select DP.* from DepartmentPhoneAll DP consolidate on DP.deptWithPhone; An extremely simplistic Department extractor of a single uppercase word or two uppercase words can help illustrate the issues. Assume the following sample is the input text: CS DEPARTMENT 555-1234 REGISTRAR 555-5678 The semantics of the extract block statement are to return all mentions, including those mentions that overlap. Therefore, the view Department contains the following spans: CS DEPARTMENT CS DEPARTMENT REGISTRAR The result of DepartmentPhoneAll is: CS, 555-1234 DEPARTMENT, 555-1234 CS DEPARTMENT, 555-1234 REGISTRAR, 555-5678 Notice that DepartmentPhoneAll contains overlapping matches that are unnecessary at this point in the extractor. Those overlapping matches are removed by the consolidate clause in the final view DepartmentPhone , to return this desired result: CS DEPARTMENT, 555-1234 REGISTRAR, 555-5678 However, the overlapping Department spans are probably not needed when you are looking for corresponding phones. The spans result in extra tuples that are created in DepartmentPhoneAll and carried downstream to DepartmentPhone , only to be removed later on by the consolidate clause. Instead, the unnecessary Department partial spans can be removed earlier, in the Department view: -- Find organization names as one to two capitalized words -- Use the consolidate clause to remove unnecessary overlapping mentions create view DepartmentBetter as extract blocks with count between 1 and 2 and separation 0 tokens on UW.word as name from UppercaseWord UW consolidate on name;","title":"Use the consolidate clause wisely"},{"location":"ana_txtan_extract-perform/#define-common-subpatterns-explicitly","text":"Define a view that computes common subpatterns so that the Optimizer does not need to evaluate each pattern. The Optimizer does not currently perform common subexpression elimination for sequence pattern expressions. Therefore, subpatterns that appear in multiple pattern expressions are evaluated multiple times. In the following code snippet, the common subexpression <A.match> <B.match> is computed twice; once for Pattern1 , and again for Pattern2 : -- subexpression <A.match> and <B.match> computed twice; once each for Pattern1 and Pattern2 create view Pattern1 as extract pattern <A.match> <B.match> <C.match> as match from A A, B B, C C; create view Pattern2 as extract pattern <A.match> <B.match> <D.match> as match from A A, B B, D D; If you find that multiples of your pattern expressions use a common subpattern, it is a good idea to explicitly define a view that computes the subpattern. You can reuse that view when you define the larger patterns. In this example, the view SubPattern explicitly defines <A.match> <B.match> . This is then reused in the views Pattern1 and Pattern2 : -- Define the common subpattern in its own view so not computed unnecessarily create view SubPattern as extract pattern <A.match> <B.match> as match from A A, B B; create view Pattern3 as extract pattern <AB.match> <C.match> as match from SubPattern AB, C C; create view Pattern4 as extract pattern <AB.match> <D.match> as match from SubPattern AB, D D;","title":"Define common subpatterns explicitly"},{"location":"ana_txtan_extract-perform/#step-3-focus-on-writing-aql-instead-of-tuning-performance","text":"You can usually rely on the Optimizer to choose an efficient plan, so do not tune performance while you are writing AQL. The SystemT engine has a sophisticated cost-based Optimizer, so you can concentrate on writing AQL rules that are easy to understand and easy to maintain. Tuning your AQL to force a particular plan can be counter-productive The best execution plan is often not obvious, especially when you are considering an extractor that might consist of multiple different modules written by different developers. Tuning AQL rules to produce a particular execution plan that seems optimal can be counter-productive. This tuning always makes the extractor harder to maintain, and it often makes the extractor slower. Write AQL for ease of maintenance and understanding Aside from following the basic AQL guidelines, in most cases you will have better performance, as well as improved accuracy and maintainability, if you write AQL rules for ease of maintenance and understanding. Let the Optimizer do its job, then check to make sure that the Optimizer chose the most efficient plan by using the AQL Profiler.","title":"Step 3: Focus on writing AQL instead of tuning performance"},{"location":"ana_txtan_extract-perform/#step-4-use-the-aql-profiler-to-find-problems","text":"Sometimes there is not an efficient plan for the particular semantics of the AQL expressions. The AQL Profiler is designed to help you quickly zero in on these problem cases. The AQL Profiler is included in the SystemT low-level Java API (class com.ibm.avatar.api.AQLProfiler) and is accessible in the AQL Developer Tooling (Eclipse-based) by right clicking on your AQL project > Profile as > Text Analytics Profiler Configuration . The AQL Profiler is a sampling profiler for SystemT operator graphs. While one thread runs the extractor over a collection of documents, a second thread periodically samples the state of the first thread, or what the operator is running at that moment. After the extraction thread is done running, the AQL Profiler sums up all of these samples to produce a breakdown of what views and operators account for what percentage of overall run time. This approach gives an accurate picture of the run time of various views and operators, as long as the AQL Profiler is allowed to run for a few tens of seconds. The SystemT low-level Java APIs describe how to start the AQL Profiler: You provide a collection of example documents to use as a target of extraction during profiling. The AQL Profiler runs on the set of documents that you specified. The documents should represent the kinds of documents that will be used in the end-to-end application, but you do not need a large document set. The AQL Profiler loops over the document collection until it has run for the specified minimum number of seconds. It is recommended to run the AQL Profiler for at least 30 seconds, to ensure the JVM has sufficient time to optimize byte code. This ensures the profiler results are realistic and as expected in production mode when usually an extractor is loaded once, and subsequently used to process many documents over a long period of time. Hot Views Hot views are views within the compiled plans that are responsible for the largest fraction of execution time. The views are sorted from least to most expensive. The view at the lower portion of the table is the most expensive and it is the best target for any hand-tuning efforts. Sequence patterns and subqueries Sequence patterns are implemented via an AQL to AQL rewrite. Each sequence pattern turns into one or more AQL views. Each extract pattern statement is turned into a collection of internal views that are based on select and extract statements before they are fed to the Optimizer. You will see these view names in the operator graph plan and the AQL Profiler output. The naming convention for sequence patterns is: ```bash _[original view name]_TmpView_[pattern]__[number] ``` For example: ``` _AmountWithUnit_TmpView_<N.match> <U.match>__3 ``` A similar approach applies to subqueries in the from clause of a `select` statement. Hot Documents Hot documents are documents that took unusually long amounts of time to process during the profiling run. The Optimizer attempts to produce plans that are not sensitive to the input documents, but sometimes such plans do not exist. For example, there might be a regular expression that happens to run slowly on particular input strings, or an AQL view can require the extractor to output a view whose size is very large for certain documents. If you find a problem document, you can create a document collection with just that document and run the Profiler again. The first document in the collection often comes to the top of this list, because the \u201cjust in time\u201d Java\u2122 compiler might be generating object code at the same time that it is processing this document.","title":"Step 4: Use the AQL Profiler to find problems"},{"location":"ana_txtan_extract-perform/#step-5-tune-the-most-expensive-view-first","text":"Focus your performance tuning on views that are heavily represented in the AQL Profiler output. A standard performance tuning practice is that if a view is taking up 10% of the execution time, even if you drop its running time to zero, the overall extractor will only get 10% faster. When you tune for performance it impacts readability and ease of maintenance, so be selective. Use Amdahl's law Amdahl\u2019s Law states that the maximum possible speed enhancement from tuning a view is inversely proportional to what fraction of the execution time that the view represents. If a view takes 5% of the execution time and you make it 100 times faster, the extractor becomes about 1.05 times faster. If a view takes 90% of the execution time and you make it 100 times faster, the extractor becomes about 9.2 times faster. Concentrate efforts on the most expensive view Concentrate any tuning efforts on the most expensive view. Each time you make a change to improve performance, it is important to rerun the AQL Profiler and to check whether the view that you are working on is still the most expensive. If another view moves to the top of the list, switch to working on that view. After each round of tuning, rerun the AQL Profiler and repeat This process should continue until either the throughput goals of the application are met or all views take approximately the same amount of time to process.","title":"Step 5: Tune the most expensive view first"},{"location":"ana_txtan_extract-perform/#examples-solving-common-performance-problems","text":"The main goal of tuning an AQL view is to identify the reason that the view is running slowly, and then fix that problem. Once you determine which view to tune for performance, you can focus on tuning that view. These scenarios illustrate regular causes of performance problems, from most common to least common, and explain ways to fix these performance problems.","title":"Examples: Solving common performance problems"},{"location":"ana_txtan_extract-perform/#did-not-follow-an-aql-guideline-correct-the-aql","text":"The most common performance tuning scenario is the need to correct a case where the basic AQL writing guidelines were not followed. In this example, word boundary checks ( \\b ) in the regular expression prevent Regular Expression Strength Reduction and Shared Regular Expression Matching (RSR/SRM) optimization. These are the most powerful regular expression optimizations. create view Number as extract regex /\\b\\d+\\b/ on D.text as match from Document D; This problem is easily corrected by modifying the view so that it conforms to the guideline. In this case, you can rewrite the extract regex statement to use the on 1 token clause in place of the word boundary checks. This change enables both RSR/SRM to be applied, without changing the results of the view. You can convert the regular expression to an RSR/SRM compatible view that produces the same result. create view Number as extract regex /\\d+/ on 1 token in D.text as match from Document D;","title":"Did not follow an AQL guideline: Correct the AQL"},{"location":"ana_txtan_extract-perform/#optimizer-could-not-apply-an-optimization-refine-the-view-definition-so-that-optimization-applies","text":"Another common scenario occurs when the Optimizer is unable to apply an optimization due to unnecessary constraints built into the structure of the AQL rules. In this example, the extractor identifies expressions of sentiment that apply to particular IBM\u00ae products. The rules are composed of two parts: a very complex and expensive sentiment extractor, and a simple dictionary evaluation that looks for product names: -- Table containing <name, sentiment> pairs of well-known software products create table SentimentTable (name Text, sentiment Text) as values ('Cognos', 'positive'), ('QuickBooks', 'neutral'), ('Office 365', 'positive'), ('Lotus Notes', 'neutral'), ('DB2', 'positive'), ('SQL Server', 'neutral'), ('BigInsights', 'positive'), ('Photoshop', 'positive'), ('Watson', 'positive'); create dictionary IBMProductNameDict as ( 'DB2', 'BigInsights', 'Watson'); -- Classify mentions of products as positive or negative create view ProductPairs as select S.name as name, S.sentiment as sentiment from SentimentTable S where MatchesDict('IBMProductNameDict', S.name); The Optimizer cannot apply the most productive optimizations because of the way the AQL was written. This issue often happens because there are optimizations that occur behind the scenes that conflict with an attempt to hand-tune the AQL for performance while the rules are being written. In this case, the constraint is expressed that only sentiments about IBM products are relevant as an explicit filtering condition over the output tuples of the Sentiment view. There is only one plan that the Optimizer can generate that completely implements the semantics of the view; the Optimizer must evaluate the entire sentiment extractor all of the time, then run the results through a Select operator that filters the sentiments that do not apply to the target products. This plan leads to wasted work, particularly if most of the documents do not contain a product name. You can rewrite this view to give the Optimizer more leeway to choose an efficient plan. Take that dictionary match that was expressed as a filtering condition (a MatchesDict() predicate), and rewrite it into a join operator with the results of an extract dictionary statement. The result is a set of rules that has almost the same low-level semantics, and from the perspective of the application the semantics are exactly the same. But the Optimizer can now apply Conditional Evaluation to skip sentiment analysis entirely on documents that do not contain a product name. -- Table containing <name, sentiment> pairs of well-known software products create table SentimentTable (name Text, sentiment Text) as values ('Cognos', 'positive'), ('QuickBooks', 'neutral'), ('Office 365', 'positive'), ('Lotus Notes', 'neutral'), ('DB2', 'positive'), ('SQL Server', 'neutral'), ('BigInsights', 'positive'), ('Photoshop', 'positive'), ('Watson', 'positive'); create dictionary IBMProductNameDict as ( 'DB2', 'BigInsights', 'Watson'); create view IBMProductName as extract dictionary 'IBMProductNameDict' on D.text as name from Document D; -- Classify mentions of products as positive or negative create view ProductPairs as select S.name as name, S.sentiment as sentiment from SentimentTable S, IBMProductName I where Equals(S.name, I.name);","title":"Optimizer could not apply an optimization: Refine the view definition so that optimization applies"},{"location":"ana_txtan_extract-perform/#optimizer-made-a-mistake-rework-the-view-definition-to-force-the-correct-plan","text":"Occasionally, the Optimizer makes a mistake. The Optimizer chooses its plan that is based on a cost model, and this model is not perfect. For example, the cost model currently assumes that all dictionary evaluations are faster than all regular expressions. But in some cases, such as the following example, this assumption is not true. create dictionary TheDict as ('the'); create view The as extract dictionary 'TheDict' on D.text as match from Document D; create view Nth as extract regex /\\d+th/ on 1 token in D.text as match from Document D; -- Find phrases like \"the 10th\" create view TheNth as select CombineSpans(T.match, N.match) as match from The T, Nth N where FollowsTok(T.match, N.match,0,0); In this case, evaluating a dictionary that contains the common word the is fairly expensive because that word produces a large number of matches, and it takes time to construct a tuple for each match. The regular expression in this example is small and straightforward. It produces very few matches on most documents, so evaluating this regular expression is likely to be less expensive than running the TheDict dictionary. For the overall extractor, which looks for phrases like the 10th , the Optimizer always incorrectly chooses to evaluate the dictionary and to use conditional evaluation or Restricted Span Evaluation (RSE) to reduce the time that is spent on the regular expression. The alternate plan, to evaluate the regular expression first, is actually the one that the Optimizer should choose. This mistake can be corrected by rewriting the AQL so that the Optimizer has no choice but to choose the \u201ccorrect\u201d plan. Basically, the modification is the inverse of the previous example. Take the join between the views The and NTh and replace that join with a filtering condition on the contents of the Nth view. This rewrite forces the compiler to generate an execution plan that uses RSE to run the TheDict dictionary only on selected portions of each document. create dictionary TheDict as ('the'); create view Nth as extract regex /\\d+th/ on between 1 and 1 tokens in D.text as match from Document D; -- Find phrases like \"the 10th\" create view TheNth as select CombineSpans ( LeftContextTok(N.match, 1), N.match ) as match from Nth N where MatchesDict('TheDict', LeftContextTok(N.match, 1)); Important: These rewrites harm maintainability and can make the extractor slower if you are not careful. Rework views in this way only if the Profiler indicates that there is a serious performance problem.","title":"Optimizer made a mistake: Rework the view definition to force the correct plan"},{"location":"ana_txtan_extract-perform/#no-efficient-plan-exists-reevaluate-the-semantics-of-the-extractor","text":"A relatively uncommon performance tuning scenario is when there is no possible efficient plan for a given set of AQL rules. Here is an example of how this kind of scenario can occur: create view SentenceBoundary as extract D.text as text, regex /(([\\.\\?!]+\\s)|(\\n\\s*\\n))/ on D.text as boundary from Document D; create view Sentence as extract split using S.boundary retain right split point on S.text as sentence from SentenceBoundary S; create dictionary ProductNameDict as ( 'DB2', 'BigInsights', 'Watson'); create view ProductName as extract dictionary 'ProductNameDict' on D.text as match from Document D; -- Find pairs of product names that occurred in the same sentence create view ProductPairs as select N1.match as name1, N2.match as name2 from ProductName N1, ProductName N2, Sentence S where FollowsTok(N1.match, N2.match, 0, 10) and Contains(S.sentence, N1.match) and Contains(S.sentence, N2.match) and Not(Equals(N1.match, N2.match)); In this example, the view Sentence splits the contents of the document into individual sentences. Imagine that the SentenceBoundary view is expensive because it contains an expensive regular expression. Another view, ProductPairs , looks for sentences that contain mentions of two different product names. Since the Sentence view uses the extract split statement, the join in the ProductPairs view must take the entire collection of sentences as input and identifying sentence boundaries can be expensive. The Optimizer can improve throughput by leveraging Conditional Evaluation to ensure that the Sentence view is only evaluated when the document contains at least one pair of product names. However, that entire view must still run even if the document contains only one product name pair. Basically, even the best possible plan is not very good. This problem can be corrected by rethinking the semantics of the extractor. Instead of looking for two product mentions within the same sentence, you can find two product names that do not have a sentence boundary between them. Convert the expensive regular expression for sentence boundaries into a filtering condition (the Not(MatchesRegex())), and the Optimizer can access an execution plan that avoids evaluating the expensive regular expression over most of the document. create dictionary ProductNameDict as ( 'DB2', 'BigInsights', 'Watson'); create view ProductName as extract dictionary 'ProductNameDict' on D.text as match from Document D; -- Find pairs of product names that occurred in the same sentence without splitting sentences create view ProductPairs as select N1.match as name1, N2.match as name2 from ProductName N1, ProductName N2 where FollowsTok(N1.match, N2.match,0,10) and Not(MatchesRegex(/(([\\.\\?!]+\\s)|(\\n\\s*\\n))/, SpanBetween(N1.match, N2.match)));s","title":"No efficient plan exists: Reevaluate the semantics of the extractor"},{"location":"ana_txtan_extractor-libraries/","text":"Pre-built extractor libraries AQL pre-built extractor libraries extract mentions of specific information from input text. You can also extend the views of these extractors to develop custom extractors. TODO : Describe the prebuilt AQL libraries available in Watson NLP and how to obtain them from Artifactory. Extractors for Generic Entities The extractors export the following views: Person Organization Location Address City Continent Country County DateTime EmailAddress NotesEmailAddress PhoneNumber StateOrProvince Town URL ZipCode CompanyEarningsAnnouncement AnalystEarningsEstimate CompanyEarningsGuidance Alliance Acquisition JointVenture Merger Note: When you import modules from the pre-built extractor library, or other extractor libraries, you need to resolve naming conflicts between the modules that you develop, and the modules that are being imported or pointed to. To resolve this issue, you must alter the names of the conflicting modules that you develop. The module names for the Generic Extractors are: Address EntitiesExport EntitiesOutput City CommonFeatures Continent County Date DateTime Dictionaries Disambiguation EmailAddress Facility FinancialAnnouncements FinancialDictionaries FinancialEvents InputDocumentProcessor Linguistics Location LocationCandidates NotesEmailAddress Organization OrganizationCandidates Person PersonCandidates PhoneNumber Region SentenceBoundary StateOrProvince Time Town UDFs URL WaterBody ZipCode Named entity extractors The pre-built named entity extractors can be used with general knowledge data such as news articles, news reports, news websites, and blogs. These extractors have high coverage for English, and limited coverage for German input documents. Financial extractors The pre-built financial extractors can be used with data such as finance reports, earnings reports, and analyst estimates. These extractors have coverage only for English. Generic extractors These pre-built extractors can be used to extract generic text and numeric information such as capital words or integers. These extractors have coverage for English input documents. Other extractors These pre-built extractors can be used to extract domain independent information such as dates, URLs, and emails. These extractors have coverage for English input documents. Sentiment extractors These pre-built extractors can be used to extract sentiment information from surveys or domain-independent content. These extractors have coverage for English input documents. Base modules Base modules provide the fundamental extractors that are used by all the above Named entity extractors The pre-built named entity extractors can be used with general knowledge data such as news articles, news reports, news websites, and blogs. These extractors have high coverage for English, and limited coverage for German input documents. The extractor libraries detag the data collection before extraction. To transform the extraction results, you can use the REMAP scalar function . There are primary and secondary attributes for these extractors. Extractor results for attributes are determined by the available information within the input document. Primary attributes are always populated, and are the main purpose of the analysis. Secondary attributes are populated whenever possible, but the extractor might not always return results. In some of the following extractor examples, extraction results for the attributes are shown with their corresponding span offset values ( [offset begin-offset end] ). Person The Person extractor identifies the mentions of person names. The primary attribute is the full mention of a name and secondary attributes are first name, last name, and middle initial of a person. The Person extractor can be used with documents in English and German. Output schema attribute Type Description firstname Span The first name part of a Person extraction. middlename Span The middle initial part of a Person extraction. lastname Span The last name part of a Person extraction person Span The person mentions as extracted. For example, consider the results of the extractor on the following text: Samuel J. Palmisano, IBM chairman, president and chief executive officer, said: \"IBM had a terrific quarter and a good year with record cash performance, profit and EPS, as well as record payouts to shareholders.\" The extraction results are: firstname: Samuel [0-6] middlename: J. [7-9] lastname: Palmisano [10-19] person:Samuel J. Palmisano [0-19] Organization The Organization extractor identifies the mentions of organization names, such as a company, government agency, military organization, school, or committee. The Organization extractor can be used with documents in English. Output schema attribute Type Description organization Span The organization mentions as extracted For example, consider the results of the extractor on the following text: IBM will help Citi explore how the Watson technology could help improve and simplify the banking experience. The extraction result is: organization: IBM [0-3], Citi [14-18] Location The Location extractor identifies the mentions of locations. The extracted information contains the mentions of addresses, cities, counties, countries, continents, states or provinces, and zip codes. This extractor cannot combine adjacent location names, identify types of locations, or postal or zip codes. Use the Address extractor for these extraction tasks. The Location extractor can be used with documents in English. Output schema attribute Type Description location Span The location mentions as extracted For example, consider the results of the extractor on the following text: Revenues from Europe/Middle East/Africa were $9.3 billion, up 11 percent(3 percent, adjusting for currency). The extraction result is: location: Europe [14-20], Middle East [21-32], Africa [33-39] Address This extractor identifies the mentions of U.S. postal addresses. Primary attributes are the street name and number, and a post office box if that information is available. Secondary attributes are the associated zip code and city and state information. Matches for a country are not included. Use the Location extractor to match general names of geographic locations. The Address extractor can be used with documents in English and German. Output schema attribute Type Description city Span The city part of the extraction. stateorprovince Span The state or province part of the extraction. zip Span The zip code part of the extraction. address Span The address mentions as extracted For example, consider the results of the extractor on the following text: For further information, please contact the corporate headquarters of IBM at: IBM Corporation 1 New Orchard Road Armonk, New York 10504-1722 United States 914-499-1900 The extraction results are: city: Armonk [118-124] stateorprovince: New York [126-134] zip: 10504-1722 [135-145] address: 1 New Orchard Road Armonk, New York 10504-1722 [98-145] City This extractor identifies the mentions of city names. Secondary attributes are the associated state, country, and continent of the city. Zip codes are not recognized. The City extractor can be used with documents in English and German. Output schema attribute Type Description city Span The city mentions as extracted. stateorprovince Text The state or province part of the extraction. country Text The country part of the extraction. continent Text The continent part of the extraction. For example, consider the results of the extractor on the following data: Their work is getting it's first real-world tryout with Fondazione IRCCS IstitutoNazionale dei Tumori , a public health institute in Milan, Italy, which specializes in the study and treatment of cancer. The extraction results are: city: Milan [133-138] stateorprovince: country: Italy [140-145] continent: Town This extractor identifies the mentions of town names when terms such as \u201ctown of\u201d are detected. This extractor is designed to recognize geographic locations, not postal addresses and zip codes. The Town extractor can be used with documents in English. Output schema attribute Type Description town Span The town mentions as extracted Consider this example: IBM is also working closely with the town of Littleton to build partnerships and engage community members. Last fall, IBM awarded Littleton Public Schools a $40,000 grant for IBM's Reading Companion program. The extraction result is: town: Littleton [45-64] County This extractor identifies the mentions of county names located in the United States. Secondary attributes are the associated state, country, and continent of the county if the information is available. County names that are part of an organization name such as Essex County Council are ignored. The County extractor can be used with documents in English. Output schema attribute Type Description county Span The county mentions as extracted stateorprovince Span The state or province part of the extraction. country Span The country part of the extraction. continent Span The continent part of the extraction. For example, consider the results of the extractor on the following text: Initial production at IBM's 300mm fab in East Fishkill has begun, with volume production set to begin at GlobalFoundries' new 300mm manufacturing facility in Saratoga County, NY. The extraction results are: county: Saratoga County [158-173] stateorprovince: country: continent: Country This extractor identifies the mentions of country names, such as Canada or Kuwait. The Country extractor can be used with documents in English. Output schema attribute Type Description country Span The country mentions as extracted continent Span The continent part of the extraction. For example, consider the results of the extractor on the following text: Revenues in the BRIC countries \u2014 Brazil, Russia, India and China \u2014 increased 19 percent (17 percent, adjusting for currency), and a total of 50 growth market countries had double-digit revenue growth. The extraction results return all four countries noted in the input data collection: country: Brazil [33-39], Russia [41-47], India [49-54], China [59-64] Continent This extractor identifies the mentions of the seven continents Asia, Africa, North America, South America, Antarctica, Europe, and Australia. The Continent extractor can be used with documents in English. Output schema attribute Type Description continent Span The continent mentions as extracted. For example, consider the results of the extractor on the following text: Strategic outsourcing signings up 20 percent worldwide, up 44 percent in North America. The extraction result is: continent: North America [73-87] StateOrProvince This extractor identifies the mentions of states or provinces. for many countries around the world. Abbreviations for state or province will be extracted when the country name immediately follows the abbreviation. Secondary attributes are the associated country and continent names. To include territories, add entries to the Additional State/Province Names list. Extracts the names of states or provinces The StateOrProvince extractor can be used with documents in English. Output schema attribute Type Description stateorprovince Span The state or province mentions as extracted. country Span The country mentions as extracted continent Span The continent part of the extraction. Consider this example: Baker is speaking at the upcoming IBM Finance Forum (May 9) and IBM Performance (May 10) events in Calgary, Alberta, Canada. The extraction results are: stateorprovince: Alberta [109-116] country: Canada [118-124] continent: Zipcode This extractor identifies the mentions of U.S. zip codes. Numeric post codes for other countries such as France might also be extracted. The Zipcode extractor can be used with documents in English. Output schema attribute Type Description zipcode Span The zip code mentions as extracted. For example, consider the results of the extractor on the following text: For further information, please contact the corporate headquarters of IBM at: IBM Corporation 1 New Orchard Road Armonk, New York 10504-1722 United States 914-499-1900 The extraction result is: zipCode: 10504-1722 [135-145] Financial extractors The pre-built financial extractors can be used with data such as finance reports, earnings reports, and analyst estimates. These extractors have coverage only for English. The extractor libraries detag the data collection before extraction. There are primary and secondary attributes for these extractors. Extractor results for attributes are determined by the available information within the input document. Primary attributes are always populated, and are the main purpose of the analysis. Secondary attributes are populated whenever possible, but the extractor might not always return results. In some of the following extractor examples, extraction results for the attributes are shown with their corresponding span offset values ( [offset begin-offset end] ). Acquisition The Acquisition extractor identifies the mentions of acquisition transactions. The primary attributes are the names of the companies that are involved in the acquisition. Secondary attributes are the respective stock symbols and the status of the acquisition. Output schema attribute Type Description company1 Span The first company part of the extractor. company1_detag Span The first company part in a detagged format. company1_text Text The first company part in a text format. company1_detag_text Text The first company part in a detagged text format. stockexchange1 Span The stock exchange of the first company part. stocksymbol1 Span The stock symbol of the first company part. company2 Span The second company part of the extractor. company2_detag Span The second company part in a detagged format. company2_text Text The second company part in a text format. company2_detag_text Text The second company part in a detagged text format. stockexchange2 Span The stock exchange of the second company part. stocksymbol2 Span The stock symbol of the second company part. company3 Span The third company part of the extractor. stockexchange3 Span The stock exchange of the third company part. stocksymbol3 Span The stock symbol of the third company part. date Text The date part of the extractor. datestring Text The date string part of the extractor. status Text The status part of the extractor. match Span The match part of the extractor. match_detag Span The match part in a detagged format. match_text Text The match part in a text format. match_detag_text Text The match part in a detagged text format. For example, consider the results of the Acquisition extractor on the following text: ARMONK, NY, - 22 Apr 2013: IBM (NYSE: IBM) today announced it has acquired UrbanCode Inc. Based in Cleveland, Ohio, UrbanCode automates the delivery of software, helping businesses quickly release and update mobile, social, big data, cloud applications. The extraction results are: company1: IBM [27-30] company1_detag: IBM [27-30] company1_text: IBM company1_detag_text: IBM stockexchange1: NYSE [32-36] stocksymbol1: IBM [38-41] company2: Urban Code, Inc.[75-89] company2_detag: Urban Code, Inc.[75-89] company2_text: Urban Code, Inc. company2_detag_text: Urban Code, Inc. stockexchange2: stocksymbol2: company3: stockexchange3: stocksymbol3: date: datestring: status: announced match: IBM (NYSE: IBM) today announced it has acquired UrbanCode Inc. [27-89] match_detag: IBM (NYSE: IBM) today announced it has acquired UrbanCode Inc. [27-89] match_text: IBM (NYSE: IBM) today announced it has acquired UrbanCode Inc. match_detag_text: IBM (NYSE: IBM) today announced it has acquired UrbanCode Inc. Alliance The Alliance extractor identifies the mentions of alliance agreements. The primary attributes are the names of the companies that are involved in the alliance. Secondary attributes are the respective stock symbols and the status of the alliance. Output schema attribute Type Description company1 Span The first company part of the extractor. company1_detag Span The first company part in a detagged format. company1_text Text The first company part in a text format. company1_detag_text Text The first company part in a detagged text format. stockexchange1 Span The stock exchange of the first company part. stocksymbol1 Span The stock symbol of the first company part. company2 Span The second company part of the extractor. company2_detag Span The second company part in a detagged format. company2_text Text The second company part in a text format. company2_detag_text Text The second company part in a detagged text format. stockexchange2 Span The stock exchange of the second company part. stocksymbol2 Span The stock symbol of the second company part. company3 Span The third company part of the extractor. stockexchange3 Span The stock exchange of the third company part. stocksymbol3 Span The stock symbol of the third company part. date Text The date part of the extractor. datestring Text The date string part of the extractor. status Text The status part of the extractor. match Span The match part of the extractor. match_detag Span The match part in a detagged format. match_text Text The match part in a text format. match_detag_text Text The match part in a detagged text format. For example, consider the results of the Alliance extractor on the following text: In December, ABC Insurance (NYSE: ABC) announced an alliance with SecondEast (NYSE: SEC) that likely influenced investor activity. The extraction results are: company1: ABC Insurance [13-26] company1_detag: ABC Insurance [13-26] company1_text: ABC Insurance company1_detag_text: ABC Insurance stockexchange1: NYSE [28-32] stocksymbol1: ABC [34-37] company2: SecondEast [66-76] company2_detag: SecondEast [66-76] company2_text: SecondEast company2_detag_text: SecondEast stockexchange2: NYSE [78-82] stocksymbol2: SEC [84-87] company3: stockexchange3: stocksymbol3: date: datestring: status: announced match: ABC Insurance (NYSE: ABC) announced an alliance with SecondEast (NYSE: SEC) [13-87] match_detag: ABC Insurance (NYSE: ABC) announced an alliance with SecondEast (NYSE: SEC) [13-87] match_text: ABC Insurance (NYSE: ABC) announced an alliance with SecondEast (NYSE: SEC) match_detag_text: ABC Insurance (NYSE: ABC) announced an alliance with SecondEast (NYSE: SEC) AnalystEarningsEstimate The AnalystEarningsEstimate extractor identifies the mentions of earnings estimates of companies that are issued by an analyst. The primary attributes are the name of the analyst, the analyst's company, the company whose estimates are being drawn and its stock symbol, and the financial metric on which the estimate is based. Secondary attributes are the quarter and year of the statement. Output schema attribute Type Description companysource Span The company source part of the extractor. personsource Text The person source part. companyrated Span The company rated part. companyrated_detag Span The company rated part in a detagged format. companyrated_text Text The company rated part in a text format. companyrated_detag_text Text The company rated part in a detagged text format. stockexchange Span The stock exchange of the company part. stocksymbol Span The stock symbol of the company part. year Text The year part of the extractor. quarter Text The quarter part of the extractor. financialmetricname Span The financial metric name part of the extractor. financialmetricestimate Span The financial metric estimate part. financialmetricpreviousestimate Text The financial metric previous estimate part. date Text The date part of the extractor. datestring Text The date string part of the extractor. match Span The match part of the extractor. match_detag Span The match part in a detagged format. match_text Text The match part in a text format. match_detag_text Text The match part in a detagged text format. CompanyEarningsAnnouncement The CompanyEarningsAnnouncement extractor identifies the mentions of earnings announcements that are released by a company. The primary attributes are the name of the company that makes the announcement, its stock symbol, the financial metric, and the financial metric value. Secondary attributes are the quarter and year for the announcement. Output schema attribute Type Description company Span The company part of the extractor. company_detag Span The company part in a detagged format. company_text Text The company part in a text format. company_detag_text Text The company part in a detagged text format. stockexchange Span The stock exchange of the company part. stocksymbol Span The stock symbol of the company part. year Span The year part of the extractor. quarter Text The quarter part of the extractor. financialmetricname Span The financial metric name part of the extractor. financialmetricname_detag Span The financial metric name part in a detagged format. financialmetricname_text Text The financial metric name part in a text format. financialmetricname_detag_text Text The financial metric name part in a detagged text format. financialmetricvalue Span The financial metric value part. financialmetricvalue_detag Span The financial metric value part in a detagged format. financialmetricvalue_text Text The financial metric value part in a text format. financialmetricvalue_detag_text Text The financial metric value part in a detagged text format. financialmetricestimate Span The financial metric estimate part. date Text The date part of the extractor. datestring Text The date string part of the extractor. match Span The match part of the extractor. match_detag Span The match part in a detagged format. match_text Text The match part in a text format. match_detag_text Text The match part in a detagged text format. CompanyEarningsGuidance The CompanyEarningsGuidance extractor identifies the mentions of earnings guidance that is issued by a company. The primary attributes are the name of the company that issues the guidance, its stock ticker, the financial metric, and value. Secondary attributes are the quarter and year for the guidance. Output schema attribute Type Description company Span The company part of the extractor. company_detag Span The company part in a detagged format. company_text Text The company part in a text format. company_detag_text Text The company part in a detagged text format. stockexchange Span The stock exchange of the company part. stocksymbol Span The stock symbol of the company part. year Text The year part of the extractor. quarter Text The quarter part of the extractor. financialmetricname Span The financial metric name part of the extractor. financialmetricname_detag Span The financial metric name part in a detagged format. financialmetricname_text Text The financial metric name part in a text format. financialmetricname_detag_text Text The financial metric name part in a detagged text format. financialmetricestimate Text The financial metric estimate part. financialmetricpreviousestimate Span The financial metric previous estimate part. financialmetricconsensusestimate Span The financial metric consensus estimate part. date Text The date part of the extractor. datestring Text The date string part of the extractor. match Span The match part of the extractor. match_detag Span The match part in a detagged format. match_text Text The match part in a text format. match_detag_text Text The match part in a detagged text format. JointVenture The JointVenture extractor identifies the mentions of joint venture transactions. The primary attributes are the names of companies that are involved in a joint venture. Secondary attributes are their respective stock symbols and the status of the joint venture. Output schema attribute Type Description company1 Span The first company part of the extractor. company1_detag Span The first company part in a detagged format. company1_text Text The first company part in a text format. company1_detag_text Text The first company part in a detagged text format. stockexchange1 Span The stock exchange of the first company part. stocksymbol1 Span The stock symbol of the first company part. company2 Span The second company part of the extractor. company2_detag Span The second company part in a detagged format. company2_text Text The second company part in a text format. company2_detag_text Text The second company part in a detagged text format. stockexchange2 Span The stock exchange of the second company part. stocksymbol2 Span The stock symbol of the second company part. company3 Span The third company part of the extractor. stockexchange3 Span The stock exchange of the third company part. stocksymbol3 Span The stock symbol of the third company part. companycreated Text The company created part. date Text The date part of the extractor. datestring Text The date string part of the extractor. status Text The status part of the extractor. match Span The match part of the extractor. match_detag Span The match part in a detagged format. match_text Text The match part in a text format. match_detag_text Text The match part in a detagged text format. For example, consider the results of the JointVenture extractor on the following text: Our financing and leasing assets include amounts related to the ABC Insurance (NYSE: ABC), and the SecondEast (NYSE: SEC) and Sample Outdoor Company joint venture programs. These amounts include receivables originated directly by ABC Insurance, as well as receivables purchased from joint venture entities. The extraction results are: company1: SecondEast [91-101] company1_detag: SecondEast [91-101] company1_text: SecondEast company1_detag_text: SecondEast stockexchange1: NYSE [103-107] stocksymbol1: SEC [109-112] company2: Sample Outdoor Company [119-141] company2_detag: Sample Outdoor Company [119-141] company2_text: Sample Outdoor Company company2_detag_text: Sample Outdoor Company stockexchange2: stocksymbol2: company3: stockexchange3: stocksymbol3: companycreated: date: datestring: status: match: SecondEast (NYSE: SEC) and Sample Outdoor Company joint venture [91-155] match_detag: SecondEast (NYSE: SEC) and Sample Outdoor Company joint venture [91-155] match_text: SecondEast (NYSE: SEC) and Sample Outdoor Company joint venture match_detag_text: SecondEast (NYSE: SEC) and Sample Outdoor Company joint venture Merger The Merger extractor identifies the mentions of merger transactions. The extracted primary attributes are the names of companies that are involved in a merger. Secondary attributes are their respective stock tickers and the status of the merger. Output schema attribute Type Description company1 Span The first company part of the extractor. company1_detag Span The first company part in a detagged format. company1_text Text The first company part in a text format. company1_detag_text Text The first company part in a detagged text format. stockexchange1 Span The stock exchange of the first company part. stocksymbol1 Span The stock symbol of the first company part. company2 Span The second company part of the extractor. company2_detag Span The second company part in a detagged format. company2_text Text The second company part in a text format. company2_detag_text Text The second company part in a detagged text format. stockexchange2 Span The stock exchange of the second company part. stocksymbol2 Text The stock symbol of the second company part. company3 Text The third company part of the extractor. stockexchange3 Text The stock exchange of the third company part. stocksymbol3 Span The stock symbol of the third company part. companycreated Text The company created part. date Text The date part of the extractor. datestring Text The date string part of the extractor. status Text The status part of the extractor. match Span The match part of the extractor. match_detag Span The match part in a detagged format. match_text Text The match part in a text format. match_detag_text Text The match part in a detagged text format. For example, consider the results of the Merger extractor on the following text: Noninterest expense was $25.2 million, an increase of 33.4% from $18.9 million in the third quarter of 2007, driven by the acquisition of ABC Insurance (NYSE: ABC)and charges related to the upcoming merger with XYZ Financial Bancorp (NYSE: XYZ)as well as branch expansion. The extraction results are: company1: ABC Insurance [138-151] company1_detag: ABC Insurance [138-151] company1_text: ABC Insurance company1_detag_text: ABC Insurance stockexchange1: NYSE [153-157] stocksymbol1: ABC [159-162] company2: XYZ Financial Bancorp [212-233] company2_detag: XYZ Financial Bancorp [212-233] company2_text: XYZ Financial Bancorp company2_detag_text: XYZ Financial Bancorp stockexchange2: NYSE [235-239] stocksymbol2: XYZ [241-244] company3: stockexchange3: stocksymbol3: companycreated: date: datestring: status: match: ABC Insurance (NYSE: ABC) and charges related to the upcoming merger with XYZ Financial Bancorp (NYSE: XYZ) [138-244] match_detag: ABC Insurance (NYSE: ABC) and charges related to the upcoming merger with XYZ Financial Bancorp (NYSE: XYZ) [138-244] match_text: ABC Insurance (NYSE: ABC) and charges related to the upcoming merger with XYZ Financial Bancorp (NYSE: XYZ) match_detag_text: ABC Insurance (NYSE: ABC) and charges related to the upcoming merger with XYZ Financial Bancorp (NYSE: XYZ) Generic extractors These pre-built extractors can be used to extract generic text and numeric information such as capital words or integers. These extractors have coverage for English input documents. CapsWord Matches any capitalized word including the optional symbols (.), (-), (&), and ('). Output schema attribute Type Description match Span The capitalized word including the optional symbols (.), (-), (&), and ('). For example, consider the results of the extractor on the following text: IBM and AT&T are participants in the \"SmartAmerica Challenge.\" The result is: match: IBM match: AT&T match: SmartAmerica match: Challenge CurrencyAmount Extracts a numeric currency amount and its preceding currency symbol. (Currency symbols are those defined in Unicode category \\p{Sc} .) Extracts a range of currency amounts when the numbers are separated by \"to\", for example, \"$8 to $45\" . Output schema attribute Type Description amountrange Span String corresponding to currency amount or a range of currency amounts. For example, consider the results of the extractor on the following text: Revenues from the Software segment were $6.5 billion, up 1 percent. The result is: amountrange: $6.5 billion Decimal Matches any decimal number. Decimal numbers include the decimal point and are expressed as numerals (not words). Output schema attribute Type Description decimal Span String corresponding to an decimal number. For example, consider the results of the extractor on the following text: 33 Attempts Off-Target; Tournament average 49.9 The result is: decimal: 49.9 FileName Matches file names with common extensions, except for those contain white space characters. Hundreds of common file extensions are supported, including those for text, data, audio, video, language, and compressed files. Output schema attribute Type Description name Span String corresponding to a file name. For example, consider the results of the extractor on the following text: hdfs:/path/output.json The result is: name: output.json FileNameExtension Matches common file extensions that begin with a period (for example, .exe). Matches file names with common extensions, except for those contain white space characters. Hundreds of common file extensions are supported, including those for text, data, audio, video, language, and compressed files. Output schema attribute Type Description extension Span String corresponding to a file name extension. For example, consider the results of the extractor on the following text: hdfs:/path/output.json The result is: extension: .json Integer Matches any integer number expressed in numerals (not words). Output schema attribute Type Description integerview Span String corresponding to an integer number. For example, consider the results of the extractor on the following text: SECTION 13 OR 15(d) The result is: integerview: 13 integerview: 15 Number Matches any integer or decimal number expressed in numerals (not words). Output schema attribute Type Description number Span String corresponding to a number. For example, consider the results of the extractor on the following text: SECTION 13 OR 15(d) The result is: number: 13 number: 15 Percentage Matches any percentage. Output schema attribute Type Description percentage Span String corresponding to a percentage number. For example, consider the results of the extractor on the following text: Revenues from the Software segment were $6.5 billion, up 1 percent. The result is: percentage: 1 percent Other extractors These pre-built extractors can be used to extract domain independent information such as dates, URLs, and emails. These extractors have coverage for English input documents. The extractor libraries detag the data collection before extraction. There are primary and secondary attributes for these extractors. Extractor results for attributes are determined by the available information within the input document. Primary attributes are always populated, and are the main purpose of the analysis. Secondary attributes are populated whenever possible, but the extractor might not always return results. In some of the following extractor examples, extraction results for the attributes are shown with their corresponding span offset values ( [offset begin-offset end] ). DateTime The DateTime extractor identifies the mentions of date and time. The day, month, year, hours, minutes, seconds, and time zone of the date and time are secondary attributes that might be included in the information. The DateTime uses patterns that are based on numeric and special symbols, English days of the week, and English names of the months. Output schema attribute Type Description datetime Span The datetime mention as extracted. For example, consider the results of the extractor on the following text: Income from continuing operations for the year ended December 31, 2007 was $10.4 billion compared with $9.4 billion in the year-ago period, an increase of 11 percent. The result is: datetime: December 31, 2007 [53-70] This example shows another output for this extractor: Tomorrow (all times Eastern): 9 AM: Get Bold! IBM VP and author Sandy Carter hosts a coffee talk and book signing at New York City's Birch Coffee. The result is: datetime: 9 AM [30-34] EmailAddress The EmailAddress extractor identifies the mentions of email addresses, which includes the local and the domain parts of the email address. Output schema attribute Type Description emailAddress Span The email address mentions as extracted. This example shows the EmailAddress extractor: IBM Easy Access For large enterprise, government and education customers. 888-839-9289 ews@us.ibm.com IBM Internal Contact : John Doe/Almaden/IBM The result for this extractor is: emailAddress : ews@us.ibm.com [90-104] NotesEmailAddress The NotesEmailAddress extractor identifies the mentions of Lotus Notes specific email addresses. The extracted information includes the local and domain parts of the Lotus Notes email address. Output schema attribute Type Description notesEmailAddress Span The Notes email address mentions as extracted. For example, consider the following text: IBM Easy Access For large enterprise, government and education customers. 888-839-9289 ews@us.ibm.com IBM Internal Contact : John Doe/Almaden/IBM This is the output for this extractor: notesEmailAddress : John Doe/Almaden/IBM [131-151] PhoneNumber The PhoneNumber extractor identifies the mentions of phone numbers. A secondary attribute is the type of phone number that is extracted, such as mobile, fax, or office. The PhoneNumber extractor can be used with documents in English and German. Output schema attribute Type Description phoneNumber Span The phone number mentions as extracted. For example, consider the following text: IBM Easy Access For large enterprise, government and education customers. 888-839-9289 ews@us.ibm.com IBM Internal Contact : John Doe/Almaden/IBM This is the output for this extractor: phoneNumber: 888-839-9289 [76-88] URL The URL extractor identifies mention of URL values. Secondary attributes are the host and protocol value. Output schema attribute Type Description url Span The URL mentions as extracted. For example, consider the following text: These materials are available on the IBM investor relations Web site at www.ibm.com/investor and are being included in Attachment II (\"Non-GAAP Supplementary Materials\")to the Form 8-K that includes this press release and is being submitted today to the SEC. The result of the URL extractor is: url: www.ibm.com/investor [76-92] Sentiment extractors These pre-built extractors can be used to extract sentiment information from surveys or domain-independent content. These extractors have coverage for English input documents. Before using sentiment extractors, set either a system property or an environment variable named TEXTANALYTICS_HOME to point to where the Text Analytics runtime RPM is installed on your cluster. By default, the RPM is installed at /usr/ibmpacks/current/text-analytics-runtime. Note: Sentiment extractors are only supported on Windows or Linux operating systems. Sentiment_Survey The Sentiment_Survey extractor extracts the expressions of sentiment from survey comments. Output schema attribute Type Description text Span Mention of the sentiment. patternName Textual Pattern used in detecting sentiment:- Target does positive Captures cases where the target performs a positive action. Example: The design improved . Target does negative Captures cases where the target performs a negative action. Example: The design worsened . | |clue|Span|Indicator of the presence of the sentiment.| |target|Span|Target object associated with the sentiment.| |polarity|Textual|Polarity of the sentiment: positive or negative.| For example, consider the results of the extractor on the following text: My manager is good at setting priorities. The result is: text: My manager is good at setting priorities. target: My manager polarity: positive patternName: Target is positive clue: good at setting priorities This example shows another output for this extractor: I am happy to be on this project. The result is: text: I am happy to be on this project. target: I polarity: positive patternName: Target is positive clue: happy to be on this project This example shows extraction of a negative sentiment: The email system should be improved. The result is: text: The email system should be improved. target: The email system polarity: negative patternName: Target is subjunctively positive clue: improved Sentiment_General The Sentiment_General extractor extracts the expressions of sentiment from textual content that is not specific to any domain. Output schema attribute Type Description text Span Mention of the sentiment. patternName Span Pattern used in detecting sentiment:- Target does positive Captures cases where the target performs a positive action. Example: The design improved . Target does negative Captures cases where the target performs a negative action. Example: The design worsened . | |clue|Span|Indicator of the presence of the sentiment.| |target|Span|Target object associated with the sentiment.| |polarity|Textual|Polarity of the sentiment: positive or negative.| For example, consider the results of the extractor on the following text: The food was great. The result is: text: The food was great. target: The food polarity: positive patternName: Target is positive clue: great This example shows another output for this extractor: I love my new phone. The result is: text: I love my new phone. target: I polarity: positive patternName: Speaker does positive on Target clue: love This example shows extraction of a negative sentiment: This is the worst thing that ever happened to me. The result is: text: This is the worst thing that ever happened to me. target: the worst thing that ever happened to me. polarity: negative patternName: Target has negative adjective clue: worst Base modules Base modules provide the fundamental extractors that are used by all other libraries above. The following are the base modules: InputDocumentProcessor This module performs the initial document pre-processing. This module shows the view that contains the input document content after it extracts relevant markup content from the document. CommonFeatures This module extracts features that multiple modules use to build their extraction semantics. Examples of such features include cities, states, countries, stock tickers, and more. Dictionaries This module contains all of the dictionaries that are used by other named entity modules. Examples of such dictionaries include names of cities, famous personalities, sports teams, organizations, and more. FinancialDictionaries This module contains all of the dictionaries that are used by other financial entity modules. Examples of such dictionaries include clues for identifying financial events, and clues for identifying relevant terms across financial reports. Linguistics This module contains syntactic features that some of the other modules use for their own rules. Disambiguation This module is used to avoid overlap of the same extraction that belongs to multiple unrelated entities. For example, this module is used to disambiguate extractions for the Person entity against extractions for the Organization entity. SentenceBoundary This module contains extraction logic to identify boundaries in sentences, such as boundaries that are in plain text content and boundaries that are within markup-like content. UDFs This module contains user-defined functions that are used by the rest of the modules, such as getAbbreviation and toUpperCase . Region, WaterBody, Facility These modules are used by some internal modules to identify regions such as national parks, oceans, seas, and facilities such as buildings and known constructions. EntitiesExport This module is the sink module for this library. All of the views that represent both named and financial entities are aggregated in this module from the rest of the modules for export purposes.","title":"Pre-built Extractors"},{"location":"ana_txtan_extractor-libraries/#pre-built-extractor-libraries","text":"AQL pre-built extractor libraries extract mentions of specific information from input text. You can also extend the views of these extractors to develop custom extractors. TODO : Describe the prebuilt AQL libraries available in Watson NLP and how to obtain them from Artifactory.","title":"Pre-built extractor libraries"},{"location":"ana_txtan_extractor-libraries/#extractors-for-generic-entities","text":"The extractors export the following views: Person Organization Location Address City Continent Country County DateTime EmailAddress NotesEmailAddress PhoneNumber StateOrProvince Town URL ZipCode CompanyEarningsAnnouncement AnalystEarningsEstimate CompanyEarningsGuidance Alliance Acquisition JointVenture Merger Note: When you import modules from the pre-built extractor library, or other extractor libraries, you need to resolve naming conflicts between the modules that you develop, and the modules that are being imported or pointed to. To resolve this issue, you must alter the names of the conflicting modules that you develop. The module names for the Generic Extractors are: Address EntitiesExport EntitiesOutput City CommonFeatures Continent County Date DateTime Dictionaries Disambiguation EmailAddress Facility FinancialAnnouncements FinancialDictionaries FinancialEvents InputDocumentProcessor Linguistics Location LocationCandidates NotesEmailAddress Organization OrganizationCandidates Person PersonCandidates PhoneNumber Region SentenceBoundary StateOrProvince Time Town UDFs URL WaterBody ZipCode Named entity extractors The pre-built named entity extractors can be used with general knowledge data such as news articles, news reports, news websites, and blogs. These extractors have high coverage for English, and limited coverage for German input documents. Financial extractors The pre-built financial extractors can be used with data such as finance reports, earnings reports, and analyst estimates. These extractors have coverage only for English. Generic extractors These pre-built extractors can be used to extract generic text and numeric information such as capital words or integers. These extractors have coverage for English input documents. Other extractors These pre-built extractors can be used to extract domain independent information such as dates, URLs, and emails. These extractors have coverage for English input documents. Sentiment extractors These pre-built extractors can be used to extract sentiment information from surveys or domain-independent content. These extractors have coverage for English input documents. Base modules Base modules provide the fundamental extractors that are used by all the above","title":"Extractors for Generic Entities"},{"location":"ana_txtan_extractor-libraries/#named-entity-extractors","text":"The pre-built named entity extractors can be used with general knowledge data such as news articles, news reports, news websites, and blogs. These extractors have high coverage for English, and limited coverage for German input documents. The extractor libraries detag the data collection before extraction. To transform the extraction results, you can use the REMAP scalar function . There are primary and secondary attributes for these extractors. Extractor results for attributes are determined by the available information within the input document. Primary attributes are always populated, and are the main purpose of the analysis. Secondary attributes are populated whenever possible, but the extractor might not always return results. In some of the following extractor examples, extraction results for the attributes are shown with their corresponding span offset values ( [offset begin-offset end] ).","title":"Named entity extractors"},{"location":"ana_txtan_extractor-libraries/#person","text":"The Person extractor identifies the mentions of person names. The primary attribute is the full mention of a name and secondary attributes are first name, last name, and middle initial of a person. The Person extractor can be used with documents in English and German. Output schema attribute Type Description firstname Span The first name part of a Person extraction. middlename Span The middle initial part of a Person extraction. lastname Span The last name part of a Person extraction person Span The person mentions as extracted. For example, consider the results of the extractor on the following text: Samuel J. Palmisano, IBM chairman, president and chief executive officer, said: \"IBM had a terrific quarter and a good year with record cash performance, profit and EPS, as well as record payouts to shareholders.\" The extraction results are: firstname: Samuel [0-6] middlename: J. [7-9] lastname: Palmisano [10-19] person:Samuel J. Palmisano [0-19]","title":"Person"},{"location":"ana_txtan_extractor-libraries/#organization","text":"The Organization extractor identifies the mentions of organization names, such as a company, government agency, military organization, school, or committee. The Organization extractor can be used with documents in English. Output schema attribute Type Description organization Span The organization mentions as extracted For example, consider the results of the extractor on the following text: IBM will help Citi explore how the Watson technology could help improve and simplify the banking experience. The extraction result is: organization: IBM [0-3], Citi [14-18]","title":"Organization"},{"location":"ana_txtan_extractor-libraries/#location","text":"The Location extractor identifies the mentions of locations. The extracted information contains the mentions of addresses, cities, counties, countries, continents, states or provinces, and zip codes. This extractor cannot combine adjacent location names, identify types of locations, or postal or zip codes. Use the Address extractor for these extraction tasks. The Location extractor can be used with documents in English. Output schema attribute Type Description location Span The location mentions as extracted For example, consider the results of the extractor on the following text: Revenues from Europe/Middle East/Africa were $9.3 billion, up 11 percent(3 percent, adjusting for currency). The extraction result is: location: Europe [14-20], Middle East [21-32], Africa [33-39]","title":"Location"},{"location":"ana_txtan_extractor-libraries/#address","text":"This extractor identifies the mentions of U.S. postal addresses. Primary attributes are the street name and number, and a post office box if that information is available. Secondary attributes are the associated zip code and city and state information. Matches for a country are not included. Use the Location extractor to match general names of geographic locations. The Address extractor can be used with documents in English and German. Output schema attribute Type Description city Span The city part of the extraction. stateorprovince Span The state or province part of the extraction. zip Span The zip code part of the extraction. address Span The address mentions as extracted For example, consider the results of the extractor on the following text: For further information, please contact the corporate headquarters of IBM at: IBM Corporation 1 New Orchard Road Armonk, New York 10504-1722 United States 914-499-1900 The extraction results are: city: Armonk [118-124] stateorprovince: New York [126-134] zip: 10504-1722 [135-145] address: 1 New Orchard Road Armonk, New York 10504-1722 [98-145]","title":"Address"},{"location":"ana_txtan_extractor-libraries/#city","text":"This extractor identifies the mentions of city names. Secondary attributes are the associated state, country, and continent of the city. Zip codes are not recognized. The City extractor can be used with documents in English and German. Output schema attribute Type Description city Span The city mentions as extracted. stateorprovince Text The state or province part of the extraction. country Text The country part of the extraction. continent Text The continent part of the extraction. For example, consider the results of the extractor on the following data: Their work is getting it's first real-world tryout with Fondazione IRCCS IstitutoNazionale dei Tumori , a public health institute in Milan, Italy, which specializes in the study and treatment of cancer. The extraction results are: city: Milan [133-138] stateorprovince: country: Italy [140-145] continent:","title":"City"},{"location":"ana_txtan_extractor-libraries/#town","text":"This extractor identifies the mentions of town names when terms such as \u201ctown of\u201d are detected. This extractor is designed to recognize geographic locations, not postal addresses and zip codes. The Town extractor can be used with documents in English. Output schema attribute Type Description town Span The town mentions as extracted Consider this example: IBM is also working closely with the town of Littleton to build partnerships and engage community members. Last fall, IBM awarded Littleton Public Schools a $40,000 grant for IBM's Reading Companion program. The extraction result is: town: Littleton [45-64]","title":"Town"},{"location":"ana_txtan_extractor-libraries/#county","text":"This extractor identifies the mentions of county names located in the United States. Secondary attributes are the associated state, country, and continent of the county if the information is available. County names that are part of an organization name such as Essex County Council are ignored. The County extractor can be used with documents in English. Output schema attribute Type Description county Span The county mentions as extracted stateorprovince Span The state or province part of the extraction. country Span The country part of the extraction. continent Span The continent part of the extraction. For example, consider the results of the extractor on the following text: Initial production at IBM's 300mm fab in East Fishkill has begun, with volume production set to begin at GlobalFoundries' new 300mm manufacturing facility in Saratoga County, NY. The extraction results are: county: Saratoga County [158-173] stateorprovince: country: continent:","title":"County"},{"location":"ana_txtan_extractor-libraries/#country","text":"This extractor identifies the mentions of country names, such as Canada or Kuwait. The Country extractor can be used with documents in English. Output schema attribute Type Description country Span The country mentions as extracted continent Span The continent part of the extraction. For example, consider the results of the extractor on the following text: Revenues in the BRIC countries \u2014 Brazil, Russia, India and China \u2014 increased 19 percent (17 percent, adjusting for currency), and a total of 50 growth market countries had double-digit revenue growth. The extraction results return all four countries noted in the input data collection: country: Brazil [33-39], Russia [41-47], India [49-54], China [59-64]","title":"Country"},{"location":"ana_txtan_extractor-libraries/#continent","text":"This extractor identifies the mentions of the seven continents Asia, Africa, North America, South America, Antarctica, Europe, and Australia. The Continent extractor can be used with documents in English. Output schema attribute Type Description continent Span The continent mentions as extracted. For example, consider the results of the extractor on the following text: Strategic outsourcing signings up 20 percent worldwide, up 44 percent in North America. The extraction result is: continent: North America [73-87]","title":"Continent"},{"location":"ana_txtan_extractor-libraries/#stateorprovince","text":"This extractor identifies the mentions of states or provinces. for many countries around the world. Abbreviations for state or province will be extracted when the country name immediately follows the abbreviation. Secondary attributes are the associated country and continent names. To include territories, add entries to the Additional State/Province Names list. Extracts the names of states or provinces The StateOrProvince extractor can be used with documents in English. Output schema attribute Type Description stateorprovince Span The state or province mentions as extracted. country Span The country mentions as extracted continent Span The continent part of the extraction. Consider this example: Baker is speaking at the upcoming IBM Finance Forum (May 9) and IBM Performance (May 10) events in Calgary, Alberta, Canada. The extraction results are: stateorprovince: Alberta [109-116] country: Canada [118-124] continent:","title":"StateOrProvince"},{"location":"ana_txtan_extractor-libraries/#zipcode","text":"This extractor identifies the mentions of U.S. zip codes. Numeric post codes for other countries such as France might also be extracted. The Zipcode extractor can be used with documents in English. Output schema attribute Type Description zipcode Span The zip code mentions as extracted. For example, consider the results of the extractor on the following text: For further information, please contact the corporate headquarters of IBM at: IBM Corporation 1 New Orchard Road Armonk, New York 10504-1722 United States 914-499-1900 The extraction result is: zipCode: 10504-1722 [135-145]","title":"Zipcode"},{"location":"ana_txtan_extractor-libraries/#financial-extractors","text":"The pre-built financial extractors can be used with data such as finance reports, earnings reports, and analyst estimates. These extractors have coverage only for English. The extractor libraries detag the data collection before extraction. There are primary and secondary attributes for these extractors. Extractor results for attributes are determined by the available information within the input document. Primary attributes are always populated, and are the main purpose of the analysis. Secondary attributes are populated whenever possible, but the extractor might not always return results. In some of the following extractor examples, extraction results for the attributes are shown with their corresponding span offset values ( [offset begin-offset end] ).","title":"Financial extractors"},{"location":"ana_txtan_extractor-libraries/#acquisition","text":"The Acquisition extractor identifies the mentions of acquisition transactions. The primary attributes are the names of the companies that are involved in the acquisition. Secondary attributes are the respective stock symbols and the status of the acquisition. Output schema attribute Type Description company1 Span The first company part of the extractor. company1_detag Span The first company part in a detagged format. company1_text Text The first company part in a text format. company1_detag_text Text The first company part in a detagged text format. stockexchange1 Span The stock exchange of the first company part. stocksymbol1 Span The stock symbol of the first company part. company2 Span The second company part of the extractor. company2_detag Span The second company part in a detagged format. company2_text Text The second company part in a text format. company2_detag_text Text The second company part in a detagged text format. stockexchange2 Span The stock exchange of the second company part. stocksymbol2 Span The stock symbol of the second company part. company3 Span The third company part of the extractor. stockexchange3 Span The stock exchange of the third company part. stocksymbol3 Span The stock symbol of the third company part. date Text The date part of the extractor. datestring Text The date string part of the extractor. status Text The status part of the extractor. match Span The match part of the extractor. match_detag Span The match part in a detagged format. match_text Text The match part in a text format. match_detag_text Text The match part in a detagged text format. For example, consider the results of the Acquisition extractor on the following text: ARMONK, NY, - 22 Apr 2013: IBM (NYSE: IBM) today announced it has acquired UrbanCode Inc. Based in Cleveland, Ohio, UrbanCode automates the delivery of software, helping businesses quickly release and update mobile, social, big data, cloud applications. The extraction results are: company1: IBM [27-30] company1_detag: IBM [27-30] company1_text: IBM company1_detag_text: IBM stockexchange1: NYSE [32-36] stocksymbol1: IBM [38-41] company2: Urban Code, Inc.[75-89] company2_detag: Urban Code, Inc.[75-89] company2_text: Urban Code, Inc. company2_detag_text: Urban Code, Inc. stockexchange2: stocksymbol2: company3: stockexchange3: stocksymbol3: date: datestring: status: announced match: IBM (NYSE: IBM) today announced it has acquired UrbanCode Inc. [27-89] match_detag: IBM (NYSE: IBM) today announced it has acquired UrbanCode Inc. [27-89] match_text: IBM (NYSE: IBM) today announced it has acquired UrbanCode Inc. match_detag_text: IBM (NYSE: IBM) today announced it has acquired UrbanCode Inc.","title":"Acquisition"},{"location":"ana_txtan_extractor-libraries/#alliance","text":"The Alliance extractor identifies the mentions of alliance agreements. The primary attributes are the names of the companies that are involved in the alliance. Secondary attributes are the respective stock symbols and the status of the alliance. Output schema attribute Type Description company1 Span The first company part of the extractor. company1_detag Span The first company part in a detagged format. company1_text Text The first company part in a text format. company1_detag_text Text The first company part in a detagged text format. stockexchange1 Span The stock exchange of the first company part. stocksymbol1 Span The stock symbol of the first company part. company2 Span The second company part of the extractor. company2_detag Span The second company part in a detagged format. company2_text Text The second company part in a text format. company2_detag_text Text The second company part in a detagged text format. stockexchange2 Span The stock exchange of the second company part. stocksymbol2 Span The stock symbol of the second company part. company3 Span The third company part of the extractor. stockexchange3 Span The stock exchange of the third company part. stocksymbol3 Span The stock symbol of the third company part. date Text The date part of the extractor. datestring Text The date string part of the extractor. status Text The status part of the extractor. match Span The match part of the extractor. match_detag Span The match part in a detagged format. match_text Text The match part in a text format. match_detag_text Text The match part in a detagged text format. For example, consider the results of the Alliance extractor on the following text: In December, ABC Insurance (NYSE: ABC) announced an alliance with SecondEast (NYSE: SEC) that likely influenced investor activity. The extraction results are: company1: ABC Insurance [13-26] company1_detag: ABC Insurance [13-26] company1_text: ABC Insurance company1_detag_text: ABC Insurance stockexchange1: NYSE [28-32] stocksymbol1: ABC [34-37] company2: SecondEast [66-76] company2_detag: SecondEast [66-76] company2_text: SecondEast company2_detag_text: SecondEast stockexchange2: NYSE [78-82] stocksymbol2: SEC [84-87] company3: stockexchange3: stocksymbol3: date: datestring: status: announced match: ABC Insurance (NYSE: ABC) announced an alliance with SecondEast (NYSE: SEC) [13-87] match_detag: ABC Insurance (NYSE: ABC) announced an alliance with SecondEast (NYSE: SEC) [13-87] match_text: ABC Insurance (NYSE: ABC) announced an alliance with SecondEast (NYSE: SEC) match_detag_text: ABC Insurance (NYSE: ABC) announced an alliance with SecondEast (NYSE: SEC)","title":"Alliance"},{"location":"ana_txtan_extractor-libraries/#analystearningsestimate","text":"The AnalystEarningsEstimate extractor identifies the mentions of earnings estimates of companies that are issued by an analyst. The primary attributes are the name of the analyst, the analyst's company, the company whose estimates are being drawn and its stock symbol, and the financial metric on which the estimate is based. Secondary attributes are the quarter and year of the statement. Output schema attribute Type Description companysource Span The company source part of the extractor. personsource Text The person source part. companyrated Span The company rated part. companyrated_detag Span The company rated part in a detagged format. companyrated_text Text The company rated part in a text format. companyrated_detag_text Text The company rated part in a detagged text format. stockexchange Span The stock exchange of the company part. stocksymbol Span The stock symbol of the company part. year Text The year part of the extractor. quarter Text The quarter part of the extractor. financialmetricname Span The financial metric name part of the extractor. financialmetricestimate Span The financial metric estimate part. financialmetricpreviousestimate Text The financial metric previous estimate part. date Text The date part of the extractor. datestring Text The date string part of the extractor. match Span The match part of the extractor. match_detag Span The match part in a detagged format. match_text Text The match part in a text format. match_detag_text Text The match part in a detagged text format.","title":"AnalystEarningsEstimate"},{"location":"ana_txtan_extractor-libraries/#companyearningsannouncement","text":"The CompanyEarningsAnnouncement extractor identifies the mentions of earnings announcements that are released by a company. The primary attributes are the name of the company that makes the announcement, its stock symbol, the financial metric, and the financial metric value. Secondary attributes are the quarter and year for the announcement. Output schema attribute Type Description company Span The company part of the extractor. company_detag Span The company part in a detagged format. company_text Text The company part in a text format. company_detag_text Text The company part in a detagged text format. stockexchange Span The stock exchange of the company part. stocksymbol Span The stock symbol of the company part. year Span The year part of the extractor. quarter Text The quarter part of the extractor. financialmetricname Span The financial metric name part of the extractor. financialmetricname_detag Span The financial metric name part in a detagged format. financialmetricname_text Text The financial metric name part in a text format. financialmetricname_detag_text Text The financial metric name part in a detagged text format. financialmetricvalue Span The financial metric value part. financialmetricvalue_detag Span The financial metric value part in a detagged format. financialmetricvalue_text Text The financial metric value part in a text format. financialmetricvalue_detag_text Text The financial metric value part in a detagged text format. financialmetricestimate Span The financial metric estimate part. date Text The date part of the extractor. datestring Text The date string part of the extractor. match Span The match part of the extractor. match_detag Span The match part in a detagged format. match_text Text The match part in a text format. match_detag_text Text The match part in a detagged text format.","title":"CompanyEarningsAnnouncement"},{"location":"ana_txtan_extractor-libraries/#companyearningsguidance","text":"The CompanyEarningsGuidance extractor identifies the mentions of earnings guidance that is issued by a company. The primary attributes are the name of the company that issues the guidance, its stock ticker, the financial metric, and value. Secondary attributes are the quarter and year for the guidance. Output schema attribute Type Description company Span The company part of the extractor. company_detag Span The company part in a detagged format. company_text Text The company part in a text format. company_detag_text Text The company part in a detagged text format. stockexchange Span The stock exchange of the company part. stocksymbol Span The stock symbol of the company part. year Text The year part of the extractor. quarter Text The quarter part of the extractor. financialmetricname Span The financial metric name part of the extractor. financialmetricname_detag Span The financial metric name part in a detagged format. financialmetricname_text Text The financial metric name part in a text format. financialmetricname_detag_text Text The financial metric name part in a detagged text format. financialmetricestimate Text The financial metric estimate part. financialmetricpreviousestimate Span The financial metric previous estimate part. financialmetricconsensusestimate Span The financial metric consensus estimate part. date Text The date part of the extractor. datestring Text The date string part of the extractor. match Span The match part of the extractor. match_detag Span The match part in a detagged format. match_text Text The match part in a text format. match_detag_text Text The match part in a detagged text format.","title":"CompanyEarningsGuidance"},{"location":"ana_txtan_extractor-libraries/#jointventure","text":"The JointVenture extractor identifies the mentions of joint venture transactions. The primary attributes are the names of companies that are involved in a joint venture. Secondary attributes are their respective stock symbols and the status of the joint venture. Output schema attribute Type Description company1 Span The first company part of the extractor. company1_detag Span The first company part in a detagged format. company1_text Text The first company part in a text format. company1_detag_text Text The first company part in a detagged text format. stockexchange1 Span The stock exchange of the first company part. stocksymbol1 Span The stock symbol of the first company part. company2 Span The second company part of the extractor. company2_detag Span The second company part in a detagged format. company2_text Text The second company part in a text format. company2_detag_text Text The second company part in a detagged text format. stockexchange2 Span The stock exchange of the second company part. stocksymbol2 Span The stock symbol of the second company part. company3 Span The third company part of the extractor. stockexchange3 Span The stock exchange of the third company part. stocksymbol3 Span The stock symbol of the third company part. companycreated Text The company created part. date Text The date part of the extractor. datestring Text The date string part of the extractor. status Text The status part of the extractor. match Span The match part of the extractor. match_detag Span The match part in a detagged format. match_text Text The match part in a text format. match_detag_text Text The match part in a detagged text format. For example, consider the results of the JointVenture extractor on the following text: Our financing and leasing assets include amounts related to the ABC Insurance (NYSE: ABC), and the SecondEast (NYSE: SEC) and Sample Outdoor Company joint venture programs. These amounts include receivables originated directly by ABC Insurance, as well as receivables purchased from joint venture entities. The extraction results are: company1: SecondEast [91-101] company1_detag: SecondEast [91-101] company1_text: SecondEast company1_detag_text: SecondEast stockexchange1: NYSE [103-107] stocksymbol1: SEC [109-112] company2: Sample Outdoor Company [119-141] company2_detag: Sample Outdoor Company [119-141] company2_text: Sample Outdoor Company company2_detag_text: Sample Outdoor Company stockexchange2: stocksymbol2: company3: stockexchange3: stocksymbol3: companycreated: date: datestring: status: match: SecondEast (NYSE: SEC) and Sample Outdoor Company joint venture [91-155] match_detag: SecondEast (NYSE: SEC) and Sample Outdoor Company joint venture [91-155] match_text: SecondEast (NYSE: SEC) and Sample Outdoor Company joint venture match_detag_text: SecondEast (NYSE: SEC) and Sample Outdoor Company joint venture","title":"JointVenture"},{"location":"ana_txtan_extractor-libraries/#merger","text":"The Merger extractor identifies the mentions of merger transactions. The extracted primary attributes are the names of companies that are involved in a merger. Secondary attributes are their respective stock tickers and the status of the merger. Output schema attribute Type Description company1 Span The first company part of the extractor. company1_detag Span The first company part in a detagged format. company1_text Text The first company part in a text format. company1_detag_text Text The first company part in a detagged text format. stockexchange1 Span The stock exchange of the first company part. stocksymbol1 Span The stock symbol of the first company part. company2 Span The second company part of the extractor. company2_detag Span The second company part in a detagged format. company2_text Text The second company part in a text format. company2_detag_text Text The second company part in a detagged text format. stockexchange2 Span The stock exchange of the second company part. stocksymbol2 Text The stock symbol of the second company part. company3 Text The third company part of the extractor. stockexchange3 Text The stock exchange of the third company part. stocksymbol3 Span The stock symbol of the third company part. companycreated Text The company created part. date Text The date part of the extractor. datestring Text The date string part of the extractor. status Text The status part of the extractor. match Span The match part of the extractor. match_detag Span The match part in a detagged format. match_text Text The match part in a text format. match_detag_text Text The match part in a detagged text format. For example, consider the results of the Merger extractor on the following text: Noninterest expense was $25.2 million, an increase of 33.4% from $18.9 million in the third quarter of 2007, driven by the acquisition of ABC Insurance (NYSE: ABC)and charges related to the upcoming merger with XYZ Financial Bancorp (NYSE: XYZ)as well as branch expansion. The extraction results are: company1: ABC Insurance [138-151] company1_detag: ABC Insurance [138-151] company1_text: ABC Insurance company1_detag_text: ABC Insurance stockexchange1: NYSE [153-157] stocksymbol1: ABC [159-162] company2: XYZ Financial Bancorp [212-233] company2_detag: XYZ Financial Bancorp [212-233] company2_text: XYZ Financial Bancorp company2_detag_text: XYZ Financial Bancorp stockexchange2: NYSE [235-239] stocksymbol2: XYZ [241-244] company3: stockexchange3: stocksymbol3: companycreated: date: datestring: status: match: ABC Insurance (NYSE: ABC) and charges related to the upcoming merger with XYZ Financial Bancorp (NYSE: XYZ) [138-244] match_detag: ABC Insurance (NYSE: ABC) and charges related to the upcoming merger with XYZ Financial Bancorp (NYSE: XYZ) [138-244] match_text: ABC Insurance (NYSE: ABC) and charges related to the upcoming merger with XYZ Financial Bancorp (NYSE: XYZ) match_detag_text: ABC Insurance (NYSE: ABC) and charges related to the upcoming merger with XYZ Financial Bancorp (NYSE: XYZ)","title":"Merger"},{"location":"ana_txtan_extractor-libraries/#generic-extractors","text":"These pre-built extractors can be used to extract generic text and numeric information such as capital words or integers. These extractors have coverage for English input documents.","title":"Generic extractors"},{"location":"ana_txtan_extractor-libraries/#capsword","text":"Matches any capitalized word including the optional symbols (.), (-), (&), and ('). Output schema attribute Type Description match Span The capitalized word including the optional symbols (.), (-), (&), and ('). For example, consider the results of the extractor on the following text: IBM and AT&T are participants in the \"SmartAmerica Challenge.\" The result is: match: IBM match: AT&T match: SmartAmerica match: Challenge","title":"CapsWord"},{"location":"ana_txtan_extractor-libraries/#currencyamount","text":"Extracts a numeric currency amount and its preceding currency symbol. (Currency symbols are those defined in Unicode category \\p{Sc} .) Extracts a range of currency amounts when the numbers are separated by \"to\", for example, \"$8 to $45\" . Output schema attribute Type Description amountrange Span String corresponding to currency amount or a range of currency amounts. For example, consider the results of the extractor on the following text: Revenues from the Software segment were $6.5 billion, up 1 percent. The result is: amountrange: $6.5 billion","title":"CurrencyAmount"},{"location":"ana_txtan_extractor-libraries/#decimal","text":"Matches any decimal number. Decimal numbers include the decimal point and are expressed as numerals (not words). Output schema attribute Type Description decimal Span String corresponding to an decimal number. For example, consider the results of the extractor on the following text: 33 Attempts Off-Target; Tournament average 49.9 The result is: decimal: 49.9","title":"Decimal"},{"location":"ana_txtan_extractor-libraries/#filename","text":"Matches file names with common extensions, except for those contain white space characters. Hundreds of common file extensions are supported, including those for text, data, audio, video, language, and compressed files. Output schema attribute Type Description name Span String corresponding to a file name. For example, consider the results of the extractor on the following text: hdfs:/path/output.json The result is: name: output.json","title":"FileName"},{"location":"ana_txtan_extractor-libraries/#filenameextension","text":"Matches common file extensions that begin with a period (for example, .exe). Matches file names with common extensions, except for those contain white space characters. Hundreds of common file extensions are supported, including those for text, data, audio, video, language, and compressed files. Output schema attribute Type Description extension Span String corresponding to a file name extension. For example, consider the results of the extractor on the following text: hdfs:/path/output.json The result is: extension: .json","title":"FileNameExtension"},{"location":"ana_txtan_extractor-libraries/#integer","text":"Matches any integer number expressed in numerals (not words). Output schema attribute Type Description integerview Span String corresponding to an integer number. For example, consider the results of the extractor on the following text: SECTION 13 OR 15(d) The result is: integerview: 13 integerview: 15","title":"Integer"},{"location":"ana_txtan_extractor-libraries/#number","text":"Matches any integer or decimal number expressed in numerals (not words). Output schema attribute Type Description number Span String corresponding to a number. For example, consider the results of the extractor on the following text: SECTION 13 OR 15(d) The result is: number: 13 number: 15","title":"Number"},{"location":"ana_txtan_extractor-libraries/#percentage","text":"Matches any percentage. Output schema attribute Type Description percentage Span String corresponding to a percentage number. For example, consider the results of the extractor on the following text: Revenues from the Software segment were $6.5 billion, up 1 percent. The result is: percentage: 1 percent","title":"Percentage"},{"location":"ana_txtan_extractor-libraries/#other-extractors","text":"These pre-built extractors can be used to extract domain independent information such as dates, URLs, and emails. These extractors have coverage for English input documents. The extractor libraries detag the data collection before extraction. There are primary and secondary attributes for these extractors. Extractor results for attributes are determined by the available information within the input document. Primary attributes are always populated, and are the main purpose of the analysis. Secondary attributes are populated whenever possible, but the extractor might not always return results. In some of the following extractor examples, extraction results for the attributes are shown with their corresponding span offset values ( [offset begin-offset end] ).","title":"Other extractors"},{"location":"ana_txtan_extractor-libraries/#datetime","text":"The DateTime extractor identifies the mentions of date and time. The day, month, year, hours, minutes, seconds, and time zone of the date and time are secondary attributes that might be included in the information. The DateTime uses patterns that are based on numeric and special symbols, English days of the week, and English names of the months. Output schema attribute Type Description datetime Span The datetime mention as extracted. For example, consider the results of the extractor on the following text: Income from continuing operations for the year ended December 31, 2007 was $10.4 billion compared with $9.4 billion in the year-ago period, an increase of 11 percent. The result is: datetime: December 31, 2007 [53-70] This example shows another output for this extractor: Tomorrow (all times Eastern): 9 AM: Get Bold! IBM VP and author Sandy Carter hosts a coffee talk and book signing at New York City's Birch Coffee. The result is: datetime: 9 AM [30-34]","title":"DateTime"},{"location":"ana_txtan_extractor-libraries/#emailaddress","text":"The EmailAddress extractor identifies the mentions of email addresses, which includes the local and the domain parts of the email address. Output schema attribute Type Description emailAddress Span The email address mentions as extracted. This example shows the EmailAddress extractor: IBM Easy Access For large enterprise, government and education customers. 888-839-9289 ews@us.ibm.com IBM Internal Contact : John Doe/Almaden/IBM The result for this extractor is: emailAddress : ews@us.ibm.com [90-104]","title":"EmailAddress"},{"location":"ana_txtan_extractor-libraries/#notesemailaddress","text":"The NotesEmailAddress extractor identifies the mentions of Lotus Notes specific email addresses. The extracted information includes the local and domain parts of the Lotus Notes email address. Output schema attribute Type Description notesEmailAddress Span The Notes email address mentions as extracted. For example, consider the following text: IBM Easy Access For large enterprise, government and education customers. 888-839-9289 ews@us.ibm.com IBM Internal Contact : John Doe/Almaden/IBM This is the output for this extractor: notesEmailAddress : John Doe/Almaden/IBM [131-151]","title":"NotesEmailAddress"},{"location":"ana_txtan_extractor-libraries/#phonenumber","text":"The PhoneNumber extractor identifies the mentions of phone numbers. A secondary attribute is the type of phone number that is extracted, such as mobile, fax, or office. The PhoneNumber extractor can be used with documents in English and German. Output schema attribute Type Description phoneNumber Span The phone number mentions as extracted. For example, consider the following text: IBM Easy Access For large enterprise, government and education customers. 888-839-9289 ews@us.ibm.com IBM Internal Contact : John Doe/Almaden/IBM This is the output for this extractor: phoneNumber: 888-839-9289 [76-88]","title":"PhoneNumber"},{"location":"ana_txtan_extractor-libraries/#url","text":"The URL extractor identifies mention of URL values. Secondary attributes are the host and protocol value. Output schema attribute Type Description url Span The URL mentions as extracted. For example, consider the following text: These materials are available on the IBM investor relations Web site at www.ibm.com/investor and are being included in Attachment II (\"Non-GAAP Supplementary Materials\")to the Form 8-K that includes this press release and is being submitted today to the SEC. The result of the URL extractor is: url: www.ibm.com/investor [76-92]","title":"URL"},{"location":"ana_txtan_extractor-libraries/#sentiment-extractors","text":"These pre-built extractors can be used to extract sentiment information from surveys or domain-independent content. These extractors have coverage for English input documents. Before using sentiment extractors, set either a system property or an environment variable named TEXTANALYTICS_HOME to point to where the Text Analytics runtime RPM is installed on your cluster. By default, the RPM is installed at /usr/ibmpacks/current/text-analytics-runtime. Note: Sentiment extractors are only supported on Windows or Linux operating systems.","title":"Sentiment extractors"},{"location":"ana_txtan_extractor-libraries/#sentiment_survey","text":"The Sentiment_Survey extractor extracts the expressions of sentiment from survey comments. Output schema attribute Type Description text Span Mention of the sentiment. patternName Textual Pattern used in detecting sentiment:- Target does positive Captures cases where the target performs a positive action. Example: The design improved . Target does negative Captures cases where the target performs a negative action. Example: The design worsened . | |clue|Span|Indicator of the presence of the sentiment.| |target|Span|Target object associated with the sentiment.| |polarity|Textual|Polarity of the sentiment: positive or negative.| For example, consider the results of the extractor on the following text: My manager is good at setting priorities. The result is: text: My manager is good at setting priorities. target: My manager polarity: positive patternName: Target is positive clue: good at setting priorities This example shows another output for this extractor: I am happy to be on this project. The result is: text: I am happy to be on this project. target: I polarity: positive patternName: Target is positive clue: happy to be on this project This example shows extraction of a negative sentiment: The email system should be improved. The result is: text: The email system should be improved. target: The email system polarity: negative patternName: Target is subjunctively positive clue: improved","title":"Sentiment_Survey"},{"location":"ana_txtan_extractor-libraries/#sentiment_general","text":"The Sentiment_General extractor extracts the expressions of sentiment from textual content that is not specific to any domain. Output schema attribute Type Description text Span Mention of the sentiment. patternName Span Pattern used in detecting sentiment:- Target does positive Captures cases where the target performs a positive action. Example: The design improved . Target does negative Captures cases where the target performs a negative action. Example: The design worsened . | |clue|Span|Indicator of the presence of the sentiment.| |target|Span|Target object associated with the sentiment.| |polarity|Textual|Polarity of the sentiment: positive or negative.| For example, consider the results of the extractor on the following text: The food was great. The result is: text: The food was great. target: The food polarity: positive patternName: Target is positive clue: great This example shows another output for this extractor: I love my new phone. The result is: text: I love my new phone. target: I polarity: positive patternName: Speaker does positive on Target clue: love This example shows extraction of a negative sentiment: This is the worst thing that ever happened to me. The result is: text: This is the worst thing that ever happened to me. target: the worst thing that ever happened to me. polarity: negative patternName: Target has negative adjective clue: worst","title":"Sentiment_General"},{"location":"ana_txtan_extractor-libraries/#base-modules","text":"Base modules provide the fundamental extractors that are used by all other libraries above. The following are the base modules: InputDocumentProcessor This module performs the initial document pre-processing. This module shows the view that contains the input document content after it extracts relevant markup content from the document. CommonFeatures This module extracts features that multiple modules use to build their extraction semantics. Examples of such features include cities, states, countries, stock tickers, and more. Dictionaries This module contains all of the dictionaries that are used by other named entity modules. Examples of such dictionaries include names of cities, famous personalities, sports teams, organizations, and more. FinancialDictionaries This module contains all of the dictionaries that are used by other financial entity modules. Examples of such dictionaries include clues for identifying financial events, and clues for identifying relevant terms across financial reports. Linguistics This module contains syntactic features that some of the other modules use for their own rules. Disambiguation This module is used to avoid overlap of the same extraction that belongs to multiple unrelated entities. For example, this module is used to disambiguate extractions for the Person entity against extractions for the Organization entity. SentenceBoundary This module contains extraction logic to identify boundaries in sentences, such as boundaries that are in plain text content and boundaries that are within markup-like content. UDFs This module contains user-defined functions that are used by the rest of the modules, such as getAbbreviation and toUpperCase . Region, WaterBody, Facility These modules are used by some internal modules to identify regions such as national parks, oceans, seas, and facilities such as buildings and known constructions. EntitiesExport This module is the sink module for this library. All of the views that represent both named and financial entities are aggregated in this module from the rest of the modules for export purposes.","title":"Base modules"},{"location":"ana_txtan_extractors/","text":"AQL concepts Table of Contents {:toc} AQL extractors overview Extractors are programs that extract structured information from unstructured or semistructured text by using AQL constructs. An extractor consists of compiled modules, or Text Analytics module (TAM) files, and content for external dictionary and table artifacts. At a high level, the extractor can be regarded as a collection of views, each of which defines a relationship. Some of these views are designated as output views, while others are non-output views. In addition, there is a special view called Document . This view represents the document that is being annotated. Furthermore, the extractor might also have external views whose content can be customized at run time with extra metadata about the document that is being annotated. The following figure illustrates how SystemT compiles and runs extractors. The SystemT Optimizer compiles the source modules into compiled TAM files. An extractor is instantiated based on a set of one or more compiled modules, with the content for external dictionaries and tables that are required by these modules. You can reuse and combine modules to create more complex extractors. This process results in a complete executable plan for the extractor. This executable plan is passed as input to the SystemT runtime component. External views can be included as input to customize the extractor by your data needs. SystemT has a document-at-a-time runtime model, which means that at run time, the view Document is populated with a single tuple that represents the content of the document to process. The runtime component then runs the execution plan and populates the output views of the extractor with the extracted results. AQL modules Text Analytics modules are self-contained packages. They contain a set of text extraction rules that are created by using AQL, and other resources that are required for text extraction. Structure of a module Documenting a module with AQL Doc comments Example Scenarios that illustrate modules Best practices for developing modules Structure of a module Resources that are included in a source module can include AQL files, dictionary files, UDF JAR files, and a special file named module.info that contains the AQL Doc comment that details the specifics of the module. When you import a module, these resources in the form of AQL objects (such as a view , table , dictionary , or function ) are available for reuse. This increases the efficiency of development. The structure of a module consists of, and is defined by, a top-level folder. The name of the folder is also the name of the module. One or more AQL files are located directly in the top-level module folder. AQL files that are located within subdirectories of the top-level folder are ignored by the compiler. Dictionary files and UDF JAR files can be located in the top-level module folder, or within subdirectories of the module folder. References to these files inside a module must be done by using a path relative to the top-level module folder. Compiling your source module results in a TAM file that includes: The compiled form of the AQL files, which is known as the execution plan Metadata of the elements that are defined in the module Module metadata includes all of the details that are necessary to consume the module without instantiating the module. These details include the following information: The schema of the special view Document The views (output, external, and exported) with the schema and AQL Doc comments, and cost information that is associated with the view that is used to optimize modules that depend on this view The tables (external and exported) with the schema and AQL Doc comments The dictionaries (external and exported) with AQL Doc comments The exported functions with AQL Doc comments The type of tokenizer used to compile the dictionaries of the module (This tokenizer must coincide with the tokenizer used at execution time) Any other modules that this module depends on, such as modules needed at compile time or at initialization time, to ensure that all components of the extractor are completely loaded After you have a compiled module, you can combine the modules or reuse modules that contain established AQL elements, such as tables and artifacts, to build new extractors. Your AQL project can be modified easily with modular AQL, and you can reuse your data and share your projects with others. Documenting a module with AQL Doc comments Each module should have a comment that describes its high-level semantics. The comment for a module is placed in a file named module.info in the module folder. This file contains only the comment for the current module. Capture the following information in a module-level comment: A high-level description that contains the application areas, types of data sources, and languages used The schema of the Document view Author information The customization points of the module, and what must be modified to customize the module for a different domain or language Any other assumptions necessary to effectively use the module. Input format expectations Expectations about the data source, for example, the domain and average size of the documents on which the module is expected to perform Higher level types of analysis that the module completes Patterns that are captured or not captured by the module Specific tags to represent the information The module comment format consists of a top-level comment followed by a tag. The following pre-defined special AQL Doc comment tags can be used to convey specific portions of the module information. Any other necessary information that does not fall into any of the specific tags should be included in the top-level comment. @appType Specifies application type and areas that are covered by the module. For example, one can specify whether the extractor works for a specific vertical, such as financial, or healthcare, or a cross vertical, such as Social Media Analytics. @dataSource Specifies data sources that the module expects to work with or was tested with. For example, you can indicate the type of input documents that the extractor works with, such as email, social media data, news reports. You can also indicate other characteristics of the data, such as: The average document size The type of content (formal or informal/noisy) The structure (completely unstructured, semi-structured such as HTML or XML, or intrinsic structured such as log records) @languages A comma-separated list of language codes that are supported by the module. @author The name of the author of this module \u2013 one each for every author. @docField A tag each for every field in the required input document schema of the module. Example Consider this common AQL module comment format: /** * This module contains extractors for social data analytics. Extractors include Buzz, Intent, Sentiment, and Person microsegmentation. * * @appType Social Data Analytics * @dataSource Supports social media data * @languages en,fr,de * @docField text the content of the social media message * @docField location the \u201clocation\u201d field of a social media data record. Used for extracting attributes such as city and state. * */ Scenarios that illustrate modules You can use common design patterns to create Text Analytics modules. These examples encompass various AQL statements, and each example uses the AQL Doc comments to explain the code. Importing and exporting modules In the following example, the module personEn creates and makes a view available called Person . The module signature reuses the views from the personEn and email modules. module personEn; /** * Extract mentions of person names, primarily from English text. * Limited support for Italian and French. * * @field fullName person's complete name * @field firstName person's given name (optionally populated, when possible) * @field lastName person's surname (optionally populated, when possible) */ create view Person as select P.name as fullName, P.first as firstName, P.last as lastName from PersonFinal P; export view Person; module signature; import view Person from module personEn; import view EmailAddress from module email as Email; /** * Find mentions of person full names followed * within 0 to 5 tokens by an email address. * * @field match the span covering the full name and email address */ create view PersonWithEmail as extract pattern <P.fullName> <Token>{0,5} <E.address> as match from personEn.Person P, Email E; output view PersonWithEmail; This example illustrates the two forms of the import view statement: Without a local name. The name, fully qualified by module name ( personEn.Person ), is placed in the namespace of the current module. With a local name. The local name Email is placed within the namespace of the current module signature , instead of the fully qualified name email.EmailAddress . The name that is imported from the first module is used throughout the second module in the from clause of that statement, which uses the fully qualified name, or the local name for the two views. Domain-customization of an extractor In the following example, common is a library of basic extractors that are useful for Machine Data Analysis. The datapower and syslog modules are implementations of Machine Data Analysis extractors for two different types of logs. The extractor_datapower and extractor_syslog modules import relevant libraries and output the views. module common; /** * Extract mentions of IP addresses of * the form xxx.xxx.xxx.xxx * from the text field of view Document. * * @field address the IP address */ create view IP as \u2026 ; export view IP; module datapower; import module common; /** * Extract mentions of DeviceIP addresses. In DataPower logs, * let\u2019s imagine that the DeviceIP is the first IP mention * after the timestamp. * @field address the IP address */ create view DeviceIP as \u2026 ; export view DeviceIP; module syslog; import module common; /** * Extract mentions of DeviceIP addresses. In system logs, * let\u2019s imagine the DeviceIP is at the end of the log record, * so we must provide an implementation different than for DataPower. * @field address the IP address */ create view DeviceIP as \u2026 ; export view DeviceIP; module extractor_datapower; import module datapower; output view datapower.DeviceIP as 'DeviceIP'; module extractor_syslog; import module syslog; output view syslog.DeviceIP as 'DeviceIP'; The import module statement makes names available that are fully qualified by the module name. The view names (and dictionary, table, and function names) are always fully qualified by the module name. The output view DeviceIP results in different output names (such as datapower.DeviceIP and syslog.DeviceIP ). The application code must change depending on which modules you use for extraction. For example, Extractor1 (which uses common, datapower, and extractor_datapower) and Extractor2 (which uses common, syslog, and extractor_syslog) generate different output names for the DeviceIP type. The best practice is to use the output view <view-name> as '<alias>' statement. The output view <moduleName>.DeviceIP as 'DeviceIP' results in both Extractor1 and Extractor2 exporting identical names, DeviceIP. Combining modules to create an extractor You can combine modules to create an extractor. This example uses two modules, phone and person , to create an extractor that identifies all of the occurrences of a person with a phone number that immediately follows it (for example, the view PersonPhone ). The Person and PhoneNumber views might be used in other extractors, so they are placed in separate modules for easy reuse. module phone; create view PhoneNumber as select P.num as number from { extract regexes /\\+?\\[1-9]\\d{2}\\)\\d{3}-\\d{4}/ and /\\+?[Xx]\\.?\\d{4,5}/ on D.text as num from Document D } P; export view PhoneNumber; module person; -- first names, derived from a dictionary of common first names create dictionary FirstNameDict from file \u2018dictionaries/first.dict\u2019; create view FirstName as extract dictionary FirstNameDict on D.text as name from Document D; -- last names, derived from a dictionary of common last names create dictionary LastNameDict from file \u2018dictionaries/last.dict\u2019; create view LastName as extract dictionary LastNameDict on D.text as name from Document D; -- a person is a first name followed immediately by a last name create view Person as select F.name as firstName, L.name as lastName, CombineSpans(F.name, L.name) as person from FirstName F, LastName L where FollowsTok(F.name, L.name, 0, 0); export view Person; module personPhone; import module person; import module phone; -- generate people followed immediately by a phone number create view PersonPhone as select PE.person as name, PH.number as number from person.Person PE, phone.PhoneNumber as PH where FollowsTok(PE.person, PH.number, 0, 0); output view PersonPhone; Building a library of UDFs This example illustrates a common design pattern on how to build a module of user-defined functions (UDFs) that you can reuse in other modules. The module normalizationLib defines and exports one UDF function. The module signature reuses all of the views, tables, dictionaries, and functions that are made available by the personEn module, and one function from the normalizationLib module. Although the import function statement is not illustrated in this example, it is similar to other import statements in that it has two forms; with or without local name. Notice in the example, the use of the local name of the function, normalize , in the select clause of the view PersonNormalized : module normalizationLib; /** * UDF function for normalizing a span value. * * @param val the input span value * @return string content of the span, normalized as follows: * all characters converted to lower case; * multiple consecutive white spaces replaced by a single space */ create function normalizeSpan(val Span) return String external_name 'udfjars/exampleUDFs.jar:textanalytics.udf.ExampleUDFs!norm' language java deterministic return null on null input; export function normalizeSpan; module signature; import module personEn; import function normalizeSpan from module normalizationLib as normalize; /** * Generate a normalized version of a Person mention * by replacing consecutive white spaces with a single space * and converting the result to lower case. * * @field normFullName normalized string representation of the full name */ create view PersonNormalized as select normalize(P.fullName) as normFullName from personEn.Person P; Customizing dictionaries In this example, the commonPersonNames module makes two dictionaries available, an internal dictionary and an external dictionary. The external dictionary is initially empty (at compile time). It is populated at initialization time. The format for the external dictionary file is .dict. There is one dictionary entry per line. The scope of set default dictionary language is all external or internal dictionaries inside the entire module (not just the current AQL file) because the statement does not use the with language as clause. The set default dictionary language statement affects only the dictionary CustomFirstNames_WesternEurope because this dictionary is declared without an explicit with language as clause. In contrast, the statement does not affect the dictionary FirstNames_en_fr because that dictionary is declared with an explicit with language as clause. module commonPersonNames; set default dictionary language as 'en,fr,it,de,pt,es'; /** * Dictionary of top 1000 baby names in the U.S. and France. * Evaluated on English and French text only. */ create dictionary FirstNames_en_fr from file 'dicts/firstNames.dict' with language as 'en,fr'; /** * Customizable dictionary of first names. * Evaluated on English, French, Italian, German, Portuguese, Spanish text. */ create external dictionary CustomFirstNames_WesternEurope allow_empty true; export dictionary FirstNames_en_fr; export dictionary CustomFirstNames_WesternEurope; module personWesternEurope; import dictionary FirstNames_en_fr from module commonPersonNames; import dictionary CustomFirstNames_WesternEurope from module commonPersonNames as CustomFirstNames ; /** * Find mentions of common first names. Use a variety of * person first names in various languages, * as well as a customizable external dictionary */ create view FirstName as extract dictionaries commonPersonNames.FirstNames_en_fr and CustomFirstNames -- and <... other dictionaries here ...> on D.text as name from Document D; Customizing tables In this example, the sentiment module makes available an external table of product names. As with external dictionaries, external tables are initially empty at compile time. They are populated at initialization time. The format of the external table is a .CSV file with a header. The first row is the header and the other rows are the table rows. Internal tables and external tables can be imported in two ways, with or without local name. As with an internal table, you can create a dictionary from a column of an external table. However, you cannot create a dictionary from an external table that is imported. The work-around for this issue is to create the dictionary in the same module where the external table is defined. nickName,formalName \u201cCanon 1D\u201d,\u201dCannon EOS 1D\u201d \u201cCanon 1\u201d,\u201dCannon EOS 1D\u201d \u201cCanon 7\u201d,\u201dCanon EOS 7\u201d module sentiment; /** * A table of products around which we extract sentiment. * * @field nickName common name for the product as it may appear in text * @field formalName formal name of the product, used for normalization */ create external table Product (nickName Text, formalName Text) allow_empty false; /** * Dictionary of product nicknames, from the nickName field * of the customizable external table Product. */ create dictionary ProductDict from table Product with entries from nickName; create view ProductSentiment as extract pattern 'I like' (<P.match>)return group 1 as product from ( extract dictionary 'ProductDict' on D.text as match from Document D ) P; /** * Products with positive sentiment. * @field nickName product mention extracted from the text * @field formalName normalized name of the product */ create view ProductLikeNormalized as select S.product as nickName, P.formalName from ProductSentiment S, Product P where Equals(GetText(S.product), P.nickName); export view ProductLikeNormalized; Using custom documents and external views In this example, the module common defines the schema of the special view Document, which contains two attributes: twitterID of type Integer and twitterText of type Text. The module also uses additional metadata about the document, in the form of a set of hash tags that are defined by using the HashTags external view. You can write extractors that operate on documents with additional metadata. There are two types of metadata: Scalar-value metadata. Use fields in the view Document (the require document with columns statement). The scope of the statement is the entire module, not just the current AQL file. Set-value metadata Use external views. The JSON data collection format (Hadoop) is used to represent custom documents and external view content. The existing data collection formats work only with the default Document schema (text Text, label Text). The JSON text format is defined by one record per line. Each record contains content for the Document and external views. The AQL Doc is in the module.info file that documents the entire module. module common; require document with columns twitterId Integer and twitterText Text; /** * Hash tags associated with a Twitter message. * @field tag string content of the hash tag, without # */ create external view HashTags (tag Text) external_name 'Hashes'; /** * Input documents that contain at least one hashtag * are treated in a special way during extraction. * @field twitterText the Twitter message text * @field twitterId the Twitter ID, as integer * @field tag tags associated with the twitter message */ create view DocumentWithHashTag as select D.twitterId, D.twitterText, H.tag from Document D, HashTags H; output view DocumentWithHashTag; {\u201ctwitterId\u201d:12345678, \u201ctwitterText\u201d:\u201dGone fishing\u201d, \u201cExternalViews\u201d:{\u201cHashes\u201d:[{\u201ctag\u201d:\u201dhalfmoonbay\u201d}]}} {\u201ctwitterId\u201d:23456789, \u201ctwitterText\u201d:\u201dGone surfing\u201d, \u201cExternalViews\u201d:{\u201cHashes\u201d:[{\u201ctag\u201d:\u201dusa\u201d}, {\u201ctag\u201d:\u201dsantacruz\u201d}]}} Best practices for developing modules These best practices present practical advice to improve your modules. Document the source code by using AQL Doc comments. Document your exported artifacts, the output views, the Document view, and the module itself. By using informative AQL Doc comments, you can make sure that your module is consumable by others in the expected fashion. Place large dictionaries and tables in separate modules. When you place large dictionaries and tables that do not frequently change in separate modules, it results in a decrease in compilation time for the entire extractor. Place UDF JAR files in their own separate module and export the functions. Build a library of UDFs that you can reuse in other modules to decrease unnecessary redundancy of the files. Do not use the output view statement when you develop extractor libraries. The output directive in one module cannot be overridden in another module. If you do not use the output statement in your extractor library, the consumer of the library can choose what types to output. Use the output view \u2026 as \u2026 statement when you customize an extractor for different domains. The use of this statement ensures that output names are identical across different implementations of the customization. AQL files AQL files are text files that contain text extraction rules that are written in the AQL programming language. A module contains one or more AQL files. AQL files are a way of managing your AQL artifacts separately from larger dictionary files and reusable UDF files to increase performance and opportunities for modularization. AQL files include: A module statement A module statement indicates the name of the containing module. AQL files must be placed directly under the top-level module directory. The first statement in an AQL file must be module <module name> to indicate the name of the containing module. These files are not allowed to be in subdirectories, therefore there can be no submodules. The AQL statements for defining artifacts (view, dictionary, table, function) AQL files provide the definition of AQL artifacts by using views, dictionaries, tables, and user-defined functions. The scope of the statement is the entire module, not just the current AQL file. The export statements Export statements expose the artifacts created in the module to other modules. Unless exported, an artifact is private to that specific module. You can use the import statement to reuse artifacts from another module. The create external dictionary statements These statements allow you to customize dictionaries without recompiling AQL. The content is generated when the extractor is initialized, and the dictionary remains constant for each document that is annotated. The create external table statements These statements allow you to customize tables without recompiling AQL. The content is generated when the extractor is initialized, and the dictionary remains constant for each document that is annotated. The require document with columns statement This statement defines the necessary columns in the view Document . The set default dictionary language statement Defines the default set of languages to use when you compile a dictionary that is defined without the with language clause. When you compile a dictionary, you must tokenize each dictionary in each language. Views A view is a logical statement that defines what to match in your document or how to refine your extraction. Views are the top-level components of an extractor. A common way that views are used is to define what to match in a document. Views can also be used to select information from previously created views to refine or combine constraints, and to define what to return (as tuples) when the AQL script is run. Views define the tuples, but do not compute them. All of the tuples in a view have the same schema There are three types of views in AQL: Internal views describe content that is computed, if necessary, by the extractor, which is based on the input that is supplied at run time in the special view Document , and the external views. To define an internal view, use one of the statements: The create view statement The detag statement The select into statement The special view Document is the most basic view that captures the input document. The view Document is populated at run time with the necessary values for each of the attributes of its schema. By default, the schema of this view is text: Text, label: Text , but you can specify a custom schema by using the require document with columns statement. External views define content that is not explicitly specified at compile time. External views are useful if you want to inject metadata about the input document that cannot be captured by using the Document view. For example, if you want to use metadata that cannot be represented as scalar values. To define an external view, use the create external view statement. The SystemT engine does not compute the content of a view unless you explicitly request it by using the output view or select into statements. For more information about the Document view and external views, see the AQL execution model. Views can be used in the following AQL constructs: The from clause of a select or extract statement The target of an extract statement or detag statement The export or import statements to expose the view, and then use the view in another module The following is a simple example that illustrates how you can define an internal view and specify that it should be computed: require document with columns text Text; create view Phone as extract regex /(\\d{3})-\\d{4}/ on 3 tokens in D.text return group 0 as fullNumber and group 1 as areaCode from Document D; output view Phone; The first statement specifies that the view Document has an attribute that is called text of type Text. Consider that the value of the text field in the input document is My number is 555-1234, yours is 500-5678 . The extractor that is specified by this AQL code computes a view Phone with the schema fullNumber: Span, areaCode: Span . The content consists of the tuples [beginOffset, endOffset] . Notice that the span values are shown for clarity in the format 'matched text' . fullNumber areaCode '320-555-1234' [13-24] '320' [13-16] '480-500-5678' [32-43] '480' [32-35] Dictionaries A dictionary is a set of terms that is used to identify matching words or phrases in the input text. There are two types of dictionaries in AQL: Internal dictionaries contain content that is fully specified in AQL. Internal dictionaries are compiled and serialized in the compiled representation of AQL code (TAM files). To define an internal dictionary, use the create dictionary statement. External dictionaries contain content that is not specified in AQL. Instead, the content is supplied when the compiled extractor is run. You can customize extractors for different scenarios with external dictionaries. You can supply different content for the external dictionary for each scenario without needing to recompile the source AQL of your extractor. The content of external views might change for each input document. However, the content of external dictionaries remains the same throughout a particular instantiation of the extractor. External dictionaries can be defined directly by using the create external dictionary statement, or indirectly by using the create dictionary from table statement if the table is an external table. Dictionaries can be used in the following AQL constructs: The extract dictionary statement to identify all matches of dictionary entries in the input The MatchesDict() and ContainsDict() predicates to test if the input precisely matches or contains a match for one of the dictionary terms The export and import statements to expose the dictionary and then use the dictionary in another module Tables A table is a static set of tuples in a file that contains the terms that you want to use in your extractor. The content of views can differ for every input document that is processed by the extractor, but the content of a table is static and remains unchanged for the lifetime of an extractor. You can use internal and external tables when you develop your module. There are two types of tables in AQL: Internal tables have content that is fully specified in AQL, and the content is compiled and serialized in the compiled representation of AQL code (TAM files). To define an internal table, use the create table statement External tables have content that is not specified in AQL. Instead, the content is supplied when the compiled extractor is run. Extractors can be customized for different scenarios with external tables. You can supply different content for the external table for each scenario without recompiling the source AQL of your extractor. Unlike external views, whose content can change for each input document, the content of external tables remains the same throughout an instantiation of the extractor. External tables can be defined by using the create external table statement. Tables can be used in the following AQL constructs: The from clause of a select or extract statement. The create dictionary from table statement. The export and import statements to expose the table and then run the table in another module. Functions A user-defined function (UDF) specifies custom functions that you can use in your extraction rules. AQL has a collection of built-in functions and predicates that you can use in extraction rules. If the built-in functions and predicates are not sufficient for your extraction task, you can define your own user-defined function (UDF) in AQL by using the create function statement. AQL supports scalar UDFs and table UDFs that are implemented in Java. A scalar UDF outputs a single scalar value, whereas a table UDF outputs a multiset of tuples. UDFs can be used in the following AQL constructs: Scalar UDFs can be used in all constructs where built-in functions can be used, such as the where clause of a select statement and the having clause of the extract statement. Table UDFs can be used in the from clause of a select or extract statement. The export and import statements to expose the function, and then use the function in another module. For more information about how to build libraries of user-defined functions, see Scenarios that illustrate modules. For more information about how to define, implement, and use UDFs, see the user-defined functions topics in the AQL reference.","title":"AQL concepts"},{"location":"ana_txtan_extractors/#aql-concepts","text":"Table of Contents {:toc}","title":"AQL concepts"},{"location":"ana_txtan_extractors/#aql-extractors-overview","text":"Extractors are programs that extract structured information from unstructured or semistructured text by using AQL constructs. An extractor consists of compiled modules, or Text Analytics module (TAM) files, and content for external dictionary and table artifacts. At a high level, the extractor can be regarded as a collection of views, each of which defines a relationship. Some of these views are designated as output views, while others are non-output views. In addition, there is a special view called Document . This view represents the document that is being annotated. Furthermore, the extractor might also have external views whose content can be customized at run time with extra metadata about the document that is being annotated. The following figure illustrates how SystemT compiles and runs extractors. The SystemT Optimizer compiles the source modules into compiled TAM files. An extractor is instantiated based on a set of one or more compiled modules, with the content for external dictionaries and tables that are required by these modules. You can reuse and combine modules to create more complex extractors. This process results in a complete executable plan for the extractor. This executable plan is passed as input to the SystemT runtime component. External views can be included as input to customize the extractor by your data needs. SystemT has a document-at-a-time runtime model, which means that at run time, the view Document is populated with a single tuple that represents the content of the document to process. The runtime component then runs the execution plan and populates the output views of the extractor with the extracted results.","title":"AQL extractors overview"},{"location":"ana_txtan_extractors/#aql-modules","text":"Text Analytics modules are self-contained packages. They contain a set of text extraction rules that are created by using AQL, and other resources that are required for text extraction. Structure of a module Documenting a module with AQL Doc comments Example Scenarios that illustrate modules Best practices for developing modules","title":"AQL modules"},{"location":"ana_txtan_extractors/#structure-of-a-module","text":"Resources that are included in a source module can include AQL files, dictionary files, UDF JAR files, and a special file named module.info that contains the AQL Doc comment that details the specifics of the module. When you import a module, these resources in the form of AQL objects (such as a view , table , dictionary , or function ) are available for reuse. This increases the efficiency of development. The structure of a module consists of, and is defined by, a top-level folder. The name of the folder is also the name of the module. One or more AQL files are located directly in the top-level module folder. AQL files that are located within subdirectories of the top-level folder are ignored by the compiler. Dictionary files and UDF JAR files can be located in the top-level module folder, or within subdirectories of the module folder. References to these files inside a module must be done by using a path relative to the top-level module folder. Compiling your source module results in a TAM file that includes: The compiled form of the AQL files, which is known as the execution plan Metadata of the elements that are defined in the module Module metadata includes all of the details that are necessary to consume the module without instantiating the module. These details include the following information: The schema of the special view Document The views (output, external, and exported) with the schema and AQL Doc comments, and cost information that is associated with the view that is used to optimize modules that depend on this view The tables (external and exported) with the schema and AQL Doc comments The dictionaries (external and exported) with AQL Doc comments The exported functions with AQL Doc comments The type of tokenizer used to compile the dictionaries of the module (This tokenizer must coincide with the tokenizer used at execution time) Any other modules that this module depends on, such as modules needed at compile time or at initialization time, to ensure that all components of the extractor are completely loaded After you have a compiled module, you can combine the modules or reuse modules that contain established AQL elements, such as tables and artifacts, to build new extractors. Your AQL project can be modified easily with modular AQL, and you can reuse your data and share your projects with others.","title":"Structure of a module"},{"location":"ana_txtan_extractors/#documenting-a-module-with-aql-doc-comments","text":"Each module should have a comment that describes its high-level semantics. The comment for a module is placed in a file named module.info in the module folder. This file contains only the comment for the current module. Capture the following information in a module-level comment: A high-level description that contains the application areas, types of data sources, and languages used The schema of the Document view Author information The customization points of the module, and what must be modified to customize the module for a different domain or language Any other assumptions necessary to effectively use the module. Input format expectations Expectations about the data source, for example, the domain and average size of the documents on which the module is expected to perform Higher level types of analysis that the module completes Patterns that are captured or not captured by the module Specific tags to represent the information The module comment format consists of a top-level comment followed by a tag. The following pre-defined special AQL Doc comment tags can be used to convey specific portions of the module information. Any other necessary information that does not fall into any of the specific tags should be included in the top-level comment. @appType Specifies application type and areas that are covered by the module. For example, one can specify whether the extractor works for a specific vertical, such as financial, or healthcare, or a cross vertical, such as Social Media Analytics. @dataSource Specifies data sources that the module expects to work with or was tested with. For example, you can indicate the type of input documents that the extractor works with, such as email, social media data, news reports. You can also indicate other characteristics of the data, such as: The average document size The type of content (formal or informal/noisy) The structure (completely unstructured, semi-structured such as HTML or XML, or intrinsic structured such as log records) @languages A comma-separated list of language codes that are supported by the module. @author The name of the author of this module \u2013 one each for every author. @docField A tag each for every field in the required input document schema of the module.","title":"Documenting a module with AQL Doc comments"},{"location":"ana_txtan_extractors/#example","text":"Consider this common AQL module comment format: /** * This module contains extractors for social data analytics. Extractors include Buzz, Intent, Sentiment, and Person microsegmentation. * * @appType Social Data Analytics * @dataSource Supports social media data * @languages en,fr,de * @docField text the content of the social media message * @docField location the \u201clocation\u201d field of a social media data record. Used for extracting attributes such as city and state. * */","title":"Example"},{"location":"ana_txtan_extractors/#scenarios-that-illustrate-modules","text":"You can use common design patterns to create Text Analytics modules. These examples encompass various AQL statements, and each example uses the AQL Doc comments to explain the code.","title":"Scenarios that illustrate modules"},{"location":"ana_txtan_extractors/#importing-and-exporting-modules","text":"In the following example, the module personEn creates and makes a view available called Person . The module signature reuses the views from the personEn and email modules. module personEn; /** * Extract mentions of person names, primarily from English text. * Limited support for Italian and French. * * @field fullName person's complete name * @field firstName person's given name (optionally populated, when possible) * @field lastName person's surname (optionally populated, when possible) */ create view Person as select P.name as fullName, P.first as firstName, P.last as lastName from PersonFinal P; export view Person; module signature; import view Person from module personEn; import view EmailAddress from module email as Email; /** * Find mentions of person full names followed * within 0 to 5 tokens by an email address. * * @field match the span covering the full name and email address */ create view PersonWithEmail as extract pattern <P.fullName> <Token>{0,5} <E.address> as match from personEn.Person P, Email E; output view PersonWithEmail; This example illustrates the two forms of the import view statement: Without a local name. The name, fully qualified by module name ( personEn.Person ), is placed in the namespace of the current module. With a local name. The local name Email is placed within the namespace of the current module signature , instead of the fully qualified name email.EmailAddress . The name that is imported from the first module is used throughout the second module in the from clause of that statement, which uses the fully qualified name, or the local name for the two views.","title":"Importing and exporting modules"},{"location":"ana_txtan_extractors/#domain-customization-of-an-extractor","text":"In the following example, common is a library of basic extractors that are useful for Machine Data Analysis. The datapower and syslog modules are implementations of Machine Data Analysis extractors for two different types of logs. The extractor_datapower and extractor_syslog modules import relevant libraries and output the views. module common; /** * Extract mentions of IP addresses of * the form xxx.xxx.xxx.xxx * from the text field of view Document. * * @field address the IP address */ create view IP as \u2026 ; export view IP; module datapower; import module common; /** * Extract mentions of DeviceIP addresses. In DataPower logs, * let\u2019s imagine that the DeviceIP is the first IP mention * after the timestamp. * @field address the IP address */ create view DeviceIP as \u2026 ; export view DeviceIP; module syslog; import module common; /** * Extract mentions of DeviceIP addresses. In system logs, * let\u2019s imagine the DeviceIP is at the end of the log record, * so we must provide an implementation different than for DataPower. * @field address the IP address */ create view DeviceIP as \u2026 ; export view DeviceIP; module extractor_datapower; import module datapower; output view datapower.DeviceIP as 'DeviceIP'; module extractor_syslog; import module syslog; output view syslog.DeviceIP as 'DeviceIP'; The import module statement makes names available that are fully qualified by the module name. The view names (and dictionary, table, and function names) are always fully qualified by the module name. The output view DeviceIP results in different output names (such as datapower.DeviceIP and syslog.DeviceIP ). The application code must change depending on which modules you use for extraction. For example, Extractor1 (which uses common, datapower, and extractor_datapower) and Extractor2 (which uses common, syslog, and extractor_syslog) generate different output names for the DeviceIP type. The best practice is to use the output view <view-name> as '<alias>' statement. The output view <moduleName>.DeviceIP as 'DeviceIP' results in both Extractor1 and Extractor2 exporting identical names, DeviceIP.","title":"Domain-customization of an extractor"},{"location":"ana_txtan_extractors/#combining-modules-to-create-an-extractor","text":"You can combine modules to create an extractor. This example uses two modules, phone and person , to create an extractor that identifies all of the occurrences of a person with a phone number that immediately follows it (for example, the view PersonPhone ). The Person and PhoneNumber views might be used in other extractors, so they are placed in separate modules for easy reuse. module phone; create view PhoneNumber as select P.num as number from { extract regexes /\\+?\\[1-9]\\d{2}\\)\\d{3}-\\d{4}/ and /\\+?[Xx]\\.?\\d{4,5}/ on D.text as num from Document D } P; export view PhoneNumber; module person; -- first names, derived from a dictionary of common first names create dictionary FirstNameDict from file \u2018dictionaries/first.dict\u2019; create view FirstName as extract dictionary FirstNameDict on D.text as name from Document D; -- last names, derived from a dictionary of common last names create dictionary LastNameDict from file \u2018dictionaries/last.dict\u2019; create view LastName as extract dictionary LastNameDict on D.text as name from Document D; -- a person is a first name followed immediately by a last name create view Person as select F.name as firstName, L.name as lastName, CombineSpans(F.name, L.name) as person from FirstName F, LastName L where FollowsTok(F.name, L.name, 0, 0); export view Person; module personPhone; import module person; import module phone; -- generate people followed immediately by a phone number create view PersonPhone as select PE.person as name, PH.number as number from person.Person PE, phone.PhoneNumber as PH where FollowsTok(PE.person, PH.number, 0, 0); output view PersonPhone;","title":"Combining modules to create an extractor"},{"location":"ana_txtan_extractors/#building-a-library-of-udfs","text":"This example illustrates a common design pattern on how to build a module of user-defined functions (UDFs) that you can reuse in other modules. The module normalizationLib defines and exports one UDF function. The module signature reuses all of the views, tables, dictionaries, and functions that are made available by the personEn module, and one function from the normalizationLib module. Although the import function statement is not illustrated in this example, it is similar to other import statements in that it has two forms; with or without local name. Notice in the example, the use of the local name of the function, normalize , in the select clause of the view PersonNormalized : module normalizationLib; /** * UDF function for normalizing a span value. * * @param val the input span value * @return string content of the span, normalized as follows: * all characters converted to lower case; * multiple consecutive white spaces replaced by a single space */ create function normalizeSpan(val Span) return String external_name 'udfjars/exampleUDFs.jar:textanalytics.udf.ExampleUDFs!norm' language java deterministic return null on null input; export function normalizeSpan; module signature; import module personEn; import function normalizeSpan from module normalizationLib as normalize; /** * Generate a normalized version of a Person mention * by replacing consecutive white spaces with a single space * and converting the result to lower case. * * @field normFullName normalized string representation of the full name */ create view PersonNormalized as select normalize(P.fullName) as normFullName from personEn.Person P;","title":"Building a library of UDFs"},{"location":"ana_txtan_extractors/#customizing-dictionaries","text":"In this example, the commonPersonNames module makes two dictionaries available, an internal dictionary and an external dictionary. The external dictionary is initially empty (at compile time). It is populated at initialization time. The format for the external dictionary file is .dict. There is one dictionary entry per line. The scope of set default dictionary language is all external or internal dictionaries inside the entire module (not just the current AQL file) because the statement does not use the with language as clause. The set default dictionary language statement affects only the dictionary CustomFirstNames_WesternEurope because this dictionary is declared without an explicit with language as clause. In contrast, the statement does not affect the dictionary FirstNames_en_fr because that dictionary is declared with an explicit with language as clause. module commonPersonNames; set default dictionary language as 'en,fr,it,de,pt,es'; /** * Dictionary of top 1000 baby names in the U.S. and France. * Evaluated on English and French text only. */ create dictionary FirstNames_en_fr from file 'dicts/firstNames.dict' with language as 'en,fr'; /** * Customizable dictionary of first names. * Evaluated on English, French, Italian, German, Portuguese, Spanish text. */ create external dictionary CustomFirstNames_WesternEurope allow_empty true; export dictionary FirstNames_en_fr; export dictionary CustomFirstNames_WesternEurope; module personWesternEurope; import dictionary FirstNames_en_fr from module commonPersonNames; import dictionary CustomFirstNames_WesternEurope from module commonPersonNames as CustomFirstNames ; /** * Find mentions of common first names. Use a variety of * person first names in various languages, * as well as a customizable external dictionary */ create view FirstName as extract dictionaries commonPersonNames.FirstNames_en_fr and CustomFirstNames -- and <... other dictionaries here ...> on D.text as name from Document D;","title":"Customizing dictionaries"},{"location":"ana_txtan_extractors/#customizing-tables","text":"In this example, the sentiment module makes available an external table of product names. As with external dictionaries, external tables are initially empty at compile time. They are populated at initialization time. The format of the external table is a .CSV file with a header. The first row is the header and the other rows are the table rows. Internal tables and external tables can be imported in two ways, with or without local name. As with an internal table, you can create a dictionary from a column of an external table. However, you cannot create a dictionary from an external table that is imported. The work-around for this issue is to create the dictionary in the same module where the external table is defined. nickName,formalName \u201cCanon 1D\u201d,\u201dCannon EOS 1D\u201d \u201cCanon 1\u201d,\u201dCannon EOS 1D\u201d \u201cCanon 7\u201d,\u201dCanon EOS 7\u201d module sentiment; /** * A table of products around which we extract sentiment. * * @field nickName common name for the product as it may appear in text * @field formalName formal name of the product, used for normalization */ create external table Product (nickName Text, formalName Text) allow_empty false; /** * Dictionary of product nicknames, from the nickName field * of the customizable external table Product. */ create dictionary ProductDict from table Product with entries from nickName; create view ProductSentiment as extract pattern 'I like' (<P.match>)return group 1 as product from ( extract dictionary 'ProductDict' on D.text as match from Document D ) P; /** * Products with positive sentiment. * @field nickName product mention extracted from the text * @field formalName normalized name of the product */ create view ProductLikeNormalized as select S.product as nickName, P.formalName from ProductSentiment S, Product P where Equals(GetText(S.product), P.nickName); export view ProductLikeNormalized;","title":"Customizing tables"},{"location":"ana_txtan_extractors/#using-custom-documents-and-external-views","text":"In this example, the module common defines the schema of the special view Document, which contains two attributes: twitterID of type Integer and twitterText of type Text. The module also uses additional metadata about the document, in the form of a set of hash tags that are defined by using the HashTags external view. You can write extractors that operate on documents with additional metadata. There are two types of metadata: Scalar-value metadata. Use fields in the view Document (the require document with columns statement). The scope of the statement is the entire module, not just the current AQL file. Set-value metadata Use external views. The JSON data collection format (Hadoop) is used to represent custom documents and external view content. The existing data collection formats work only with the default Document schema (text Text, label Text). The JSON text format is defined by one record per line. Each record contains content for the Document and external views. The AQL Doc is in the module.info file that documents the entire module. module common; require document with columns twitterId Integer and twitterText Text; /** * Hash tags associated with a Twitter message. * @field tag string content of the hash tag, without # */ create external view HashTags (tag Text) external_name 'Hashes'; /** * Input documents that contain at least one hashtag * are treated in a special way during extraction. * @field twitterText the Twitter message text * @field twitterId the Twitter ID, as integer * @field tag tags associated with the twitter message */ create view DocumentWithHashTag as select D.twitterId, D.twitterText, H.tag from Document D, HashTags H; output view DocumentWithHashTag; {\u201ctwitterId\u201d:12345678, \u201ctwitterText\u201d:\u201dGone fishing\u201d, \u201cExternalViews\u201d:{\u201cHashes\u201d:[{\u201ctag\u201d:\u201dhalfmoonbay\u201d}]}} {\u201ctwitterId\u201d:23456789, \u201ctwitterText\u201d:\u201dGone surfing\u201d, \u201cExternalViews\u201d:{\u201cHashes\u201d:[{\u201ctag\u201d:\u201dusa\u201d}, {\u201ctag\u201d:\u201dsantacruz\u201d}]}}","title":"Using custom documents and external views"},{"location":"ana_txtan_extractors/#best-practices-for-developing-modules","text":"These best practices present practical advice to improve your modules. Document the source code by using AQL Doc comments. Document your exported artifacts, the output views, the Document view, and the module itself. By using informative AQL Doc comments, you can make sure that your module is consumable by others in the expected fashion. Place large dictionaries and tables in separate modules. When you place large dictionaries and tables that do not frequently change in separate modules, it results in a decrease in compilation time for the entire extractor. Place UDF JAR files in their own separate module and export the functions. Build a library of UDFs that you can reuse in other modules to decrease unnecessary redundancy of the files. Do not use the output view statement when you develop extractor libraries. The output directive in one module cannot be overridden in another module. If you do not use the output statement in your extractor library, the consumer of the library can choose what types to output. Use the output view \u2026 as \u2026 statement when you customize an extractor for different domains. The use of this statement ensures that output names are identical across different implementations of the customization.","title":"Best practices for developing modules"},{"location":"ana_txtan_extractors/#aql-files","text":"AQL files are text files that contain text extraction rules that are written in the AQL programming language. A module contains one or more AQL files. AQL files are a way of managing your AQL artifacts separately from larger dictionary files and reusable UDF files to increase performance and opportunities for modularization. AQL files include: A module statement A module statement indicates the name of the containing module. AQL files must be placed directly under the top-level module directory. The first statement in an AQL file must be module <module name> to indicate the name of the containing module. These files are not allowed to be in subdirectories, therefore there can be no submodules. The AQL statements for defining artifacts (view, dictionary, table, function) AQL files provide the definition of AQL artifacts by using views, dictionaries, tables, and user-defined functions. The scope of the statement is the entire module, not just the current AQL file. The export statements Export statements expose the artifacts created in the module to other modules. Unless exported, an artifact is private to that specific module. You can use the import statement to reuse artifacts from another module. The create external dictionary statements These statements allow you to customize dictionaries without recompiling AQL. The content is generated when the extractor is initialized, and the dictionary remains constant for each document that is annotated. The create external table statements These statements allow you to customize tables without recompiling AQL. The content is generated when the extractor is initialized, and the dictionary remains constant for each document that is annotated. The require document with columns statement This statement defines the necessary columns in the view Document . The set default dictionary language statement Defines the default set of languages to use when you compile a dictionary that is defined without the with language clause. When you compile a dictionary, you must tokenize each dictionary in each language.","title":"AQL files"},{"location":"ana_txtan_extractors/#views","text":"A view is a logical statement that defines what to match in your document or how to refine your extraction. Views are the top-level components of an extractor. A common way that views are used is to define what to match in a document. Views can also be used to select information from previously created views to refine or combine constraints, and to define what to return (as tuples) when the AQL script is run. Views define the tuples, but do not compute them. All of the tuples in a view have the same schema There are three types of views in AQL: Internal views describe content that is computed, if necessary, by the extractor, which is based on the input that is supplied at run time in the special view Document , and the external views. To define an internal view, use one of the statements: The create view statement The detag statement The select into statement The special view Document is the most basic view that captures the input document. The view Document is populated at run time with the necessary values for each of the attributes of its schema. By default, the schema of this view is text: Text, label: Text , but you can specify a custom schema by using the require document with columns statement. External views define content that is not explicitly specified at compile time. External views are useful if you want to inject metadata about the input document that cannot be captured by using the Document view. For example, if you want to use metadata that cannot be represented as scalar values. To define an external view, use the create external view statement. The SystemT engine does not compute the content of a view unless you explicitly request it by using the output view or select into statements. For more information about the Document view and external views, see the AQL execution model. Views can be used in the following AQL constructs: The from clause of a select or extract statement The target of an extract statement or detag statement The export or import statements to expose the view, and then use the view in another module The following is a simple example that illustrates how you can define an internal view and specify that it should be computed: require document with columns text Text; create view Phone as extract regex /(\\d{3})-\\d{4}/ on 3 tokens in D.text return group 0 as fullNumber and group 1 as areaCode from Document D; output view Phone; The first statement specifies that the view Document has an attribute that is called text of type Text. Consider that the value of the text field in the input document is My number is 555-1234, yours is 500-5678 . The extractor that is specified by this AQL code computes a view Phone with the schema fullNumber: Span, areaCode: Span . The content consists of the tuples [beginOffset, endOffset] . Notice that the span values are shown for clarity in the format 'matched text' . fullNumber areaCode '320-555-1234' [13-24] '320' [13-16] '480-500-5678' [32-43] '480' [32-35]","title":"Views"},{"location":"ana_txtan_extractors/#dictionaries","text":"A dictionary is a set of terms that is used to identify matching words or phrases in the input text. There are two types of dictionaries in AQL: Internal dictionaries contain content that is fully specified in AQL. Internal dictionaries are compiled and serialized in the compiled representation of AQL code (TAM files). To define an internal dictionary, use the create dictionary statement. External dictionaries contain content that is not specified in AQL. Instead, the content is supplied when the compiled extractor is run. You can customize extractors for different scenarios with external dictionaries. You can supply different content for the external dictionary for each scenario without needing to recompile the source AQL of your extractor. The content of external views might change for each input document. However, the content of external dictionaries remains the same throughout a particular instantiation of the extractor. External dictionaries can be defined directly by using the create external dictionary statement, or indirectly by using the create dictionary from table statement if the table is an external table. Dictionaries can be used in the following AQL constructs: The extract dictionary statement to identify all matches of dictionary entries in the input The MatchesDict() and ContainsDict() predicates to test if the input precisely matches or contains a match for one of the dictionary terms The export and import statements to expose the dictionary and then use the dictionary in another module","title":"Dictionaries"},{"location":"ana_txtan_extractors/#tables","text":"A table is a static set of tuples in a file that contains the terms that you want to use in your extractor. The content of views can differ for every input document that is processed by the extractor, but the content of a table is static and remains unchanged for the lifetime of an extractor. You can use internal and external tables when you develop your module. There are two types of tables in AQL: Internal tables have content that is fully specified in AQL, and the content is compiled and serialized in the compiled representation of AQL code (TAM files). To define an internal table, use the create table statement External tables have content that is not specified in AQL. Instead, the content is supplied when the compiled extractor is run. Extractors can be customized for different scenarios with external tables. You can supply different content for the external table for each scenario without recompiling the source AQL of your extractor. Unlike external views, whose content can change for each input document, the content of external tables remains the same throughout an instantiation of the extractor. External tables can be defined by using the create external table statement. Tables can be used in the following AQL constructs: The from clause of a select or extract statement. The create dictionary from table statement. The export and import statements to expose the table and then run the table in another module.","title":"Tables"},{"location":"ana_txtan_extractors/#functions","text":"A user-defined function (UDF) specifies custom functions that you can use in your extraction rules. AQL has a collection of built-in functions and predicates that you can use in extraction rules. If the built-in functions and predicates are not sufficient for your extraction task, you can define your own user-defined function (UDF) in AQL by using the create function statement. AQL supports scalar UDFs and table UDFs that are implemented in Java. A scalar UDF outputs a single scalar value, whereas a table UDF outputs a multiset of tuples. UDFs can be used in the following AQL constructs: Scalar UDFs can be used in all constructs where built-in functions can be used, such as the where clause of a select statement and the having clause of the extract statement. Table UDFs can be used in the from clause of a select or extract statement. The export and import statements to expose the function, and then use the function in another module. For more information about how to build libraries of user-defined functions, see Scenarios that illustrate modules. For more information about how to define, implement, and use UDFs, see the user-defined functions topics in the AQL reference.","title":"Functions"},{"location":"ana_txtan_input-coll-format/","text":"Data collection formats The DocReader API from SystemT low-level Java API supports specific formats to represent a data collection. A data collection is a document or a set of documents from which you want to extract text or discover patterns. Restriction: All input documents must be UTF-8 encoded files, or the analysis can produce unexpected results. Also, while the data collections must contain text files, the content of these text files is not restricted to plain text. Non-binary content such as HTML or XML is also accepted. A data collection must be in one of the following formats: Table of Contents {:toc} UTF-8 encoded text files A data collection in this format can consist of a single UTF-8 encoded file, or a set of UTF-8 encoded text files, that are contained in a directory, archive, or a single comma-delimited file. A single text file A UTF-8 encoded single text file must have a .txt, .htm, .html, .xhtml, or .xml file extension. For a text file, the DocReader.next() API create one Document tuple with the schema as (text Text) or (text Text, label Text) based on the document schema of the loaded extractor. The value for the text field is the content of the file, and the value of the label field is the name of the file. Text files in a directory A data collection format can be a directory that contains UTF-8 encoded text files. For each file in the directory, the DocReader.next() API create one Document tuple with the schema as (text Text)or (text Text, label Text) based on the document schema of the loaded extractor. The value of the text field is the content of the file, and the value of the label field is the name of the file. Text files in an archive A data collection can be an archive file in a .zip, .tar, .tar.gz, or .tgz archive format that contains UTF-8 encoded text files. For each file in the archive file, the DocReader.next() API create one Document tuple with the schema as (text Text) or (text Text, label Text) based on the document schema of the loaded extractor. The value for the text field is the content of the file, and the value of the label field is the name of the file. A single comma-delimited file A UTF-8 encoded comma-delimited file (.del) must be in the following format: Each row represents a single document. Rows are separated by new lines. Each row has three comma-separated columns that represent the identifier (of type Integer), label (of type String), and text (of type String). The label and text string values must be surrounded by the character string delimiter, which is a double quotation mark (\"). Any double quotation marks inside these values must be escaped (\"\"). The document label can be any character string (for example, a file name) which must be unique. For each row in the .del file, the DocReader.next() API create one Document tuple with the schema as (text Text) or (text Text, label Text) based on the document schema of the loaded extractor. The value for the text field is the value of the third column of the row, and the value of the label field is the value of the second column of the row. The following is an example of a .del file in this format that contains two documents with labels, doc_1.txt and doc_2.txt. 1,\"doc_1.txt\",\"This is an example document.\" 2,\"doc_2.txt\",\"This is an example document with \"\"double quotation marks\"\" that need to be escaped\" UTF-8 encoded CSV files A data collection in this format can consist of a single UTF-8 encoded comma-delimited file with a header row. A single comma-delimited with header file A UTF-8 encoded comma-delimited file with header (.csv) must be in the following format: The first line of the file must be a header row that has comma-separated columns that represent the header or column-name, which specify the column field names. Columns can be of the data type Integer, Float, Text, or Boolean. Data of type Text must be surrounded by a double quotation mark (\"). In the following example of a CSV file, the field First_name is of type Text, so all of its values are enclosed in double quotation marks (such as \"Rohan\"). For each row in the CSV file, the DocReader.next() API create one Document tuple with the schema according to the document schema of the loaded extractor. You must specify the document schema in the AQL script by using the require document with columns statement. Headers that are not defined in the statement are ignored. The fields do not need to be specified in the order that they appear in the document. The following is an example of a CSV file in this format: First_name, Last_name, Id, Avg_score, Passed, Remarks \u201cRajeshwar\u201d, \u201cKalakuntla\u201d, 101, 70.96, true, \u201dPass \u2013 First class\u201d \u201cJayatheerthan\u201d, \u201cKrishna\u201d, 102, 85.96, true, \u201dPass - Distinction\u201d \u201cNisanth\u201d, \u201cSimon\u201d, 103, 80.96, true, \u201d Pass - Distinction\u201d \u201cRahul\u201d, \u201cKumar\u201d, 104, 49.96, false, \u201dFailed\u201d \u201cRohan\u201d, \u201cReddy\u201d, 105, 59.96, true, \u201dPass \u2013 Second class\u201d The first row of the CSV file is the header and it contains six columns: First_name , Last_name , Id , Avg_score , Passed , and Remarks .The remaining data is the CSV content. The following require document with columns statement establishes the column header labels and data types: require document with columns First_name Text and Last_name Text and Id Integer and Avg_score Float and Passed Boolean; Notice that the Remarks column is not used in the following statement, so the AQL will not have access to the Remarks column. UTF-8 encoded JSON files in Hadoop text input format A data collection in this format consists of a single UTF-8 encoded JSON (.json) file in Hadoop text input format. In this format, there is one JSON record per line. Each JSON record encodes the fields of one input document, and optionally, the external view content that is associated with that document. The document record is divided into multiple fields corresponding to a document schema (the structured format of the input document). Unlike the other data collection formats, which require a schema of (text Text, label Text), the JSON format allows the parsing of documents with a custom schema. The DocReader API aligns the schema that is read from the input JSON record with the expected input schema for the extractor. The expected input schema is the union of the schemas of its component modules (defined by the require document with columns statement of an AQL file) in that extractor. For example, if one module requires a document schema of (text Text) and another module requires a document schema of (id Integer), an extractor that is derived from both modules requires a document schema (text Text, id Integer). It is expected that the field names read from the JSON file are equal to the names of the extractor schema, and that the field types read from the JSON file are compatible with the types of the extractor schema. If the JSON input does not contain a required document field, an exception occurs. If the JSON input contains extraneous fields that are not specified by the extractor schema, the fields are ignored. In addition to the document schema fields, the document record can include a special field named ExternalViews. This field is a record where the name of each element is the external name of the external view (declared in AQL as external_name). The value of that element is an array of records. Each record corresponds to a tuple of an external view. As with the document schema, the field names and field types of each external view tuple must correspond with their defined schema in AQL. When a JSON input data file contains documents with ExternalViews fields, every document in that JSON data file must contain a field named ExternalViews , whether r not this field is populated with some external view content for that document. The following table shows the correspondence between JSON types and AQL types: AQL type JSON type Text String Integer Long Float Double Boolean Boolean String String Scalar list Array Span Record of the form {begin: Long, end: Long} or {begin: Long, end: Long, docref: String} A value of AQL type Span is represented as a JSON record with the following fields: begin of type JSON Long represents the begin offset of the span value. This field is mandatory. end of type JSON Long represents the end offset of the span value. This field is mandatory. docref of type JSON String represents the field of type Text in the view Document that holds the Text value that the span\u2019s begin and end offsets refer to. This attribute is optional. If missing, the span is assumed to be over the field text of the view Document . Examples Requirement: The example data is formatted for readability, but each JSON record must be entered on a single line. Example 1: Extractor with a custom document schema In this example, you see a code snippet of a single module with a required Document schema of four fields, one field for each of the four AQL data types that are supported by the require document with columns statement. module baseball; require document with columns avg Float and hr Integer and mvp Boolean and name Text; The following example shows a JSON file that consists of three input documents that are suitable for use as input documents for the extractor. The example data is formatted for readability, but each JSON record must be on a single line. No external view contents are present in any of the three documents. The rbi field in the first document is not required by the document schema, and is therefore ignored. {\"avg\":0.357,\"hr\":40,\"mvp\":true,\"rbi\":139,\"name\":\"Cabrera, Miguel\"} {\"avg\":0.326,\"hr\":30,\"mvp\":false,\"name\":\"Trout, Mike\"} {\"avg\":0.219,\"hr\":1,\"mvp\":false,\"name\":\"Punto, Nick\"} Example 2: Extractor that uses multiple modules with external views This example shows code snippets from various modules. Module email has Document schema (text Text) and an external view, EmailMetadata, with schema (toAddress Text, title Text) and external name, EmailMetadataSrc. module email; require document with columns text Text; create external view EmailMetadata(toAddress Text, title Text) external_name 'EmailMetadataSrc'; Module spam has Document schema (label Text) and one external view, SpamMetadata with schema (spamChance Float) and external name, SpamMetadataSrc. module spam; require document with columns label Text; create external view SpamMetadata(spamChance Float) external_name 'SpamMetadataSrc'; The following example shows a JSON file with a single document record that can be used as input to an extractor that combines both modules. This extractor has a unionized schema (label Text, text Text), and access to both external views. Notice the usage of external names of external views in the ExternalViews record. { \"label\": \"email.txt\", \"text\": \"Send this to 20 other people!\", \"ExternalViews\": { \"EmailMetadataSrc\": [ { \"toAddress\":\"John Doe\", \"title\":\"Make Money Fast\" }, { \"toAddress\":\"Jane Smith\", \"title\":\"Make Money Fast\" } ], \"SpamMetadataSrc\": [ { \"spamChance\":.999 } ] }} Example 3: Extractor that uses a module with external views but not all of the external views are populated The JSON input in this example contains documents with ExternalViews defined in each record, but the second record is not populated with external view content. module whitehouse; require document with columns id Integer and label Text and text Text and url Text and timeStamp Text; create external view EmailMetadata (fromAddress Text, toAddress Text, msgid Integer) external_name 'EmailMetadataSrc1'; output view EmailMetadata; create view dumpDocument as select D.id, D.label, D.text, D.url, D.timeStamp from Document D; output view dumpDocument; {\"id\": 123, \"label\": \"dem.whitehouse.txt\", \"text\": \"Yes we can!\", \"url\": \"www.whitehouse.gov\", \"timeStamp\": \"03:00\", \"ExternalViews\": { \"EmailMetadataSrc1\": [{\"fromAddress\": \"Barack Obama\",\"toAddress\": \"Hillary Clinton\",\"msgid\": 2012}]}} {\"id\": 125,\"label\": \"wh2.txt\",\"text\": \"DON'T PRESS IT!\",\"url\": \"www.whitehouse.org\",\"timeStamp\": \"22:01\",\"ExternalViews\": {\"EmailMetadataSrc1\": []}} Example 4: External views with an attribute of type Span In this example, you see a code snippet of a single module with a required Document schema with two fields, and an external view with a single field of type Span. module baseball; require document with columns text Text and url Text; create external view ExternalSpans (annotation Span) external_name 'ExternalSpansSrc'; The following example shows a JSON file that consists of one input document, with two tuples in the external view. The span in the first tuple has all three fields, including the optional field docref : {\"begin\":4, \"end\":11, \"docref\":\"url\"} and represents the span [4-11] on the text \u201cwww.ibm.com\u201d which is the value of the url field of the view Document , spanning the text \u201cibm.com\u201d. Notice that the span value in the second tuple {\"begin\":0, \"end\":4} does not have the optional docref attribute. This represents the span [0-4] on the text \u201cSome news\u201d which is the value of the text field of the view Document , therefore spanning the text \u201cSome\u201d. {\"text\":\"Some news\",\"url\":\"www.ibm.com\",\"ExternalViews\":{\"ExternalSpansSrc\":[{\"annotation\":{\"begin\":4, \"end\":11, \"docref\":\"url\"}},{\"annotation\":{\"begin\":0,\"end\":4}}]}","title":"Data formats for DocReader API"},{"location":"ana_txtan_input-coll-format/#data-collection-formats","text":"The DocReader API from SystemT low-level Java API supports specific formats to represent a data collection. A data collection is a document or a set of documents from which you want to extract text or discover patterns. Restriction: All input documents must be UTF-8 encoded files, or the analysis can produce unexpected results. Also, while the data collections must contain text files, the content of these text files is not restricted to plain text. Non-binary content such as HTML or XML is also accepted. A data collection must be in one of the following formats: Table of Contents {:toc}","title":"Data collection formats"},{"location":"ana_txtan_input-coll-format/#utf-8-encoded-text-files","text":"A data collection in this format can consist of a single UTF-8 encoded file, or a set of UTF-8 encoded text files, that are contained in a directory, archive, or a single comma-delimited file.","title":"UTF-8 encoded text files"},{"location":"ana_txtan_input-coll-format/#a-single-text-file","text":"A UTF-8 encoded single text file must have a .txt, .htm, .html, .xhtml, or .xml file extension. For a text file, the DocReader.next() API create one Document tuple with the schema as (text Text) or (text Text, label Text) based on the document schema of the loaded extractor. The value for the text field is the content of the file, and the value of the label field is the name of the file.","title":"A single text file"},{"location":"ana_txtan_input-coll-format/#text-files-in-a-directory","text":"A data collection format can be a directory that contains UTF-8 encoded text files. For each file in the directory, the DocReader.next() API create one Document tuple with the schema as (text Text)or (text Text, label Text) based on the document schema of the loaded extractor. The value of the text field is the content of the file, and the value of the label field is the name of the file.","title":"Text files in a directory"},{"location":"ana_txtan_input-coll-format/#text-files-in-an-archive","text":"A data collection can be an archive file in a .zip, .tar, .tar.gz, or .tgz archive format that contains UTF-8 encoded text files. For each file in the archive file, the DocReader.next() API create one Document tuple with the schema as (text Text) or (text Text, label Text) based on the document schema of the loaded extractor. The value for the text field is the content of the file, and the value of the label field is the name of the file.","title":"Text files in an archive"},{"location":"ana_txtan_input-coll-format/#a-single-comma-delimited-file","text":"A UTF-8 encoded comma-delimited file (.del) must be in the following format: Each row represents a single document. Rows are separated by new lines. Each row has three comma-separated columns that represent the identifier (of type Integer), label (of type String), and text (of type String). The label and text string values must be surrounded by the character string delimiter, which is a double quotation mark (\"). Any double quotation marks inside these values must be escaped (\"\"). The document label can be any character string (for example, a file name) which must be unique. For each row in the .del file, the DocReader.next() API create one Document tuple with the schema as (text Text) or (text Text, label Text) based on the document schema of the loaded extractor. The value for the text field is the value of the third column of the row, and the value of the label field is the value of the second column of the row. The following is an example of a .del file in this format that contains two documents with labels, doc_1.txt and doc_2.txt. 1,\"doc_1.txt\",\"This is an example document.\" 2,\"doc_2.txt\",\"This is an example document with \"\"double quotation marks\"\" that need to be escaped\"","title":"A single comma-delimited file"},{"location":"ana_txtan_input-coll-format/#utf-8-encoded-csv-files","text":"A data collection in this format can consist of a single UTF-8 encoded comma-delimited file with a header row.","title":"UTF-8 encoded CSV files"},{"location":"ana_txtan_input-coll-format/#a-single-comma-delimited-with-header-file","text":"A UTF-8 encoded comma-delimited file with header (.csv) must be in the following format: The first line of the file must be a header row that has comma-separated columns that represent the header or column-name, which specify the column field names. Columns can be of the data type Integer, Float, Text, or Boolean. Data of type Text must be surrounded by a double quotation mark (\"). In the following example of a CSV file, the field First_name is of type Text, so all of its values are enclosed in double quotation marks (such as \"Rohan\"). For each row in the CSV file, the DocReader.next() API create one Document tuple with the schema according to the document schema of the loaded extractor. You must specify the document schema in the AQL script by using the require document with columns statement. Headers that are not defined in the statement are ignored. The fields do not need to be specified in the order that they appear in the document. The following is an example of a CSV file in this format: First_name, Last_name, Id, Avg_score, Passed, Remarks \u201cRajeshwar\u201d, \u201cKalakuntla\u201d, 101, 70.96, true, \u201dPass \u2013 First class\u201d \u201cJayatheerthan\u201d, \u201cKrishna\u201d, 102, 85.96, true, \u201dPass - Distinction\u201d \u201cNisanth\u201d, \u201cSimon\u201d, 103, 80.96, true, \u201d Pass - Distinction\u201d \u201cRahul\u201d, \u201cKumar\u201d, 104, 49.96, false, \u201dFailed\u201d \u201cRohan\u201d, \u201cReddy\u201d, 105, 59.96, true, \u201dPass \u2013 Second class\u201d The first row of the CSV file is the header and it contains six columns: First_name , Last_name , Id , Avg_score , Passed , and Remarks .The remaining data is the CSV content. The following require document with columns statement establishes the column header labels and data types: require document with columns First_name Text and Last_name Text and Id Integer and Avg_score Float and Passed Boolean; Notice that the Remarks column is not used in the following statement, so the AQL will not have access to the Remarks column.","title":"A single comma-delimited with header file"},{"location":"ana_txtan_input-coll-format/#utf-8-encoded-json-files-in-hadoop-text-input-format","text":"A data collection in this format consists of a single UTF-8 encoded JSON (.json) file in Hadoop text input format. In this format, there is one JSON record per line. Each JSON record encodes the fields of one input document, and optionally, the external view content that is associated with that document. The document record is divided into multiple fields corresponding to a document schema (the structured format of the input document). Unlike the other data collection formats, which require a schema of (text Text, label Text), the JSON format allows the parsing of documents with a custom schema. The DocReader API aligns the schema that is read from the input JSON record with the expected input schema for the extractor. The expected input schema is the union of the schemas of its component modules (defined by the require document with columns statement of an AQL file) in that extractor. For example, if one module requires a document schema of (text Text) and another module requires a document schema of (id Integer), an extractor that is derived from both modules requires a document schema (text Text, id Integer). It is expected that the field names read from the JSON file are equal to the names of the extractor schema, and that the field types read from the JSON file are compatible with the types of the extractor schema. If the JSON input does not contain a required document field, an exception occurs. If the JSON input contains extraneous fields that are not specified by the extractor schema, the fields are ignored. In addition to the document schema fields, the document record can include a special field named ExternalViews. This field is a record where the name of each element is the external name of the external view (declared in AQL as external_name). The value of that element is an array of records. Each record corresponds to a tuple of an external view. As with the document schema, the field names and field types of each external view tuple must correspond with their defined schema in AQL. When a JSON input data file contains documents with ExternalViews fields, every document in that JSON data file must contain a field named ExternalViews , whether r not this field is populated with some external view content for that document. The following table shows the correspondence between JSON types and AQL types: AQL type JSON type Text String Integer Long Float Double Boolean Boolean String String Scalar list Array Span Record of the form {begin: Long, end: Long} or {begin: Long, end: Long, docref: String} A value of AQL type Span is represented as a JSON record with the following fields: begin of type JSON Long represents the begin offset of the span value. This field is mandatory. end of type JSON Long represents the end offset of the span value. This field is mandatory. docref of type JSON String represents the field of type Text in the view Document that holds the Text value that the span\u2019s begin and end offsets refer to. This attribute is optional. If missing, the span is assumed to be over the field text of the view Document .","title":"UTF-8 encoded JSON files in Hadoop text input format"},{"location":"ana_txtan_input-coll-format/#examples","text":"Requirement: The example data is formatted for readability, but each JSON record must be entered on a single line.","title":"Examples"},{"location":"ana_txtan_input-coll-format/#example-1-extractor-with-a-custom-document-schema","text":"In this example, you see a code snippet of a single module with a required Document schema of four fields, one field for each of the four AQL data types that are supported by the require document with columns statement. module baseball; require document with columns avg Float and hr Integer and mvp Boolean and name Text; The following example shows a JSON file that consists of three input documents that are suitable for use as input documents for the extractor. The example data is formatted for readability, but each JSON record must be on a single line. No external view contents are present in any of the three documents. The rbi field in the first document is not required by the document schema, and is therefore ignored. {\"avg\":0.357,\"hr\":40,\"mvp\":true,\"rbi\":139,\"name\":\"Cabrera, Miguel\"} {\"avg\":0.326,\"hr\":30,\"mvp\":false,\"name\":\"Trout, Mike\"} {\"avg\":0.219,\"hr\":1,\"mvp\":false,\"name\":\"Punto, Nick\"}","title":"Example 1: Extractor with a custom document schema"},{"location":"ana_txtan_input-coll-format/#example-2-extractor-that-uses-multiple-modules-with-external-views","text":"This example shows code snippets from various modules. Module email has Document schema (text Text) and an external view, EmailMetadata, with schema (toAddress Text, title Text) and external name, EmailMetadataSrc. module email; require document with columns text Text; create external view EmailMetadata(toAddress Text, title Text) external_name 'EmailMetadataSrc'; Module spam has Document schema (label Text) and one external view, SpamMetadata with schema (spamChance Float) and external name, SpamMetadataSrc. module spam; require document with columns label Text; create external view SpamMetadata(spamChance Float) external_name 'SpamMetadataSrc'; The following example shows a JSON file with a single document record that can be used as input to an extractor that combines both modules. This extractor has a unionized schema (label Text, text Text), and access to both external views. Notice the usage of external names of external views in the ExternalViews record. { \"label\": \"email.txt\", \"text\": \"Send this to 20 other people!\", \"ExternalViews\": { \"EmailMetadataSrc\": [ { \"toAddress\":\"John Doe\", \"title\":\"Make Money Fast\" }, { \"toAddress\":\"Jane Smith\", \"title\":\"Make Money Fast\" } ], \"SpamMetadataSrc\": [ { \"spamChance\":.999 } ] }}","title":"Example 2: Extractor that uses multiple modules with external views"},{"location":"ana_txtan_input-coll-format/#example-3-extractor-that-uses-a-module-with-external-views-but-not-all-of-the-external-views-are-populated","text":"The JSON input in this example contains documents with ExternalViews defined in each record, but the second record is not populated with external view content. module whitehouse; require document with columns id Integer and label Text and text Text and url Text and timeStamp Text; create external view EmailMetadata (fromAddress Text, toAddress Text, msgid Integer) external_name 'EmailMetadataSrc1'; output view EmailMetadata; create view dumpDocument as select D.id, D.label, D.text, D.url, D.timeStamp from Document D; output view dumpDocument; {\"id\": 123, \"label\": \"dem.whitehouse.txt\", \"text\": \"Yes we can!\", \"url\": \"www.whitehouse.gov\", \"timeStamp\": \"03:00\", \"ExternalViews\": { \"EmailMetadataSrc1\": [{\"fromAddress\": \"Barack Obama\",\"toAddress\": \"Hillary Clinton\",\"msgid\": 2012}]}} {\"id\": 125,\"label\": \"wh2.txt\",\"text\": \"DON'T PRESS IT!\",\"url\": \"www.whitehouse.org\",\"timeStamp\": \"22:01\",\"ExternalViews\": {\"EmailMetadataSrc1\": []}}","title":"Example 3: Extractor that uses a module with external views but not all of the external views are populated"},{"location":"ana_txtan_input-coll-format/#example-4-external-views-with-an-attribute-of-type-span","text":"In this example, you see a code snippet of a single module with a required Document schema with two fields, and an external view with a single field of type Span. module baseball; require document with columns text Text and url Text; create external view ExternalSpans (annotation Span) external_name 'ExternalSpansSrc'; The following example shows a JSON file that consists of one input document, with two tuples in the external view. The span in the first tuple has all three fields, including the optional field docref : {\"begin\":4, \"end\":11, \"docref\":\"url\"} and represents the span [4-11] on the text \u201cwww.ibm.com\u201d which is the value of the url field of the view Document , spanning the text \u201cibm.com\u201d. Notice that the span value in the second tuple {\"begin\":0, \"end\":4} does not have the optional docref attribute. This represents the span [0-4] on the text \u201cSome news\u201d which is the value of the text field of the view Document , therefore spanning the text \u201cSome\u201d. {\"text\":\"Some news\",\"url\":\"www.ibm.com\",\"ExternalViews\":{\"ExternalSpansSrc\":[{\"annotation\":{\"begin\":4, \"end\":11, \"docref\":\"url\"}},{\"annotation\":{\"begin\":0,\"end\":4}}]}","title":"Example 4: External views with an attribute of type Span"},{"location":"ana_txtan_optimizer/","text":"SystemT Optimizer Table of Contents {:toc} Overview One of the strengths of SystemT is the Optimizer. There are many ways to run a set of AQL statements. The Optimizer calculates different execution plans, evaluates these execution plans, and chooses a good plan from the alternatives. The final output of the Optimizer is a compiled execution plan . Execution plans in SystemT consist of graphs of operators . A SystemT execution plan can contain familiar relational operators, and specialized operators that find patterns and features in text, compact sets of spans, and perform unique extraction tasks. The idea of the SystemT Optimizer is similar to a relational database optimizer, but with text-specific optimizations and a text-specific cost model to select these optimizations. Unlike the optimizer in a relational database, the SystemT Optimizer is tuned to the needs of information extraction rules. The cost model focuses on the text-specific operations that tend to dominate run time. Unlike a traditional database where I/O functions are usually the most important cost, the SystemT cost model focuses on minimizing the processor cost of these text operations. The plan optimizations are also specific to information extraction. There are five types of optimization techniques: Shared Dictionary Matching (SDM) Regular Expression Strength Reduction (RSR) Shared Regular Expression Matching (SRM) Conditional Evaluation (CE) Restricted Span Evaluation (RSE) Types of optimizations Shared Dictionary Matching (SDM) The SystemT run time has a Dictionary operator that is called that implements the core of the extract dictionary statement. This operator matches an exhaustive dictionary of terms against the tokens in the document. Internally, this operator performs three different operations: The operator tokenizes the input text and essentially divides the text into individual words. The operator takes each input token, computes a hash value, and uses that hash code to probe into a hash table of dictionary entries. If it finds a match, the operator generates output tuples that contain the dictionary matches based on the data that is stored in the hash table entry. As is often the case, if there are multiple extract dictionary statements in the original AQL that target the same source text, it is possible to share the first two steps (tokenization and hashing) among all of those dictionaries. SystemT has a special Dictionaries operator that implements this sharing, and the Optimizer tries to generate this operator when it considers possible execution plans. This transformation from multiple individual Dictionary operators to the single Dictionaries operator is called Shared Dictionary Matching (SDM). The processor cost savings from this transformation can be substantial. Regular Expression Strength Reduction (RSR) The SystemT runtime component uses two engines to evaluate regular expressions: Java\u2019s built-in regular expression engine This engine supports a large class of regular expressions, including advanced constructs such as backreferences, lookahead, and lookbehind. To support such advanced constructs, Java's implementation is based on recursion and backtracking, which can be slow for certain types of regular expressions that are common in information extraction. The SimpleRegex engine To address the performance problems of the common regular expression engines, the SystemT runtime has its own regular expression evaluation engine called SimpleRegex. SimpleRegex is based on a design that is known as a Thompson NFA/DFA hybrid. The result is an engine that can run complex regular expressions that are common to information extraction applications much faster than a backtracking engine. Another benefit of this approach is that the engine can evaluate multiple regular expressions with one pass, using a single state machine. The main disadvantage of the Thompson NFA/DFA approach is that the advanced features of Java or POSIX regular expressions (such as backreferences and lookahead) are difficult to implement. Therefore, not every extract regex statement can be evaluated with SimpleRegex. Instead, the Optimizer performs a plan transformation called Regular Expression Strength Reduction (RSR). The Optimizer examines every regular expression that appears in an extract regex statement or in a scalar function call and determines whether that expression can be evaluated with the SimpleRegex engine. If the expression is SimpleRegex compatible, the Optimizer generates a plan with operators ( FastRegex and FastRegexTok ) that use the SimpleRegex engine. Otherwise, the Optimizer falls back on Java\u2019s built-in engine. You can determine whether RSR is being used if the FastRegex and FastRegexTok operators are in the execution plan. Shared Regular Expression Matching (SRM) The SimpleRegex engine that is used for Regular Expression Strength Reduction can evaluate many regular expressions in a single pass over the target text. This capability can lead to a major performance boost. The engine can take 10 regular expressions and combine them, and then evaluate all 10 in one pass in approximately 1/10th of the time. This transformation is called Shared Regular Expression Matching (SRM), and the Optimizer attempts to generate plans that take advantage of this capability. You can determine whether SRM is being used by looking for the RegexesTok operator in the execution plan. Note: Currently, the SRM optimization is only applied over extract regex statements that contain the on between X and Y tokens clause. This is one of the reasons that it is a good idea to use token-based regular expression matching in your AQL rules. Conditional Evaluation (CE) This optimization leverages the fact that the SystemT runtime processes one document at a time. If there is a join operator in the plan, the join produces no outputs on a document if one of the inputs is empty. The SystemT join operators take advantage of this fact by first evaluating the outer (left) input. The inner (right) argument is only evaluated if the outer produces at least one tuple on the current document. This approach is called Conditional Evaluation (CE), and the cost savings can be substantial. The Optimizer does not explicitly add operators to enable Conditional Evaluation. It is built into most of the join operators and is on constantly. Specifically, the Nested Loops Join ( NLJoin ), the Sort Merge Join ( SortMergeJoin ), the Hash Join ( HashJoin ), and the Adjacent Join ( AdjacentJoin ) operators all use Conditional Evaluation. The cost model of the Optimizer does take into account the probability that the inner (right) argument of a join operator will not need to be evaluated. Therefore, there are many execution plans where the order of the inputs to the join were changed from the original order in the select statement to take advantage of Conditional Evaluation. In addition, the Minus operator (which implements the minus statement in AQL) also uses Conditional Evaluation, although the Optimizer does not consider reversing the order of the inputs to that operator. Restricted Span Evaluation (RSE) The Restricted Span Evaluation (RSE) optimization is a more sophisticated version of Conditional Evaluation. This optimization is implemented by a special type of join operator that is called Restricted Span Evaluation Join ( RSEJoin ). The RSEJoin operator takes each tuple on the outer (left) input and sends information about that tuple down to the inner (or right) input. This information allows the operators on the inner input to avoid wasted work. For example, consider the following AQL code that finds person names, followed closely by phone numbers: create dictionary PersonNameDict from file 'person_names.dict'; create view PersonPhone as select Person.name as name, Phone.number as number from ( extract dictionary PersonNameDict on D.text as name from Document D ) Person, ( extract regex /\\d{3}\\-\\d{4}/ on 3 tokens in D.text as number from Document D ) Phone where FollowsTok(Person.name, Phone.number, 0, 5); The execution plan for the PersonPhone view would consist of a join between person names and phone numbers. A Dictionary operator is used to find person names, and a Regex operator is used to find phone numbers. The following diagram displays a high-level picture of three possible execution plans for the PersonPhone view: Plan A illustrates a conventional join plan that evaluates the dictionary and regular expression, then identifies pairs of spans from both sets of matches that satisfy the FollowsTok() predicate in the original where clause of the AQL statement. Plans B and C use Restricted Span Evaluation (RSE) to evaluate the PersonPhone view. The execution plan of Plan B only evaluates the regular expression on selected portions of the document. Plan C only evaluates the dictionary on selected portions of the document. This approach can greatly reduce the amount of time that is spent evaluating low-level primitives like regular expressions. This approach is particularly useful with expensive regular expressions that are not amenable to Regular Expression Strength Reduction (RSR). However, the inner operand of the join needs to be able to use the information from the outer, so the kinds of expressions where RSE can be applied are fairly limited. Specifically, RSE can only be used with joins that take Follows or FollowsTok() join predicates, and where one of the inputs to the join is an extract statement. Therefore, RSEJoin might not appear in execution plans often. Execution plan The final output of the Optimizer is a compiled execution plan. Execution plans in SystemT consist of graphs of operators. An operator is a module that performs specific extraction subtask, such as identifying matches of regular expressions within a string. These operators are tied together in a graph by connecting the output of one operator to the input of one or more other operators. Therefore, a compiled execution plan is also known as an Annotation Operator Graph (AOG). Consider the following AQL code sample that shows you how to read this text representation of an AOG. This example identifies patterns of numeric amounts followed by units of measurement, such as \u201c10 pounds\u201d or \u201c100 shillings\u201d. The Number view identifies the numeric amount, the Unit view uses a dictionary to find units of measurement, and the AmountWithUnit view combines these features into a single entity. create view Number as extract regex /\\d+/ on between 1 and 1 tokens in D.text as match from Document D; create view Unit as extract dictionary UnitDict on D.text as match from Document D; create view AmountWithUnit as select CombineSpans(N.match, U.match) as match from Number N, Unit U where FollowsTok(N.match, U.match, 0, 0); The following sample is a portion of the text representation of the AOG that corresponds to the previous AQL code: $AmountWithUnit = Project((\"FunctionCall30\" => \"match\"), ApplyFunc( CombineSpans( GetCol(\"N.match\"), GetCol(\"U.match\") ) => \"FunctionCall30\", AdjacentJoin( FollowsTok( GetCol(\"N.match\"), GetCol(\"U.match\"), IntConst(0), IntConst(0) ), Project((\"match\" => \"N.match\"), $Number ), Project((\"match\" => \"U.match\"), $Unit ) ) ) The text $AmountWithUnit = at the beginning of the AOG sample specifies that everything to the right of the equals sign is a text representation of the portion of the operator graph that implements the AmountWithUnit view. The information to the right of the equals sign contains the tree of operators that implements this view. You might be familiar with the Project operator, as it is the same projection operator from relational databases. This operator takes in a set of tuples, then rearranges and renames the fields of each individual tuple. In this example, the Project operator takes the input column that is called FunctionCall30 , renames that column to match , and drops all of the other columns in the input. The ApplyFunc operator evaluates function calls that appear in the select or extract list of an AQL statement. In this case, the original AQL select statement had a call to the built-in CombineSpans() function. The ApplyFunc operator calls this function on each input tuple, and passes the values of the columns N.match and U.match to the function as the first and second arguments. Each output tuple contains all of the columns of the input tuple, plus an extra FunctionCall30 column that contains the return value from the function. This FunctionCall30 column is what the Project operator projects to. The next operator in the sample is an AdjacentJoin operator. The SystemT runtime has several different implementations of the relational join operator that are specialized for different text-specific join predicates. In this case, the Optimizer determined that the best join implementation for the FollowsTok() predicate in the where clause of the original view is the AdjacentJoin operator. The first argument in the AOG is the join predicate. The second and third arguments are the two inputs to the join. Remember, a join operator takes two sets of tuples and finds all pairs of tuples from the two sets that satisfy a predicate. The inputs to the join come from two more Project operators, but the inputs to those operators are slightly different from the rest of this tree of operators. These inputs $Number and $Unit are actually references to other plan subtrees. If you were to examine the complete AOG representation of the operator graph, you would see a line that starts $Number = , with a subplan to the right of the equals sign. The reference to $Number in the current subplan tells the operator graph initializer that the input of the Project operator should be connected to that $Number subplan. In the initialization stage, the SystemT runtime reads the compiled plans in AOG representation of each compiled module of the extractor. Then, it stitches together plan subtrees within each module and across modules to produce a complete operator graph for the entire extractor. Operators A SystemT execution plan can contain familiar relational operators, and specialized operators that find patterns and features in text, compact sets of spans, and perform unique extraction tasks. There are four categories of operators in an execution plan, or Annotation Operator Graph (AOG). Relational operators Relational operators implement the different operations over sets of tuples. These operators are similar to operators from relational databases. Join operators Join operators take as input two sets of tuples and output a set of pairs of tuples that satisfy the join predicate. They are used to implement parts of the where clause of an AQL select statement. There are several kinds of join operators, corresponding to different built-in join predicates in AQL: Nested Loops Join (NLJoin) This operator uses the most na\u00efve join algorithm that examines every pair of input tuples. It is used for join predicates that are defined by using user-defined functions (UDFs), and built-in predicates that are not supported by other join algorithms, such as the Or() built-in predicate. Adjacent Join (AdjacentJoin) This operator is a specialized join operator that finds spans that are adjacent to each other. It is used to implement the FollowsTok() and FollowedByTok() built-in predicates. The AdjacentJoin operator uses an algorithm that performs very fast when the inputs are on token boundaries. Sort Merge Join (SortMergeJoin) This operator is a specialized join operator for comparing two spans. It is used to implement the Contains(), ContainedWithin(), Overlaps(), Follows(), FollowedBy(), FollowsTok(), and FollowedByTok() predicates. Hash Join (HashJoin) This operator is a specialized join operator for the Equals() predicate. Restricted Span Evaluation Join (RSEJoin) This operator is a specialized join operator that implements Restricted Span Evaluation (RSE). It evaluates the outer (first) argument, then passes information about the tuples that are produced by the first argument to the inner (second) argument. Other relational operators Other familiar relational operators include: Select This operator is used to implement the having clause of an extract statement, as well as parts of the where clause of a select statement that are not used as join predicates. Project This operator implements the final processing that is required for the select list of an AQL select or extract statement by dropping unnecessary columns in the input, and renaming the remaining columns as needed. Union This operator is used to implement the union all statement. Minus This operator is used to implement the minus statement. ApplyFunc This operator is used to compute the result of scalar function calls that appear in various parts of an AQL statement, such as in the select list of a select or extract statement. GroupBy This operator is used to implement the group by clause of an AQL statement. Sort This operator sorts the set of input tuples. It is used to implement the order by clause of a select statement. Span aggregation operators Span aggregation operators take sets of input spans and reduce them to more compact entities. There are three types of span aggregation operators: Block This operator groups simpler entities that are in close proximity to each other into larger entities. This operator implements the extract blocks statement when the distance between input spans is in characters. BlockTok This operator groups simpler entities that are in close proximity to each other into larger entities. This operator implements the extract blocks statement when the distance between input spans is in tokens. Consolidate The consolidate operator evaluates a collection of spans and removes overlapping spans according to a specified consolidation policy. This operator implements the consolidate clause in the select and extract statements. Span extraction operators Span extraction operators identify basic low-level patterns in text, usually for extracting features such as matches for regular expressions or dictionaries. Since these extraction primitives are crucial to overall performance, the system implements several variations of each operator and allows the Optimizer to choose the best option. Regular expression operators These operators identify matches of one or more regular expressions across the input text. They implement the AQL extract regex statement. There are multiple types of this kind of operator: RegularExpression This operator evaluates a single regular expression by using the Java regular expression engine with character-based matching semantics. RegexTok This operator evaluates a single regular expression by using the Java regular expression engine with token-based matching semantics. FastRegex This operator evaluates a single regular expression by using the SimpleRegex expression engine with character-based matching semantics. This operator is the outcome of applying the Regular Expression Strength Reduction (RSR) optimization. FastRegexTok This operator evaluates a single regular expression by using the SimpleRegex regular expression engine with token-based matching semantics. This operator is the outcome of applying the Regular Expression Strength Reduction (RSR) optimization. RegexesTok This operator evaluates multiple regular expressions at the same time. This operator is the outcome of applying the Shared Regular Expression Matching (SRM) optimization. Dictionary operators Dictionary operators are used to implement the AQL extract dictionary statement. There are two versions of this operator: Dictionary This operator evaluates a single dictionary. Dictionaries This operator evaluates multiple dictionaries at the same time, and it is the outcome of applying the Shared Dictionary Matching (SDM) optimization. PartsOfSpeech operator This operator identifies locations of common parts of speech across the input text. Specialized operators Specialized operators perform special kinds of extraction that do not fit in other operator categories. There are two types of specialized operators in SystemT: Split This operator splits a span in to multiple subparts based on a set of input spans that mark boundaries. This operator implements the AQL extract split statement. Detag HTML/XML This operator detags the input text, optionally retaining the position and content of specific tags. This operator implements the AQL detag statement.","title":"The SystemT Optimizer"},{"location":"ana_txtan_optimizer/#systemt-optimizer","text":"Table of Contents {:toc}","title":"SystemT Optimizer"},{"location":"ana_txtan_optimizer/#overview","text":"One of the strengths of SystemT is the Optimizer. There are many ways to run a set of AQL statements. The Optimizer calculates different execution plans, evaluates these execution plans, and chooses a good plan from the alternatives. The final output of the Optimizer is a compiled execution plan . Execution plans in SystemT consist of graphs of operators . A SystemT execution plan can contain familiar relational operators, and specialized operators that find patterns and features in text, compact sets of spans, and perform unique extraction tasks. The idea of the SystemT Optimizer is similar to a relational database optimizer, but with text-specific optimizations and a text-specific cost model to select these optimizations. Unlike the optimizer in a relational database, the SystemT Optimizer is tuned to the needs of information extraction rules. The cost model focuses on the text-specific operations that tend to dominate run time. Unlike a traditional database where I/O functions are usually the most important cost, the SystemT cost model focuses on minimizing the processor cost of these text operations. The plan optimizations are also specific to information extraction. There are five types of optimization techniques: Shared Dictionary Matching (SDM) Regular Expression Strength Reduction (RSR) Shared Regular Expression Matching (SRM) Conditional Evaluation (CE) Restricted Span Evaluation (RSE)","title":"Overview"},{"location":"ana_txtan_optimizer/#types-of-optimizations","text":"","title":"Types of optimizations"},{"location":"ana_txtan_optimizer/#shared-dictionary-matching-sdm","text":"The SystemT run time has a Dictionary operator that is called that implements the core of the extract dictionary statement. This operator matches an exhaustive dictionary of terms against the tokens in the document. Internally, this operator performs three different operations: The operator tokenizes the input text and essentially divides the text into individual words. The operator takes each input token, computes a hash value, and uses that hash code to probe into a hash table of dictionary entries. If it finds a match, the operator generates output tuples that contain the dictionary matches based on the data that is stored in the hash table entry. As is often the case, if there are multiple extract dictionary statements in the original AQL that target the same source text, it is possible to share the first two steps (tokenization and hashing) among all of those dictionaries. SystemT has a special Dictionaries operator that implements this sharing, and the Optimizer tries to generate this operator when it considers possible execution plans. This transformation from multiple individual Dictionary operators to the single Dictionaries operator is called Shared Dictionary Matching (SDM). The processor cost savings from this transformation can be substantial.","title":"Shared Dictionary Matching (SDM)"},{"location":"ana_txtan_optimizer/#regular-expression-strength-reduction-rsr","text":"The SystemT runtime component uses two engines to evaluate regular expressions: Java\u2019s built-in regular expression engine This engine supports a large class of regular expressions, including advanced constructs such as backreferences, lookahead, and lookbehind. To support such advanced constructs, Java's implementation is based on recursion and backtracking, which can be slow for certain types of regular expressions that are common in information extraction. The SimpleRegex engine To address the performance problems of the common regular expression engines, the SystemT runtime has its own regular expression evaluation engine called SimpleRegex. SimpleRegex is based on a design that is known as a Thompson NFA/DFA hybrid. The result is an engine that can run complex regular expressions that are common to information extraction applications much faster than a backtracking engine. Another benefit of this approach is that the engine can evaluate multiple regular expressions with one pass, using a single state machine. The main disadvantage of the Thompson NFA/DFA approach is that the advanced features of Java or POSIX regular expressions (such as backreferences and lookahead) are difficult to implement. Therefore, not every extract regex statement can be evaluated with SimpleRegex. Instead, the Optimizer performs a plan transformation called Regular Expression Strength Reduction (RSR). The Optimizer examines every regular expression that appears in an extract regex statement or in a scalar function call and determines whether that expression can be evaluated with the SimpleRegex engine. If the expression is SimpleRegex compatible, the Optimizer generates a plan with operators ( FastRegex and FastRegexTok ) that use the SimpleRegex engine. Otherwise, the Optimizer falls back on Java\u2019s built-in engine. You can determine whether RSR is being used if the FastRegex and FastRegexTok operators are in the execution plan.","title":"Regular Expression Strength Reduction (RSR)"},{"location":"ana_txtan_optimizer/#shared-regular-expression-matching-srm","text":"The SimpleRegex engine that is used for Regular Expression Strength Reduction can evaluate many regular expressions in a single pass over the target text. This capability can lead to a major performance boost. The engine can take 10 regular expressions and combine them, and then evaluate all 10 in one pass in approximately 1/10th of the time. This transformation is called Shared Regular Expression Matching (SRM), and the Optimizer attempts to generate plans that take advantage of this capability. You can determine whether SRM is being used by looking for the RegexesTok operator in the execution plan. Note: Currently, the SRM optimization is only applied over extract regex statements that contain the on between X and Y tokens clause. This is one of the reasons that it is a good idea to use token-based regular expression matching in your AQL rules.","title":"Shared Regular Expression Matching (SRM)"},{"location":"ana_txtan_optimizer/#conditional-evaluation-ce","text":"This optimization leverages the fact that the SystemT runtime processes one document at a time. If there is a join operator in the plan, the join produces no outputs on a document if one of the inputs is empty. The SystemT join operators take advantage of this fact by first evaluating the outer (left) input. The inner (right) argument is only evaluated if the outer produces at least one tuple on the current document. This approach is called Conditional Evaluation (CE), and the cost savings can be substantial. The Optimizer does not explicitly add operators to enable Conditional Evaluation. It is built into most of the join operators and is on constantly. Specifically, the Nested Loops Join ( NLJoin ), the Sort Merge Join ( SortMergeJoin ), the Hash Join ( HashJoin ), and the Adjacent Join ( AdjacentJoin ) operators all use Conditional Evaluation. The cost model of the Optimizer does take into account the probability that the inner (right) argument of a join operator will not need to be evaluated. Therefore, there are many execution plans where the order of the inputs to the join were changed from the original order in the select statement to take advantage of Conditional Evaluation. In addition, the Minus operator (which implements the minus statement in AQL) also uses Conditional Evaluation, although the Optimizer does not consider reversing the order of the inputs to that operator.","title":"Conditional Evaluation (CE)"},{"location":"ana_txtan_optimizer/#restricted-span-evaluation-rse","text":"The Restricted Span Evaluation (RSE) optimization is a more sophisticated version of Conditional Evaluation. This optimization is implemented by a special type of join operator that is called Restricted Span Evaluation Join ( RSEJoin ). The RSEJoin operator takes each tuple on the outer (left) input and sends information about that tuple down to the inner (or right) input. This information allows the operators on the inner input to avoid wasted work. For example, consider the following AQL code that finds person names, followed closely by phone numbers: create dictionary PersonNameDict from file 'person_names.dict'; create view PersonPhone as select Person.name as name, Phone.number as number from ( extract dictionary PersonNameDict on D.text as name from Document D ) Person, ( extract regex /\\d{3}\\-\\d{4}/ on 3 tokens in D.text as number from Document D ) Phone where FollowsTok(Person.name, Phone.number, 0, 5); The execution plan for the PersonPhone view would consist of a join between person names and phone numbers. A Dictionary operator is used to find person names, and a Regex operator is used to find phone numbers. The following diagram displays a high-level picture of three possible execution plans for the PersonPhone view: Plan A illustrates a conventional join plan that evaluates the dictionary and regular expression, then identifies pairs of spans from both sets of matches that satisfy the FollowsTok() predicate in the original where clause of the AQL statement. Plans B and C use Restricted Span Evaluation (RSE) to evaluate the PersonPhone view. The execution plan of Plan B only evaluates the regular expression on selected portions of the document. Plan C only evaluates the dictionary on selected portions of the document. This approach can greatly reduce the amount of time that is spent evaluating low-level primitives like regular expressions. This approach is particularly useful with expensive regular expressions that are not amenable to Regular Expression Strength Reduction (RSR). However, the inner operand of the join needs to be able to use the information from the outer, so the kinds of expressions where RSE can be applied are fairly limited. Specifically, RSE can only be used with joins that take Follows or FollowsTok() join predicates, and where one of the inputs to the join is an extract statement. Therefore, RSEJoin might not appear in execution plans often.","title":"Restricted Span Evaluation (RSE)"},{"location":"ana_txtan_optimizer/#execution-plan","text":"The final output of the Optimizer is a compiled execution plan. Execution plans in SystemT consist of graphs of operators. An operator is a module that performs specific extraction subtask, such as identifying matches of regular expressions within a string. These operators are tied together in a graph by connecting the output of one operator to the input of one or more other operators. Therefore, a compiled execution plan is also known as an Annotation Operator Graph (AOG). Consider the following AQL code sample that shows you how to read this text representation of an AOG. This example identifies patterns of numeric amounts followed by units of measurement, such as \u201c10 pounds\u201d or \u201c100 shillings\u201d. The Number view identifies the numeric amount, the Unit view uses a dictionary to find units of measurement, and the AmountWithUnit view combines these features into a single entity. create view Number as extract regex /\\d+/ on between 1 and 1 tokens in D.text as match from Document D; create view Unit as extract dictionary UnitDict on D.text as match from Document D; create view AmountWithUnit as select CombineSpans(N.match, U.match) as match from Number N, Unit U where FollowsTok(N.match, U.match, 0, 0); The following sample is a portion of the text representation of the AOG that corresponds to the previous AQL code: $AmountWithUnit = Project((\"FunctionCall30\" => \"match\"), ApplyFunc( CombineSpans( GetCol(\"N.match\"), GetCol(\"U.match\") ) => \"FunctionCall30\", AdjacentJoin( FollowsTok( GetCol(\"N.match\"), GetCol(\"U.match\"), IntConst(0), IntConst(0) ), Project((\"match\" => \"N.match\"), $Number ), Project((\"match\" => \"U.match\"), $Unit ) ) ) The text $AmountWithUnit = at the beginning of the AOG sample specifies that everything to the right of the equals sign is a text representation of the portion of the operator graph that implements the AmountWithUnit view. The information to the right of the equals sign contains the tree of operators that implements this view. You might be familiar with the Project operator, as it is the same projection operator from relational databases. This operator takes in a set of tuples, then rearranges and renames the fields of each individual tuple. In this example, the Project operator takes the input column that is called FunctionCall30 , renames that column to match , and drops all of the other columns in the input. The ApplyFunc operator evaluates function calls that appear in the select or extract list of an AQL statement. In this case, the original AQL select statement had a call to the built-in CombineSpans() function. The ApplyFunc operator calls this function on each input tuple, and passes the values of the columns N.match and U.match to the function as the first and second arguments. Each output tuple contains all of the columns of the input tuple, plus an extra FunctionCall30 column that contains the return value from the function. This FunctionCall30 column is what the Project operator projects to. The next operator in the sample is an AdjacentJoin operator. The SystemT runtime has several different implementations of the relational join operator that are specialized for different text-specific join predicates. In this case, the Optimizer determined that the best join implementation for the FollowsTok() predicate in the where clause of the original view is the AdjacentJoin operator. The first argument in the AOG is the join predicate. The second and third arguments are the two inputs to the join. Remember, a join operator takes two sets of tuples and finds all pairs of tuples from the two sets that satisfy a predicate. The inputs to the join come from two more Project operators, but the inputs to those operators are slightly different from the rest of this tree of operators. These inputs $Number and $Unit are actually references to other plan subtrees. If you were to examine the complete AOG representation of the operator graph, you would see a line that starts $Number = , with a subplan to the right of the equals sign. The reference to $Number in the current subplan tells the operator graph initializer that the input of the Project operator should be connected to that $Number subplan. In the initialization stage, the SystemT runtime reads the compiled plans in AOG representation of each compiled module of the extractor. Then, it stitches together plan subtrees within each module and across modules to produce a complete operator graph for the entire extractor.","title":"Execution plan"},{"location":"ana_txtan_optimizer/#operators","text":"A SystemT execution plan can contain familiar relational operators, and specialized operators that find patterns and features in text, compact sets of spans, and perform unique extraction tasks. There are four categories of operators in an execution plan, or Annotation Operator Graph (AOG).","title":"Operators"},{"location":"ana_txtan_optimizer/#relational-operators","text":"Relational operators implement the different operations over sets of tuples. These operators are similar to operators from relational databases. Join operators Join operators take as input two sets of tuples and output a set of pairs of tuples that satisfy the join predicate. They are used to implement parts of the where clause of an AQL select statement. There are several kinds of join operators, corresponding to different built-in join predicates in AQL: Nested Loops Join (NLJoin) This operator uses the most na\u00efve join algorithm that examines every pair of input tuples. It is used for join predicates that are defined by using user-defined functions (UDFs), and built-in predicates that are not supported by other join algorithms, such as the Or() built-in predicate. Adjacent Join (AdjacentJoin) This operator is a specialized join operator that finds spans that are adjacent to each other. It is used to implement the FollowsTok() and FollowedByTok() built-in predicates. The AdjacentJoin operator uses an algorithm that performs very fast when the inputs are on token boundaries. Sort Merge Join (SortMergeJoin) This operator is a specialized join operator for comparing two spans. It is used to implement the Contains(), ContainedWithin(), Overlaps(), Follows(), FollowedBy(), FollowsTok(), and FollowedByTok() predicates. Hash Join (HashJoin) This operator is a specialized join operator for the Equals() predicate. Restricted Span Evaluation Join (RSEJoin) This operator is a specialized join operator that implements Restricted Span Evaluation (RSE). It evaluates the outer (first) argument, then passes information about the tuples that are produced by the first argument to the inner (second) argument. Other relational operators Other familiar relational operators include: Select This operator is used to implement the having clause of an extract statement, as well as parts of the where clause of a select statement that are not used as join predicates. Project This operator implements the final processing that is required for the select list of an AQL select or extract statement by dropping unnecessary columns in the input, and renaming the remaining columns as needed. Union This operator is used to implement the union all statement. Minus This operator is used to implement the minus statement. ApplyFunc This operator is used to compute the result of scalar function calls that appear in various parts of an AQL statement, such as in the select list of a select or extract statement. GroupBy This operator is used to implement the group by clause of an AQL statement. Sort This operator sorts the set of input tuples. It is used to implement the order by clause of a select statement.","title":"Relational operators"},{"location":"ana_txtan_optimizer/#span-aggregation-operators","text":"Span aggregation operators take sets of input spans and reduce them to more compact entities. There are three types of span aggregation operators: Block This operator groups simpler entities that are in close proximity to each other into larger entities. This operator implements the extract blocks statement when the distance between input spans is in characters. BlockTok This operator groups simpler entities that are in close proximity to each other into larger entities. This operator implements the extract blocks statement when the distance between input spans is in tokens. Consolidate The consolidate operator evaluates a collection of spans and removes overlapping spans according to a specified consolidation policy. This operator implements the consolidate clause in the select and extract statements.","title":"Span aggregation operators"},{"location":"ana_txtan_optimizer/#span-extraction-operators","text":"Span extraction operators identify basic low-level patterns in text, usually for extracting features such as matches for regular expressions or dictionaries. Since these extraction primitives are crucial to overall performance, the system implements several variations of each operator and allows the Optimizer to choose the best option. Regular expression operators These operators identify matches of one or more regular expressions across the input text. They implement the AQL extract regex statement. There are multiple types of this kind of operator: RegularExpression This operator evaluates a single regular expression by using the Java regular expression engine with character-based matching semantics. RegexTok This operator evaluates a single regular expression by using the Java regular expression engine with token-based matching semantics. FastRegex This operator evaluates a single regular expression by using the SimpleRegex expression engine with character-based matching semantics. This operator is the outcome of applying the Regular Expression Strength Reduction (RSR) optimization. FastRegexTok This operator evaluates a single regular expression by using the SimpleRegex regular expression engine with token-based matching semantics. This operator is the outcome of applying the Regular Expression Strength Reduction (RSR) optimization. RegexesTok This operator evaluates multiple regular expressions at the same time. This operator is the outcome of applying the Shared Regular Expression Matching (SRM) optimization. Dictionary operators Dictionary operators are used to implement the AQL extract dictionary statement. There are two versions of this operator: Dictionary This operator evaluates a single dictionary. Dictionaries This operator evaluates multiple dictionaries at the same time, and it is the outcome of applying the Shared Dictionary Matching (SDM) optimization. PartsOfSpeech operator This operator identifies locations of common parts of speech across the input text.","title":"Span extraction operators"},{"location":"ana_txtan_optimizer/#specialized-operators","text":"Specialized operators perform special kinds of extraction that do not fit in other operator categories. There are two types of specialized operators in SystemT: Split This operator splits a span in to multiple subparts based on a set of input spans that mark boundaries. This operator implements the AQL extract split statement. Detag HTML/XML This operator detags the input text, optionally retaining the position and content of specific tags. This operator implements the AQL detag statement.","title":"Specialized operators"},{"location":"aql-guidelines/","text":"Guidelines for writing AQL Table of Contents {:toc} Best practices in organizing AQL code You can use these guidelines to develop efficient extractors by using good practices of AQL development. An extractor is a set of compiled AQL modules that work together to accomplish an extraction task. Prior to extractor development, it is important to conduct an analysis of your extraction task to establish your extraction requirements. For example, if you want to analyze the IBM\u00ae quarterly earnings reports, you might develop extractors to help answer the question, \u201cHow do quarterly revenues for different IBM divisions change over the years?\u201d To answer the question, you first need to find the mentions of quarterly revenue for each IBM division in each quarterly earnings report. The primary extraction task in this case is to extract quarterly revenue for each IBM division. Extractor development is generally an iterative process and you can follow some basic steps: Identify the basic building blocks of your extractor. Start with the most basic clues that appear in the text that you want to analyze with the extractor. In this step, you create the AQL statements for a set of basic entity extraction views. Combine the basic entities by using simple patterns to form more concise views. As a result, you have a set of statements that create more complex entities that are candidates for finalization. Review all of the candidates, and create or edit your AQL statements to filter the valid from the invalid results. You can apply AQL filtering and consolidation techniques to achieve this step. In some cases, you might need to use advanced AQL syntax to further refine your results. The ability to modularize source AQL code is a great benefit. You can ensure that multiple concepts are well-separated in terms of rule sets and patterns that are used. There are four basic questions to begin to guide you in the design process. Question 1: How complex is your extractor? Simple extractors are a single AQL module. The more complex the extractor, the greater the need to modularize it. Question 2: Does your extractor have objects that are used in other extractors? An object refers to individual views, tables, dictionaries, functions, or any combinations of them. Identify AQL views, tables, dictionaries, and functions that are common across multiple versions of your extractor to create modules that can be reused by other specialized modules. Use the output view \u2026 as \u2026 statement to ensure that different specialized modules output a consistent set of names. Examples of such objects are apparent in the following two scenarios: You have one UDF JAR file that you use in each of your AQL projects. You have an extractor that you use in other extractors. For example, you use an organization extractor in another extractor that identifies financial events, or in an extractor that identifies relationships between persons and organizations. Question 3: Would you like to expose objects inside your extractor towards extractor customization? One useful way to customize the behavior of an extractor is to abstract information that is used by the extractor to arrive at a decision (such as dictionaries and tables). Then, the consumer of the compiled extractor can complete these values when the extractor is applied to a specific domain. In this way, the same extractor can be applied to different domains, each time with a different set of entries for the customizable dictionaries and tables. For example, assume that you have a person extractor that is used to highlight mentions in an email inbox of a customer. You would like to share this extractor with multiple customers, and allow each one to customize the extractor with names from the employee directory of their organization, without providing the source AQL code and recompiling the extractor. You can expose these customization points by using the AQL statements that create an external dictionary and create an external table. Question 4: Are there domain-specific components to your extractor? Are there any common components used by these domain-specific components? Create specialized modules that provide different implementations for the same semantic concept, where each implementation is customized for a particular application, domain, or language. It can be difficult to create generic AQL extractors that perform well on every possible domain or language. In general, a generic extractor requires customization to improve the accuracy and coverage on a specific domain. Such customization generally falls into three categories: Application customization You have a generic extractor that is used in multiple applications. Each application has specific requirements on the output schema, or the types of mentions to be extracted. For example, you use a person extractor in two different applications. In one application, the extractor does not output mentions that consist of a single token (for example, mentions of the given name of a person, without a mention of their surname). In the second application, such single-token mentions are acceptable. In addition, the output schema of the extractor differs across the two applications. In one application, a single attribute for the full name is sufficient. The second application requires extra attributes in addition to the full name: the given name, the surname, and the job position if present in the text. Data source customization You have an extractor that is applied to several different types of data sources. Each data source has specifics that require specialized rules. For example, you have an extractor for location mentions that is applied on two types of text: formal news reports and emails. In formal news report, you know that there is a mention of the location at the beginning of the first line. You want to write a special rule to capture that location, but you do not want to apply that rule when the extractor is used on emails. Language customization Formal language and grammar structure varies across languages. If your extractor applies to multiple languages, you might want to implement rules that are applicable to a single language. For example, you have a sentiment extractor that is applied to documents in English and French. You might have a few specialized rules that apply only to English text. Similarly, you might have a few specialized rules that apply only to French text. In cases of language customization, use the set default dictionary language statement to set the default dictionary languages for each module according to the language it is intended to work with. Examples of frequently used AQL statements Included in the examples of how to use the AQL statements to accomplish various extraction tasks are optimization guidelines. These guidelines describe good choices to make when you write your AQL code so that the Optimizer has the most latitude in choosing efficient execution plans. Using basic feature AQL statements You can use the basic feature AQL statements to develop the core building blocks of your extractor. Then, combine and filter your AQL statements to refine your results. Using candidate generation AQL statements You can refine the basic extractor that you started with by combining entity extraction views. The candidate generation AQL statements combine basic features by using simple patterns to form larger groups of features, which further refine the concepts. Using filter and consolidate AQL statements You can use filter and consolidate statements to refine results, to remove invalid annotations, and to resolve overlap between annotations. Creating complex AQL statements In some cases, you might need to use advanced AQL syntax to further refine your results. To develop more complex extractors, you can build on the basic design patterns in AQL with these advanced patterns. Enhancing content of AQL views To output something other than text matches from your existing results, you can use AQL statements to transform data, specify fixed output, and add additional metadata. Using basic feature AQL statements You can use the basic feature AQL statements to develop the core building blocks of your extractor. Then, combine and filter your AQL statements to refine your results. An AQL module generally consists of multiple create view statements and one or more output view statements. Each view can also include the use of dictionaries, tables, and functions, as well as references to other views. The following examples explain how to extract basic features, such as financial indicators, from unstructured text. For example, you might want to begin by extracting numbers, units, and metrics. When you want to match text that is based on a character-based pattern Generally, a character-based pattern is recognizable in the text. You might see that there is always a sequence of letters followed by numbers, or numbers with known separators in between. For example, if you were to see \u201c(123) 456-7890\u201d or \u201c123-45-6789\u201d in text, you can easily specify a pattern that matches either sequence. When you analyze text, you can use a regular expression when you want to match text that is based on a character pattern. The following is an example of a create view statement that uses a regular expression to extract decimal numbers: -- Identify mentions of numbers with optional decimals -- Example: 7, 49, 11.2 create view Number as extract regex /\\d+(\\.\\d+)?/ on R.text as number from Document R; As you analyze text with regular expressions, follow the optimization guidelines for regular expressions whenever possible. When you want to find matches for a fixed set of words or phrases Use a dictionary when you want to find matches for a fixed set of words or phrases. You can list the terms to be included in the dictionary either inline (in an AQL file) or externally (in a .dict file). The following is an example of how to create an inline dictionary: create dictionary MyDict as ('Finance'); This statement creates a dictionary with just one entry, \u2018Finance\u2019 . Use an external file when you have many dictionary entries. The external file makes it easier to add and change entries without having to edit the AQL program. By default, dictionaries are tokenized and internalized at compile time, but you can use the create external dictionary statement to provide dictionaries at run time. The following sample creates a dictionary from a file on your local system: -- use a dictionary to contain a list of terms for matching purposes create dictionary MyDict as ('Finance'); -- Define a dictionary of financial amount units -- Example: million, billion create dictionary UnitDict from file 'dictionaries/unit.dict' with language as 'en'; The file dictionaries/unit.dict contains: million billion For more information about optimization by using dictionaries, follow the guidelines for regular expressions. Using candidate generation AQL statements You can refine the basic extractor that you started with by combining entity extraction views. The candidate generation AQL statements combine basic features by using simple patterns to form larger groups of features, which further refine the concepts. The following examples explain how to extract candidate features from unstructured text. For example, you might want to combine the number and the unit by using the previously built basic feature AQL statements together to produce an amount. Or you might combine a metric with an amount to produce a revenue indicator. The following common tasks use these types of AQL statements: When you want to combine information from multiple views Use the select clause inside of a create view statement to combine, or join, information from multiple views. In this example, the goal is to find a number, followed within 0 to 10 tokens, by a unit. Notice how the built-in scalar function FollowsTok is used as join predicate between the views Number and Unit . Alternatively, you can use the extract pattern statement. -- extract metrics like 46.1, $8.7 create view Number as extract regex /$?\\p{Nd}+(\\.\\p{Nd}+)?/ on D.text as match from Document D; create dictionary UnitDict as ('percent', 'billion', 'million', 'trillion'); -- extract amount cues like 'percent', 'billion' create view Unit as extract dictionary 'UnitDict' on D.text as match from Document D; -- Identify candidate indicators as a mention of metric followed within -- 0 to 10 tokens of a mention of amount -- Example: Gross profit margin of 46.1 percent, cash flow of $8.7 billion create view IndicatorCandidate as select N.match as number, U.match as unit, CombineSpans(N.match, U.match) as match from Number N, Unit U where FollowsTok(N.match, U.match, 0, 10); -- Alternatively, you can achieve the same with a much simpler statement create view IndicatorCandidate as extract N.match as number, U.match as unit, pattern <N.match> <Token>{0,10} <U.match> as match from Number N, Unit U; As you combine information from multiple views, follow the optimization guidelines and Avoid Cartesian products , Use the and keyword instead of the And() built-in predicate , and Be careful when using Or() as a join predicate . When you want to combine results from different views into a single view In the following example, a union allclause is used inside of a create view statement to capture a union of entities that were specified with different AQL statements: create dictionary QuantityAbsoluteDict as ('billion', 'trillion', 'million'); create dictionary QuantityPercentDict as ('percent', 'percentile', 'percentage'); create view QuantityAbsolute as select I.* from IndicatorCandidate I where MatchesDict('QuantityAbsoluteDict', I.amount); create view QuantityPercent as select I.* from IndicatorCandidate I where MatchesDict('QuantityPercentDict', I.amount); -- Union all absolute and percentage amount candidates -- Example: $7 billion, $11.52, 49 percent, 46.1 percent create view AmountCandidate as (select R.* from QuantityAbsolute R) union all (select R.* from QuantityPercent R); When you want to extract chunks of data Use the blocks extraction specification to identify blocks of contiguous spans across input text. This example shows how to identify blocks of exactly two capitalized words within zero to five tokens of each other: -- A single capitalized word create view CapitalizedWords as extract regex /[A-Z][a-z]+/ on 1 token in D.text as word from Document D; -- extract blocks of exactly two capitalized words within 0-5 tokens of each other create view TwoCapitalizedWords as extract blocks with count 2 and separation between 0 and 5 tokens on CW.word as capswords from CapitalizedWords CW; When you want to identify patterns of simple text or fields from other views In the following example, an amount entity is extracted with the help of a simple pattern involving other entities: -- extract metrics like 46.1, 8.7 create view Number as extract regex /\\p{Nd}+(\\.\\p{Nd}+)?/ on D.text as match from Document D; create dictionary UnitDict as ('percent', 'billion', 'million', 'trillion'); -- extract amount cues like 'percent', 'billion' create view Unit as extract dictionary 'UnitDict' on D.text as match from Document D; -- Identify mentions of absolute amounts as a sequence of '$' character, -- followed by a Number mention, optionally followed by a Unit mention -- Example: $7 billion, $11.52 create view AmountAbsolute as extract pattern /\\$/ <N.match> <U.match>? return group 0 as match from Number N, Unit U consolidate on match; As you identify patterns of text or fields from other views, follow the optimization guideline and Define common subpatterns explicitly . Using filter and consolidate AQL statements You can use filter and consolidate statements to refine results, to remove invalid annotations, and to resolve overlap between annotations. When you want to remove data that overlaps In the following example, the consolidate on clause is used to remove overlapping spans held by the column on which the consolidate clause is applied. -- extract metrics like 46.1, $8.7 create view Number as extract regex /$?\\p{Nd}+(\\.\\p{Nd}+)?/ on D.text as match from Document D; create dictionary UnitDict as ('percent', 'billion', 'million', 'trillion'); -- extract amount cues like 'percent', 'billion' create view Unit as extract dictionary 'UnitDict' on D.text as match from Document D; -- Identify candidate indicators as a mention of metric followed within -- 0 to 10 tokens of a mention of amount -- consolidate overlapping mentions -- Example: Gross profit margin of 46.1 percent, cash flow of $8.7 billion create view IndicatorCandidate as select N.match as amount, U.match as metric, CombineSpans(N.match, U.match) as match from Number N, Unit U where FollowsTok(N.match, U.match, 0, 10); create view IndicatorConsolidated as select R.* from IndicatorCandidate R consolidate on R.match using 'NotContainedWithin'; When you want to remove data that overlaps, follow the optimization guideline and Use the consolidate clause wisely . When you want to filter information based on a dictionary term or similar terms You can use a predicate-based filter to remove similar data. The following example shows a simple filter condition that is based on information from the dictionary that you created: -- Negative clues that signal a relative amount -- Example: increased, decreased, down, up create dictionary AmountNegativeClueDict with language as 'en' as ('points', 'debit', 'credit'); -- Narrow down the results to the candidate indicators which are absolute: -- the span between the metric and amount does not contain -- a \"relative amount\" term create view IndicatorAbsoluteCandidate as select I.metric as metric, I.amount as amount, I.match as match from IndicatorConsolidated I where Not(MatchesDict('AmountNegativeClueDict', SpanBetween(I.metric, I.amount))); When you want to use a view to filter data from a larger subset of data You can use a minus clause to express complex filter conditions for which predicate-based filters are not sufficient. create dictionary MetricDict as ('revenue', 'Revenue'); create view Metric as extract dictionary 'MetricDict' on D.text as match from Document D; -- Identify one type of invalid Indicator candidates: mentions that contain -- another metric in between the Metric and Amount mentions -- Example: -- [EPS growth]; Revenue of [$99.9 billion] -- [revenue] up 19 percent; Free cash flow of [$8.7 billion] create view IndicatorInvalid as select R.* from IndicatorConsolidated R, Metric M where Contains(SpanBetween(R.metric, R.amount), M.match); -- Filter out invalid Indicator mentions from the set of all -- Indicator candidates create view IndicatorAll as (select R.* from IndicatorConsolidated R) minus (select R.* from IndicatorInvalid R); Creating complex AQL statements In some cases, you might need to use advanced AQL syntax to further refine your results. To develop more complex extractors, you can build on the basic design patterns in AQL with these advanced patterns. The following are examples of advanced AQL design patterns: When you want to remove HTML or XML tags and extract information based on the tags The detag statement is used to remove the tags from input documents that are in HTML or XML format. This example shows how to remember the locations and content of all <title> and <a> tags in the original input document. It also automatically creates a view that is called DetaggedDoc with a single column called text , and additional views that are called Title and Anchor , each with a single column called match . detag Document.text as DetaggedDoc detect content_type always annotate element 'title' as Title, element 'a' as Anchor with attribute 'href' as href; This example shows how to find specific words that are interesting from a website: create dictionary InterestingSitesDict as ('ibm.com', 'slashdot.org' ); Create a view that contains all of the anchor tags whose targets contain a match of the \"interesting sites\" dictionary. create view InterestingLinks as select A.match as anchortext, A.href as href from Anchor A where ContainsDict('InterestingSitesDict', A.href); Find all dictionary matches in the title text of links to \"interesting\" websites. create view InterestingSites as extract dictionary 'InterestingSitesDict' on T.match as word from Title T; Finally, map spans in the InterestingWords document by using the Remap function. create view InterestingWords as select I.href as href, IS.word as word from InterestingLinks I, InterestingSites IS; create view InterestingWordsHTML as select I.href as href, Remap(I.word) as word from InterestingWords I; When you need to manipulate a span of text or otherwise extend the language to obtain a scalar value User-defined functions (UDFs) extend the capabilities of AQL. For example, you can develop a scalar function to check for normalization or to check whether a number is a valid credit card number. The input and output parameters must be one of the scalar types (Integer, String, Text, Span, or ScalarList). To make a scalar UDF available to your AQL code, you must first write the program that implements the function, and then declare it for usage so that the compiler recognizes the invocation from AQL and knows which program to call. For more information about how to implement and declare scalar UDFs, see User-defined functions in the AQL reference . You can implement the UDF in a Java\u2122 class as a public method. If the UDF involves a Span or Text type, the Java class must import com.ibm.avatar.datamodel.Span or com.ibm.avatar.datamodel.Text from the SystemT low-level Java API to compile. When the UDF is implemented, package the UDF as a *.JAR file and place in the data path of the AQL project. The content of UDF JAR file is serialized inside the compiled plan file. -- extract metrics like 46.1, $8.7 create view Number as extract regex /$?\\p{Nd}+(\\.\\p{Nd}+)?/ on D.text as match from Document D; create dictionary UnitDict as ('percent', 'billion', 'million', 'trillion'); -- extract amount cues like 'percent', 'billion' create view Unit as extract dictionary 'UnitDict' on D.text as match from Document D; -- Identify candidate indicators as a mention of metric followed within -- 0 to 10 tokens of a mention of amount -- consolidate overlapping mentions -- Example: Gross profit margin of 46.1 percent, cash flow of $8.7 billion create view IndicatorCandidate as select N.match as amount, U.match as metric, CombineSpans(N.match, U.match) as match from Number N, Unit U where FollowsTok(N.match, U.match, 0, 10); create view IndicatorConsolidated as select R.* from IndicatorCandidate R consolidate on R.match using 'NotContainedWithin'; Implement the UDF as a public method in a Java class com.ibm.test.udfs.MiscScalarFunc.java: -- Define the UDF create function udfToUpperCase(p1 Text) return String like p1 external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!toUpperCase' language java deterministic return null on null input; -- Use the UDF function to normalize the metric value of Indicator tuples create view IndicatorUDF as select R.*, udfToUpperCase(GetText(R.metric)) as metric_normalized from IndicatorConsolidated R; When you need to obtain a scalar value, follow the optimization guideline and Avoid UDFs as join predicates . When you want to extract occurrences of different parts of speech You can identify different parts of speech across the input text. The following is an example of code that captures all noun mentions from input text: create view Noun as extract part_of_speech 'NN, NNS, NNP, NNPS' with language 'en' on R.text as match from Document R; Enhancing content of AQL views To output something other than text matches from your existing results, you can use AQL statements to transform data, specify fixed output, and add additional metadata. You can use AQL to enhance the content of views, for instance specifying what can be returned as the value of a column in a view, be it a fixed string, the result of a function call, or the result of a table lookup. As you refine your outputs, consider the optimization guidelines. For more information about optimizing your extractor, see Follow the optimization guidelines for writing AQL . When you want to return a fixed string for documents that contain a match for a view This example illustrates the assignment of a fixed text value to a column to indicate positive polarity across an input document based on the presence of positive words inside the document. The positive words are defined by the dictionary PositiveWordDict . create dictionary PositiveWordsDict as ('happy', 'like', 'amazing', 'beautiful', 'love'); create view PositivePolarity as select D.text as text, 'positive' as polarity from Document D where ContainsDict ('PositiveWordsDict', D.text); When you want to transform the value of an output column by using a function This example shows how to use a built-in function to transform the contents of an existing view that is called Division to lowercase characters. create function toLowerCase(p1 Text) return String like p1 external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!toLowerCase' language java deterministic return null on null input; create view DocumentLowerCase as select toLowerCase(D.text) as lowerCase from Document D; When you want to return a fixed string for documents where a column of a view is null The following example illustrates case-based value assignment in AQL. If an input document does not contain a person name as defined by the dictionary PersonNamesDict , then a fixed String value of NO-MATCH is assigned for the column personName . create dictionary PersonNamesDict as ('John', 'George', 'Kevin', 'William', 'Stephen', 'Peter', 'Paul'); create view DocumentMatchingStatus as select case when ContainsDict('PersonNamesDict', D.text) then true else false as containsPersonName from Document D; When you want to create extra metadata on spans extracted from the document Use the create table statement to perform associative mapping on values of columns or on input text. Such a mapping can be used to enhance the content of an existing view towards adding useful metadata. -- table mapping company names to their headquarters create table NameToLocation (name Text, location Text) as values ('IBM', 'Endicott'), ('Microsoft','Seattle'), ('Initech', 'Dallas'); First, create a dictionary of company names from the table. -- dictionary of company names from the table create dictionary CompanyNamesDict from table NameToLocation with entries from name; Then, create a view to find all matches of the company names dictionary. -- find matches of [company-names] above dictionary terms in input text create view Company as extract dictionary 'CompanyNamesDict' on D.text as company from Document D; To finish, create a view that uses the table to augment the Company view with location information. -- enhance company names information with their headquarters information [as metadata] create view CompanyLoc as select N2C.location as loc, C.company as company from Company C, NameToLocation N2C where Equals(GetText(C.company), GetText(N2C.name)); AQL naming conventions Naming conventions can improve the readability of AQL source code. AQL uses identifiers to define the names of AQL objects, including names of modules, views, tables, dictionaries, functions, attributes, and function parameters. In general, these identifier naming conventions apply: For a module name, you must use a simple identifier. All other AQL objects can be a simple or double-quoted identifier. Identifier names that are written in English can enhance the readability of the AQL source code. If the identifier consists of multiple words, use the underscore (_) character or medial capitalization (camel case) to delimit different words. For example, three_word_identifier or threeWordIdentifier, or ThreeWordIdentifier. The following are the naming conventions for the objects that are used in AQL syntax: Module name Begin module names with a lowercase letter. For example, myModule or \"my_module\". In this code sample, the module name is sample . ```bash module sample; create dictionary GetTheDict as ('The', 'the'); create view TheMentions as extract dictionary 'GetTheDict' on D.text as theMention from Document D; ``` View name Begin view names with an uppercase letter. For example, MyView or \"My_view\". An example from the previous code sample, is TheMentions . Table name Begin inline or external table names with an uppercase letter. For example, TableA or \"Table_1\". In this example, the table name is NameToLocation . bash create table NameToLocation (name Text, location Text) as values ('IBM', 'USA'), ('Enron', 'UK'), ('Initech', 'Dallas'), ('Acme Fake Company Names', 'Somewhere'); Dictionary name Begin inline or external dictionary names with an uppercase letter. For example, MyDictionary or \"My_dictionary\". In this example, the dictionary name is GreetingDict . bash create dictionary GreetingDict as ( 'regards', 'regds', 'hello', 'hi', 'thanks', 'best', 'subj', 'to', 'from' ); User-defined function name Begin user-defined function names with a lowercase letter. This convention distinguishes UDFs from built-in functions that always start with an uppercase letter. For example, myFunction or \"my_function\". In this example, the function name is udfCompareNames . bash create function udfCompareNames(nameList ScalarList, myName String) return ScalarList like nameList external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!compareNames' language java deterministic; Attribute name Begin attribute names with a lowercase letter. An attribute can be a column in a view, a column in a table, or a function parameter. For example, my_attribute or \"myAttribute\". Examples from the previous code samples are nameList and myName (function parameters), name , and location (table columns), and theMention (view column).","title":"Guidelines for writing AQL"},{"location":"aql-guidelines/#guidelines-for-writing-aql","text":"Table of Contents {:toc}","title":"Guidelines for writing AQL"},{"location":"aql-guidelines/#best-practices-in-organizing-aql-code","text":"You can use these guidelines to develop efficient extractors by using good practices of AQL development. An extractor is a set of compiled AQL modules that work together to accomplish an extraction task. Prior to extractor development, it is important to conduct an analysis of your extraction task to establish your extraction requirements. For example, if you want to analyze the IBM\u00ae quarterly earnings reports, you might develop extractors to help answer the question, \u201cHow do quarterly revenues for different IBM divisions change over the years?\u201d To answer the question, you first need to find the mentions of quarterly revenue for each IBM division in each quarterly earnings report. The primary extraction task in this case is to extract quarterly revenue for each IBM division. Extractor development is generally an iterative process and you can follow some basic steps: Identify the basic building blocks of your extractor. Start with the most basic clues that appear in the text that you want to analyze with the extractor. In this step, you create the AQL statements for a set of basic entity extraction views. Combine the basic entities by using simple patterns to form more concise views. As a result, you have a set of statements that create more complex entities that are candidates for finalization. Review all of the candidates, and create or edit your AQL statements to filter the valid from the invalid results. You can apply AQL filtering and consolidation techniques to achieve this step. In some cases, you might need to use advanced AQL syntax to further refine your results. The ability to modularize source AQL code is a great benefit. You can ensure that multiple concepts are well-separated in terms of rule sets and patterns that are used. There are four basic questions to begin to guide you in the design process.","title":"Best practices in organizing AQL code"},{"location":"aql-guidelines/#question-1-how-complex-is-your-extractor","text":"Simple extractors are a single AQL module. The more complex the extractor, the greater the need to modularize it.","title":"Question 1: How complex is your extractor?"},{"location":"aql-guidelines/#question-2-does-your-extractor-have-objects-that-are-used-in-other-extractors","text":"An object refers to individual views, tables, dictionaries, functions, or any combinations of them. Identify AQL views, tables, dictionaries, and functions that are common across multiple versions of your extractor to create modules that can be reused by other specialized modules. Use the output view \u2026 as \u2026 statement to ensure that different specialized modules output a consistent set of names. Examples of such objects are apparent in the following two scenarios: You have one UDF JAR file that you use in each of your AQL projects. You have an extractor that you use in other extractors. For example, you use an organization extractor in another extractor that identifies financial events, or in an extractor that identifies relationships between persons and organizations.","title":"Question 2: Does your extractor have objects that are used in other extractors?"},{"location":"aql-guidelines/#question-3-would-you-like-to-expose-objects-inside-your-extractor-towards-extractor-customization","text":"One useful way to customize the behavior of an extractor is to abstract information that is used by the extractor to arrive at a decision (such as dictionaries and tables). Then, the consumer of the compiled extractor can complete these values when the extractor is applied to a specific domain. In this way, the same extractor can be applied to different domains, each time with a different set of entries for the customizable dictionaries and tables. For example, assume that you have a person extractor that is used to highlight mentions in an email inbox of a customer. You would like to share this extractor with multiple customers, and allow each one to customize the extractor with names from the employee directory of their organization, without providing the source AQL code and recompiling the extractor. You can expose these customization points by using the AQL statements that create an external dictionary and create an external table.","title":"Question 3: Would you like to expose objects inside your extractor towards extractor customization?"},{"location":"aql-guidelines/#question-4-are-there-domain-specific-components-to-your-extractor-are-there-any-common-components-used-by-these-domain-specific-components","text":"Create specialized modules that provide different implementations for the same semantic concept, where each implementation is customized for a particular application, domain, or language. It can be difficult to create generic AQL extractors that perform well on every possible domain or language. In general, a generic extractor requires customization to improve the accuracy and coverage on a specific domain. Such customization generally falls into three categories:","title":"Question 4: Are there domain-specific components to your extractor? Are there any common components used by these domain-specific components?"},{"location":"aql-guidelines/#application-customization","text":"You have a generic extractor that is used in multiple applications. Each application has specific requirements on the output schema, or the types of mentions to be extracted. For example, you use a person extractor in two different applications. In one application, the extractor does not output mentions that consist of a single token (for example, mentions of the given name of a person, without a mention of their surname). In the second application, such single-token mentions are acceptable. In addition, the output schema of the extractor differs across the two applications. In one application, a single attribute for the full name is sufficient. The second application requires extra attributes in addition to the full name: the given name, the surname, and the job position if present in the text.","title":"Application customization"},{"location":"aql-guidelines/#data-source-customization","text":"You have an extractor that is applied to several different types of data sources. Each data source has specifics that require specialized rules. For example, you have an extractor for location mentions that is applied on two types of text: formal news reports and emails. In formal news report, you know that there is a mention of the location at the beginning of the first line. You want to write a special rule to capture that location, but you do not want to apply that rule when the extractor is used on emails.","title":"Data source customization"},{"location":"aql-guidelines/#language-customization","text":"Formal language and grammar structure varies across languages. If your extractor applies to multiple languages, you might want to implement rules that are applicable to a single language. For example, you have a sentiment extractor that is applied to documents in English and French. You might have a few specialized rules that apply only to English text. Similarly, you might have a few specialized rules that apply only to French text. In cases of language customization, use the set default dictionary language statement to set the default dictionary languages for each module according to the language it is intended to work with.","title":"Language customization"},{"location":"aql-guidelines/#examples-of-frequently-used-aql-statements","text":"Included in the examples of how to use the AQL statements to accomplish various extraction tasks are optimization guidelines. These guidelines describe good choices to make when you write your AQL code so that the Optimizer has the most latitude in choosing efficient execution plans. Using basic feature AQL statements You can use the basic feature AQL statements to develop the core building blocks of your extractor. Then, combine and filter your AQL statements to refine your results. Using candidate generation AQL statements You can refine the basic extractor that you started with by combining entity extraction views. The candidate generation AQL statements combine basic features by using simple patterns to form larger groups of features, which further refine the concepts. Using filter and consolidate AQL statements You can use filter and consolidate statements to refine results, to remove invalid annotations, and to resolve overlap between annotations. Creating complex AQL statements In some cases, you might need to use advanced AQL syntax to further refine your results. To develop more complex extractors, you can build on the basic design patterns in AQL with these advanced patterns. Enhancing content of AQL views To output something other than text matches from your existing results, you can use AQL statements to transform data, specify fixed output, and add additional metadata.","title":"Examples of frequently used AQL statements"},{"location":"aql-guidelines/#using-basic-feature-aql-statements","text":"You can use the basic feature AQL statements to develop the core building blocks of your extractor. Then, combine and filter your AQL statements to refine your results. An AQL module generally consists of multiple create view statements and one or more output view statements. Each view can also include the use of dictionaries, tables, and functions, as well as references to other views. The following examples explain how to extract basic features, such as financial indicators, from unstructured text. For example, you might want to begin by extracting numbers, units, and metrics.","title":"Using basic feature AQL statements"},{"location":"aql-guidelines/#when-you-want-to-match-text-that-is-based-on-a-character-based-pattern","text":"Generally, a character-based pattern is recognizable in the text. You might see that there is always a sequence of letters followed by numbers, or numbers with known separators in between. For example, if you were to see \u201c(123) 456-7890\u201d or \u201c123-45-6789\u201d in text, you can easily specify a pattern that matches either sequence. When you analyze text, you can use a regular expression when you want to match text that is based on a character pattern. The following is an example of a create view statement that uses a regular expression to extract decimal numbers: -- Identify mentions of numbers with optional decimals -- Example: 7, 49, 11.2 create view Number as extract regex /\\d+(\\.\\d+)?/ on R.text as number from Document R; As you analyze text with regular expressions, follow the optimization guidelines for regular expressions whenever possible.","title":"When you want to match text that is based on a character-based pattern"},{"location":"aql-guidelines/#when-you-want-to-find-matches-for-a-fixed-set-of-words-or-phrases","text":"Use a dictionary when you want to find matches for a fixed set of words or phrases. You can list the terms to be included in the dictionary either inline (in an AQL file) or externally (in a .dict file). The following is an example of how to create an inline dictionary: create dictionary MyDict as ('Finance'); This statement creates a dictionary with just one entry, \u2018Finance\u2019 . Use an external file when you have many dictionary entries. The external file makes it easier to add and change entries without having to edit the AQL program. By default, dictionaries are tokenized and internalized at compile time, but you can use the create external dictionary statement to provide dictionaries at run time. The following sample creates a dictionary from a file on your local system: -- use a dictionary to contain a list of terms for matching purposes create dictionary MyDict as ('Finance'); -- Define a dictionary of financial amount units -- Example: million, billion create dictionary UnitDict from file 'dictionaries/unit.dict' with language as 'en'; The file dictionaries/unit.dict contains: million billion For more information about optimization by using dictionaries, follow the guidelines for regular expressions.","title":"When you want to find matches for a fixed set of words or phrases"},{"location":"aql-guidelines/#using-candidate-generation-aql-statements","text":"You can refine the basic extractor that you started with by combining entity extraction views. The candidate generation AQL statements combine basic features by using simple patterns to form larger groups of features, which further refine the concepts. The following examples explain how to extract candidate features from unstructured text. For example, you might want to combine the number and the unit by using the previously built basic feature AQL statements together to produce an amount. Or you might combine a metric with an amount to produce a revenue indicator. The following common tasks use these types of AQL statements:","title":"Using candidate generation AQL statements"},{"location":"aql-guidelines/#when-you-want-to-combine-information-from-multiple-views","text":"Use the select clause inside of a create view statement to combine, or join, information from multiple views. In this example, the goal is to find a number, followed within 0 to 10 tokens, by a unit. Notice how the built-in scalar function FollowsTok is used as join predicate between the views Number and Unit . Alternatively, you can use the extract pattern statement. -- extract metrics like 46.1, $8.7 create view Number as extract regex /$?\\p{Nd}+(\\.\\p{Nd}+)?/ on D.text as match from Document D; create dictionary UnitDict as ('percent', 'billion', 'million', 'trillion'); -- extract amount cues like 'percent', 'billion' create view Unit as extract dictionary 'UnitDict' on D.text as match from Document D; -- Identify candidate indicators as a mention of metric followed within -- 0 to 10 tokens of a mention of amount -- Example: Gross profit margin of 46.1 percent, cash flow of $8.7 billion create view IndicatorCandidate as select N.match as number, U.match as unit, CombineSpans(N.match, U.match) as match from Number N, Unit U where FollowsTok(N.match, U.match, 0, 10); -- Alternatively, you can achieve the same with a much simpler statement create view IndicatorCandidate as extract N.match as number, U.match as unit, pattern <N.match> <Token>{0,10} <U.match> as match from Number N, Unit U; As you combine information from multiple views, follow the optimization guidelines and Avoid Cartesian products , Use the and keyword instead of the And() built-in predicate , and Be careful when using Or() as a join predicate .","title":"When you want to combine information from multiple views"},{"location":"aql-guidelines/#when-you-want-to-combine-results-from-different-views-into-a-single-view","text":"In the following example, a union allclause is used inside of a create view statement to capture a union of entities that were specified with different AQL statements: create dictionary QuantityAbsoluteDict as ('billion', 'trillion', 'million'); create dictionary QuantityPercentDict as ('percent', 'percentile', 'percentage'); create view QuantityAbsolute as select I.* from IndicatorCandidate I where MatchesDict('QuantityAbsoluteDict', I.amount); create view QuantityPercent as select I.* from IndicatorCandidate I where MatchesDict('QuantityPercentDict', I.amount); -- Union all absolute and percentage amount candidates -- Example: $7 billion, $11.52, 49 percent, 46.1 percent create view AmountCandidate as (select R.* from QuantityAbsolute R) union all (select R.* from QuantityPercent R);","title":"When you want to combine results from different views into a single view"},{"location":"aql-guidelines/#when-you-want-to-extract-chunks-of-data","text":"Use the blocks extraction specification to identify blocks of contiguous spans across input text. This example shows how to identify blocks of exactly two capitalized words within zero to five tokens of each other: -- A single capitalized word create view CapitalizedWords as extract regex /[A-Z][a-z]+/ on 1 token in D.text as word from Document D; -- extract blocks of exactly two capitalized words within 0-5 tokens of each other create view TwoCapitalizedWords as extract blocks with count 2 and separation between 0 and 5 tokens on CW.word as capswords from CapitalizedWords CW;","title":"When you want to extract chunks of data"},{"location":"aql-guidelines/#when-you-want-to-identify-patterns-of-simple-text-or-fields-from-other-views","text":"In the following example, an amount entity is extracted with the help of a simple pattern involving other entities: -- extract metrics like 46.1, 8.7 create view Number as extract regex /\\p{Nd}+(\\.\\p{Nd}+)?/ on D.text as match from Document D; create dictionary UnitDict as ('percent', 'billion', 'million', 'trillion'); -- extract amount cues like 'percent', 'billion' create view Unit as extract dictionary 'UnitDict' on D.text as match from Document D; -- Identify mentions of absolute amounts as a sequence of '$' character, -- followed by a Number mention, optionally followed by a Unit mention -- Example: $7 billion, $11.52 create view AmountAbsolute as extract pattern /\\$/ <N.match> <U.match>? return group 0 as match from Number N, Unit U consolidate on match; As you identify patterns of text or fields from other views, follow the optimization guideline and Define common subpatterns explicitly .","title":"When you want to identify patterns of simple text or fields from other views"},{"location":"aql-guidelines/#using-filter-and-consolidate-aql-statements","text":"You can use filter and consolidate statements to refine results, to remove invalid annotations, and to resolve overlap between annotations.","title":"Using filter and consolidate AQL statements"},{"location":"aql-guidelines/#when-you-want-to-remove-data-that-overlaps","text":"In the following example, the consolidate on clause is used to remove overlapping spans held by the column on which the consolidate clause is applied. -- extract metrics like 46.1, $8.7 create view Number as extract regex /$?\\p{Nd}+(\\.\\p{Nd}+)?/ on D.text as match from Document D; create dictionary UnitDict as ('percent', 'billion', 'million', 'trillion'); -- extract amount cues like 'percent', 'billion' create view Unit as extract dictionary 'UnitDict' on D.text as match from Document D; -- Identify candidate indicators as a mention of metric followed within -- 0 to 10 tokens of a mention of amount -- consolidate overlapping mentions -- Example: Gross profit margin of 46.1 percent, cash flow of $8.7 billion create view IndicatorCandidate as select N.match as amount, U.match as metric, CombineSpans(N.match, U.match) as match from Number N, Unit U where FollowsTok(N.match, U.match, 0, 10); create view IndicatorConsolidated as select R.* from IndicatorCandidate R consolidate on R.match using 'NotContainedWithin'; When you want to remove data that overlaps, follow the optimization guideline and Use the consolidate clause wisely .","title":"When you want to remove data that overlaps"},{"location":"aql-guidelines/#when-you-want-to-filter-information-based-on-a-dictionary-term-or-similar-terms","text":"You can use a predicate-based filter to remove similar data. The following example shows a simple filter condition that is based on information from the dictionary that you created: -- Negative clues that signal a relative amount -- Example: increased, decreased, down, up create dictionary AmountNegativeClueDict with language as 'en' as ('points', 'debit', 'credit'); -- Narrow down the results to the candidate indicators which are absolute: -- the span between the metric and amount does not contain -- a \"relative amount\" term create view IndicatorAbsoluteCandidate as select I.metric as metric, I.amount as amount, I.match as match from IndicatorConsolidated I where Not(MatchesDict('AmountNegativeClueDict', SpanBetween(I.metric, I.amount)));","title":"When you want to filter information based on a dictionary term or similar terms"},{"location":"aql-guidelines/#when-you-want-to-use-a-view-to-filter-data-from-a-larger-subset-of-data","text":"You can use a minus clause to express complex filter conditions for which predicate-based filters are not sufficient. create dictionary MetricDict as ('revenue', 'Revenue'); create view Metric as extract dictionary 'MetricDict' on D.text as match from Document D; -- Identify one type of invalid Indicator candidates: mentions that contain -- another metric in between the Metric and Amount mentions -- Example: -- [EPS growth]; Revenue of [$99.9 billion] -- [revenue] up 19 percent; Free cash flow of [$8.7 billion] create view IndicatorInvalid as select R.* from IndicatorConsolidated R, Metric M where Contains(SpanBetween(R.metric, R.amount), M.match); -- Filter out invalid Indicator mentions from the set of all -- Indicator candidates create view IndicatorAll as (select R.* from IndicatorConsolidated R) minus (select R.* from IndicatorInvalid R);","title":"When you want to use a view to filter data from a larger subset of data"},{"location":"aql-guidelines/#creating-complex-aql-statements","text":"In some cases, you might need to use advanced AQL syntax to further refine your results. To develop more complex extractors, you can build on the basic design patterns in AQL with these advanced patterns. The following are examples of advanced AQL design patterns:","title":"Creating complex AQL statements"},{"location":"aql-guidelines/#when-you-want-to-remove-html-or-xml-tags-and-extract-information-based-on-the-tags","text":"The detag statement is used to remove the tags from input documents that are in HTML or XML format. This example shows how to remember the locations and content of all <title> and <a> tags in the original input document. It also automatically creates a view that is called DetaggedDoc with a single column called text , and additional views that are called Title and Anchor , each with a single column called match . detag Document.text as DetaggedDoc detect content_type always annotate element 'title' as Title, element 'a' as Anchor with attribute 'href' as href; This example shows how to find specific words that are interesting from a website: create dictionary InterestingSitesDict as ('ibm.com', 'slashdot.org' ); Create a view that contains all of the anchor tags whose targets contain a match of the \"interesting sites\" dictionary. create view InterestingLinks as select A.match as anchortext, A.href as href from Anchor A where ContainsDict('InterestingSitesDict', A.href); Find all dictionary matches in the title text of links to \"interesting\" websites. create view InterestingSites as extract dictionary 'InterestingSitesDict' on T.match as word from Title T; Finally, map spans in the InterestingWords document by using the Remap function. create view InterestingWords as select I.href as href, IS.word as word from InterestingLinks I, InterestingSites IS; create view InterestingWordsHTML as select I.href as href, Remap(I.word) as word from InterestingWords I;","title":"When you want to remove HTML or XML tags and extract information based on the tags"},{"location":"aql-guidelines/#when-you-need-to-manipulate-a-span-of-text-or-otherwise-extend-the-language-to-obtain-a-scalar-value","text":"User-defined functions (UDFs) extend the capabilities of AQL. For example, you can develop a scalar function to check for normalization or to check whether a number is a valid credit card number. The input and output parameters must be one of the scalar types (Integer, String, Text, Span, or ScalarList). To make a scalar UDF available to your AQL code, you must first write the program that implements the function, and then declare it for usage so that the compiler recognizes the invocation from AQL and knows which program to call. For more information about how to implement and declare scalar UDFs, see User-defined functions in the AQL reference . You can implement the UDF in a Java\u2122 class as a public method. If the UDF involves a Span or Text type, the Java class must import com.ibm.avatar.datamodel.Span or com.ibm.avatar.datamodel.Text from the SystemT low-level Java API to compile. When the UDF is implemented, package the UDF as a *.JAR file and place in the data path of the AQL project. The content of UDF JAR file is serialized inside the compiled plan file. -- extract metrics like 46.1, $8.7 create view Number as extract regex /$?\\p{Nd}+(\\.\\p{Nd}+)?/ on D.text as match from Document D; create dictionary UnitDict as ('percent', 'billion', 'million', 'trillion'); -- extract amount cues like 'percent', 'billion' create view Unit as extract dictionary 'UnitDict' on D.text as match from Document D; -- Identify candidate indicators as a mention of metric followed within -- 0 to 10 tokens of a mention of amount -- consolidate overlapping mentions -- Example: Gross profit margin of 46.1 percent, cash flow of $8.7 billion create view IndicatorCandidate as select N.match as amount, U.match as metric, CombineSpans(N.match, U.match) as match from Number N, Unit U where FollowsTok(N.match, U.match, 0, 10); create view IndicatorConsolidated as select R.* from IndicatorCandidate R consolidate on R.match using 'NotContainedWithin'; Implement the UDF as a public method in a Java class com.ibm.test.udfs.MiscScalarFunc.java: -- Define the UDF create function udfToUpperCase(p1 Text) return String like p1 external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!toUpperCase' language java deterministic return null on null input; -- Use the UDF function to normalize the metric value of Indicator tuples create view IndicatorUDF as select R.*, udfToUpperCase(GetText(R.metric)) as metric_normalized from IndicatorConsolidated R; When you need to obtain a scalar value, follow the optimization guideline and Avoid UDFs as join predicates .","title":"When you need to manipulate a span of text or otherwise extend the language to obtain a scalar value"},{"location":"aql-guidelines/#when-you-want-to-extract-occurrences-of-different-parts-of-speech","text":"You can identify different parts of speech across the input text. The following is an example of code that captures all noun mentions from input text: create view Noun as extract part_of_speech 'NN, NNS, NNP, NNPS' with language 'en' on R.text as match from Document R;","title":"When you want to extract occurrences of different parts of speech"},{"location":"aql-guidelines/#enhancing-content-of-aql-views","text":"To output something other than text matches from your existing results, you can use AQL statements to transform data, specify fixed output, and add additional metadata. You can use AQL to enhance the content of views, for instance specifying what can be returned as the value of a column in a view, be it a fixed string, the result of a function call, or the result of a table lookup. As you refine your outputs, consider the optimization guidelines. For more information about optimizing your extractor, see Follow the optimization guidelines for writing AQL .","title":"Enhancing content of AQL views"},{"location":"aql-guidelines/#when-you-want-to-return-a-fixed-string-for-documents-that-contain-a-match-for-a-view","text":"This example illustrates the assignment of a fixed text value to a column to indicate positive polarity across an input document based on the presence of positive words inside the document. The positive words are defined by the dictionary PositiveWordDict . create dictionary PositiveWordsDict as ('happy', 'like', 'amazing', 'beautiful', 'love'); create view PositivePolarity as select D.text as text, 'positive' as polarity from Document D where ContainsDict ('PositiveWordsDict', D.text);","title":"When you want to return a fixed string for documents that contain a match for a view"},{"location":"aql-guidelines/#when-you-want-to-transform-the-value-of-an-output-column-by-using-a-function","text":"This example shows how to use a built-in function to transform the contents of an existing view that is called Division to lowercase characters. create function toLowerCase(p1 Text) return String like p1 external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!toLowerCase' language java deterministic return null on null input; create view DocumentLowerCase as select toLowerCase(D.text) as lowerCase from Document D;","title":"When you want to transform the value of an output column by using a function"},{"location":"aql-guidelines/#when-you-want-to-return-a-fixed-string-for-documents-where-a-column-of-a-view-is-null","text":"The following example illustrates case-based value assignment in AQL. If an input document does not contain a person name as defined by the dictionary PersonNamesDict , then a fixed String value of NO-MATCH is assigned for the column personName . create dictionary PersonNamesDict as ('John', 'George', 'Kevin', 'William', 'Stephen', 'Peter', 'Paul'); create view DocumentMatchingStatus as select case when ContainsDict('PersonNamesDict', D.text) then true else false as containsPersonName from Document D;","title":"When you want to return a fixed string for documents where a column of a view is null"},{"location":"aql-guidelines/#when-you-want-to-create-extra-metadata-on-spans-extracted-from-the-document","text":"Use the create table statement to perform associative mapping on values of columns or on input text. Such a mapping can be used to enhance the content of an existing view towards adding useful metadata. -- table mapping company names to their headquarters create table NameToLocation (name Text, location Text) as values ('IBM', 'Endicott'), ('Microsoft','Seattle'), ('Initech', 'Dallas'); First, create a dictionary of company names from the table. -- dictionary of company names from the table create dictionary CompanyNamesDict from table NameToLocation with entries from name; Then, create a view to find all matches of the company names dictionary. -- find matches of [company-names] above dictionary terms in input text create view Company as extract dictionary 'CompanyNamesDict' on D.text as company from Document D; To finish, create a view that uses the table to augment the Company view with location information. -- enhance company names information with their headquarters information [as metadata] create view CompanyLoc as select N2C.location as loc, C.company as company from Company C, NameToLocation N2C where Equals(GetText(C.company), GetText(N2C.name));","title":"When you want to create extra metadata on spans extracted from the document"},{"location":"aql-guidelines/#aql-naming-conventions","text":"Naming conventions can improve the readability of AQL source code. AQL uses identifiers to define the names of AQL objects, including names of modules, views, tables, dictionaries, functions, attributes, and function parameters. In general, these identifier naming conventions apply: For a module name, you must use a simple identifier. All other AQL objects can be a simple or double-quoted identifier. Identifier names that are written in English can enhance the readability of the AQL source code. If the identifier consists of multiple words, use the underscore (_) character or medial capitalization (camel case) to delimit different words. For example, three_word_identifier or threeWordIdentifier, or ThreeWordIdentifier. The following are the naming conventions for the objects that are used in AQL syntax: Module name Begin module names with a lowercase letter. For example, myModule or \"my_module\". In this code sample, the module name is sample . ```bash module sample; create dictionary GetTheDict as ('The', 'the'); create view TheMentions as extract dictionary 'GetTheDict' on D.text as theMention from Document D; ``` View name Begin view names with an uppercase letter. For example, MyView or \"My_view\". An example from the previous code sample, is TheMentions . Table name Begin inline or external table names with an uppercase letter. For example, TableA or \"Table_1\". In this example, the table name is NameToLocation . bash create table NameToLocation (name Text, location Text) as values ('IBM', 'USA'), ('Enron', 'UK'), ('Initech', 'Dallas'), ('Acme Fake Company Names', 'Somewhere'); Dictionary name Begin inline or external dictionary names with an uppercase letter. For example, MyDictionary or \"My_dictionary\". In this example, the dictionary name is GreetingDict . bash create dictionary GreetingDict as ( 'regards', 'regds', 'hello', 'hi', 'thanks', 'best', 'subj', 'to', 'from' ); User-defined function name Begin user-defined function names with a lowercase letter. This convention distinguishes UDFs from built-in functions that always start with an uppercase letter. For example, myFunction or \"my_function\". In this example, the function name is udfCompareNames . bash create function udfCompareNames(nameList ScalarList, myName String) return ScalarList like nameList external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!compareNames' language java deterministic; Attribute name Begin attribute names with a lowercase letter. An attribute can be a column in a view, a column in a table, or a function parameter. For example, my_attribute or \"myAttribute\". Examples from the previous code samples are nameList and myName (function parameters), name , and location (table columns), and theMention (view column).","title":"AQL naming conventions"},{"location":"aql-ref-guide/","text":"Annotation Query Language reference Annotation Query Language (AQL) is the primary language that is used to create {{site.data.keyword.knowledgestudiofull}} advanced rules extractors. Data model : The data model for AQL is similar to the standard relational model that is used by an SQL database such as DB2\u00ae. All data in AQL is stored in tuples, data records of one or more columns, or fields. A collection of tuples forms a view. All tuples in a view must have the same schema that is the names and types of the fields across all tuples. Execution model : The runtime component has a document-at-a-time execution model. The runtime component receives a collection of documents and runs the extractor on each document to extract information from that document. AQL statements : By using AQL statements, you can create and then use modules, views, tables, dictionaries, and functions. Built-in functions : AQL has a collection of built-in functions for use in extraction rules. The create function statement : To perform operations on extracted values that are not supported by AQL, you can define custom functions to use in extraction rules called user-defined functions (UDFs). Table of Contents {:toc} Annotation Query Language (AQL) syntax {: #aql-syntax} Like many programming languages, AQL is based on common syntax and grammar. The lexical structure of a programming language is the set of elementary rules that define the tokens or basic components of that language such as its reserved words, identifiers, constants, and more. The syntax of AQL is similar to that of SQL, but several important differences exist: AQL is case-sensitive. AQL currently does not support advanced SQL features such as correlated subqueries and recursive queries. AQL has a new statement type, extract , which is not present in SQL. AQL does not allow keywords (reserved words) as view, column, or function names. AQL allows, but does not require, regular expressions to be expressed in Perl syntax. Regular expressions begin with a slash ( / ) and end with a slash ( / ), as in Perl syntax. AQL also allows regular expressions that start with a single quotation mark ( ' ) and end with a single quotation mark ( ' ). For example, you can use /regex/ instead of 'regex' as the regular expression in AQL. Identifiers : Identifiers are used to define the names of AQL objects, including names of modules, views, tables, dictionaries, functions, attributes, and function parameters. Reserved words : Reserved words are words that have a fixed meaning within the context of the AQL structure and cannot be redefined. Keywords are reserved words that have special meanings within the language syntax. Constants : Constants are fixed values that can be one of these data types: String, Integer, Float, or Boolean. Comments \" Use comments to augment AQL code with basic descriptions to help others understand the code and to generate self-describing compiled modules. Expressions : An AQL expression is a combination of one or more scalar values and functions that evaluates to a single scalar value. Identifiers {: #aql-identifiers} Identifiers are used to define the names of AQL objects, including names of modules, views, tables, dictionaries, functions, attributes, and function parameters. Two types of case-sensitive AQL identifiers exist: Simple identifier A simple identifier must start with a lowercase ( a-z ) or uppercase ( A-Z ) letter or the underscore character ( _ ). Subsequent characters can be lowercase or uppercase letters, the underscore character, or digits ( 0-9 ). A simple identifier must be different from any AQL key word. Double-quoted identifier A double-quoted identifier starts and ends with a double quotation mark character ( \" ). You can use any character between the beginning and ending double quotation mark characters. Double-quoted identifiers cannot contain a period ( . ) character. If a double quotation mark character occurs within name, it must be escaped by prefixing it with the backslash character (\\), for example \\\u201d . Reserved words {: #aql-reserved} Reserved words are words that have a fixed meaning within the context of the AQL structure, and cannot be redefined. Keywords are reserved words that have special meanings within the language syntax. The following AQL-reserved keywords cannot be used as identifiers because each has a well-defined purpose within the language: all allow allow_empty always and annotate as ascending ascii attribute between blocks both by called case cast ccsid character characters columns consolidate content_type count create default descending detag detect deterministic dictionary dictionaries document element else empty_fileset entries exact export external external_name extract fetch file first flags folding from function group having import in include infinity inline_match input into insensitive java language left lemma_match like limit mapping matching_regex minus module name never not null on only order output part_of_speech parts_of_speech parameter pattern point points priority regex regexes retain required return right rows select separation set specific split table tagger then token Token tokens unicode union up using values view views when where with The following reserved words are the names of built-in scalar types: Text Span Integer Float String Boolean ScalarList The following other reserved names cannot be used as identifiers: Dictionary Regex Consolidate Block BlockTok Sentence Tokenize RegexTok PosTag Constants {: #aql-constants} Constants are fixed values that can be one of these data types: String , Integer , Float , or Boolean . Constants are used in the select list of a select or extract clause, or as arguments in built-in or UDF functions and predicates. AQL supports the following types of constants: String constant A string that is enclosed in single quotation marks (\u2018), for example \u2018a string\u2019. Integer constant A 32-bit signed integer value that is not enclosed in quotation marks, for example 10 or -1. Float constant A single-precision 32-bit floating point value, not enclosed in quotation marks, for example 3.14 or -1.2. Boolean constant The value true or false not enclosed in quotation marks. Null constant The value null that is not enclosed in quotation marks. Comments {: #aql-comments} Use comments to augment AQL code with basic descriptions to help others understand the code and to generate self-describing compiled modules. Comments allow AQL developers to augment AQL code with basic descriptions, for ease of understanding of AQL source code, and they generate self-describing compiled AQL modules. Three kinds of comments are supported in AQL: Single-line comments Single-line comments begin with double hyphens ( -- ). Mulitple-line comments Multiple-line comments begin with a slash and an asterisk ( /* ) and end with an asterisk and a slash ( * ). Multiple-line comments cannot be nested. For example, the following nested mulitple-line comment is not allowed: bash /* A comment with a /*nested comment*/ */ AQL Doc comments AQL Doc comments provide a way to describe a module or an AQL object in plain language, and in an aspect-rich manner for contextual comprehension by other users. Unlike single-line comments and multiple line comments, which are ignored by the AQL Compiler, AQL Doc comments are serialized in the metadata of compiled modules (.tam files) and available for external consumption. All comments in AQL Doc for statements and modules have the following format: The comment is in plain text (no HTML tags are supported). The comment begins with a forward slash followed by two asterisks ( /** ) and ends with an asterisk forward slash ( */ ). Optionally, each line can start with an asterisk ( * ). Any number of white spaces can be used before the asterisk. Special tags that are prefixed by an at symbol ( @ ) can be used at the beginning of each line or after the optional asterisk. AQL Doc comments cannot be nested. Two levels of granularity are supported within the AQL Doc system. The format for documenting each artifact is explained in detail in the topic that describes its statement and syntax. Module-level comments Module level comments are contained inside a special file that is named module.info and located directly under the module folder. The comments are expected to describe the semantics of the module, and the schema of the view Document of the module. Statement-level comments Statement level comments are contained in the source AQL file, immediately preceding the statement that creates an AQL object. Single-line comments and multiple-line comments are allowed between the AQL Doc comment of a statement and the statement itself. The following top-level AQL statements can be documented by using AQL Doc comments: The create external view statement The create external table statement The create external dictionary statement The create function The detag statement The select... into statement AQL Doc comments are serialized inside the compiled representation of a module. Expressions {: #aql-expr} An AQL expression is a combination of one or more scalar values and functions that evaluates to a single scalar value. Expressions can be one of four types: a constant a column reference a scalar function call an aggregate function call Constant An expression can be a constant of type Integer , Float , or String , as in the following example: select 'positive' as polarity The expression is the string constant positive . Column reference An expression can be a column reference, as in the following example: create view Person as select F.name as firstname, L.name as lastname from FirstName F, LastName L where FollowsTok(F.name, L.name, 0, 0); This view identifies text that can be interpreted as a person\u2019s full name (for example, \u201cSamuel Davis\u201d, \u201cVicky Rosenberg\u201d). The expressions F.name and L.name are column-reference expressions that return the name column of the views F and L , respectively. The from statement defines views F and L as the local names for the views FirstName and LastName (which define valid given names and surnames, and are not shown in this example). Scalar function call An expression can be composed of one or more scalar function calls, each of which might contain arguments that are also expressions of type constant, column reference, or scalar function call. A scalar function call can be one of the built-in scalar functions, or a scalar user-defined function. Consider the following example: create view FamilyMembers as select ToLowerCase(GetText(P.lastname)) as lastname, List( (GetText(P.firstname))) as firstnames from Person P group by GetText(P.lastname); This view identifies potential family members by grouping people with the same surnames together, printing all of the names, with the surnames in lowercase characters (for example, lastname keaton, firstnames (Elyse, Alex, Diane)). The outputs of the GetText function calls are used as arguments to the ToLowerCase function call to display the names in lowercase. The scalar function call expressions in this example are: ToLowerCase , (GetText(P.lastname) , ToLowerCase(GetText(P.firstname)) , and GetText(P.lastname) . Aggregate function call An expression can be an aggregate function call. This expression type can have as arguments another expression of type column reference or type scalar function call. The following example shows an aggregate function call with expression type column reference: create view CountLastNames as select Count(Person.lastname) from Person; The expression is simply Count(Person.lastname) and counts the number of Person.lastname annotations in the document. An example of an aggregate function call with expression type scalar function call exists in the previous example as List(GetText(P.firstname)) with the List aggregate function taking a GetText scalar function as an argument to generate a list of given names. Aggregate function call expressions are only allowed as expressions in the select list of a select statement. Aggregate function call expressions are not allowed in the select list of an extract statement, or as arguments to a scalar or aggregate function call. Data model {: #aql-datamodel} The data model for AQL is similar to the standard relational model that is used by an SQL database such as DB2\u00ae. All data in AQL is stored in tuples, data records of one or more columns, or fields. A collection of tuples forms a view. All tuples in a view must have the same schema that is the names and types of the fields across all tuples. The content of the input document is represented as a special view called Document. The fields of a tuple must belong to one of the built-in data types of AQL: Boolean A data type that has a true or false value. Float A single-precision floating-point number. Integer A 32-bit signed integer. ScalarList A collection of values of the same scalar type (Integer, Float, Text, or Span). A value of data type ScalarList can be obtained as a result of the AQL built-in aggregate function List() , or as the result of a UDF. Span A Span is a contiguous region of a Text object, which is identified by its beginning and ending offsets in the Text object. Assume that your input text is: bash Amelia Earhart is a pilot. The text across Span [0-6] is Amelia . This Span can be visualized as: bash A m e l i a ^ ^ ^ ^ ^ ^ ^ 0 1 2 3 4 5 6 Likewise, the text across Span [20-25] is pilot . A Span of [x-x] represents the span between the end of a character and the beginning of the next character. In the previous example, [0-0] is an empty string before the character A . Likewise, a Span of [3-3] is an empty string between the characters e and l . {: note} A value of type Span can be obtained as a result of an extract statement or a built-in scalar function, or a UDF. Text The AQL data type to represent a character sequence. A text object contains a Unicode string, called its string value. When a string is formed as the concatenation of disjoint substrings of another Text object, it also contains reference to the original Text object and relevant offset mapping information. Two Text objects are considered equal to each other if all their components are correspondingly equal to each other. Comparing values of type Span and Text Prioritization factors affect comparisons between values of type Span and type Text. Comparing values of type Span and Text {: #aql-comparing} Prioritization factors affect comparisons between values of type Span and type Text. Values of type Span and type Text compare against each other in these ways: A null span value is always sorted lower than other values. A text value is always sorted higher than a span value. Values of type Text are sorted first by the lexical order of their string values, then by their original Text orders and offset mapping information if applicable. Values of type Span are sorted first by their underlying text objects, then by their begin offset, and then by their end offset. The span with a smaller begin offset is sorted lower. Between two spans that start on the same offset, the one with the smaller end offset sorts lower. Execution model {: #aql-execmodel} The runtime component has a document-at-a-time execution model. The runtime component receives a collection of documents and runs the extractor on each document to extract information from that document. An extractor consists of one or more AQL modules to create a collection of views that each defines a relation. Some of these views are designated as output views, while others are non-output views. Non-output views can include some views that are imported into or exported from a module. It is important to note that output views and exported views are orthogonal. For example, a view that is exported does not qualify as a view that is output. In addition to these views, the unique Document view represents the input document that is annotated by this extractor. Document view At the AQL module level, Document view is a special view that represents the current document that is being annotated by that module. When two or more modules are combined to form an extractor, the duplicate-free union of Document schemas of all modules is the required Document view. Use the require document with columns statement to specify the schema of the Document view. If this statement is absent from a module, the default schema that is assumed for the Document view is (text Text, label Text): text The textual content of the current document. label The label of the current document that is being annotated. The keyword Document is reserved as an identifier for the Document view, which is automatically populated during execution. Therefore, you cannot define another view or table with the same name. However, you can use the word Document as an identifier for attribute names and aliases. AQL statements {: #aql-statements} By using AQL statements, you can create and then use modules, views, tables, dictionaries, and functions. The following statements are supported in AQL: Statements for creating modules and declaring the interaction between them The module statement The export statement The import statement Statements for creating AQL objects: views, dictionaries, tables, or functions The create dictionary and create external dictionary statements The create table statement The create view statement The create external view statement The detag statement The extract statement The select statement The require document with columns statement The set default dictionary language statement The create function statement The syntax specifications in the AQL statements contain brackets ( [ ] ). These specifications indicate that the brackets and the constructs that they hold are optional when the corresponding syntax is used in writing a statement. In addition, they stand as place holders to define how to specify additional instances of a construct or argument. {: note} The module statement {: #aql-module} Use the module statement to create a module that has self-contained required resources. You can export and import these resources as AQL objects to and from other modules. Syntax {: #aql-module-syntax} module <module-name>; Description {: #aql-module-desc} <module-name> Declares that the current file is part of the module that is named <module-name> . The module name must be a simple identifier. Double-quoted identifiers are not allowed as module names. Each AQL file inside the module must have exactly one module declaration, and this declaration must be the first statement in each file. This declaration establishes a namespace identical to the module-name. All views (and other objects such as dictionaries, tables, and functions) declared in AQL files inside this module are located inside this single namespace. They are accessible to all AQL files in this namespace. All of the files that are declared as part of the module <module-name> must be inside a folder that is called <module-name> to enforce this namespace. No ordering of the AQL files within this module folder exists. The AQL compiler examines all AQL files of the module and determines the right order to compile all views, dictionaries, tables, and functions declared in the module. Usage notes {: #aql-module-usage} When the underlying AQL file is not located as part of a module (modular AQL) and is compiled in compatibility mode, this statement is not supported. Circular dependencies between views are not allowed. AQL modules do not support submodules. AQL files within subfolders of the top-level module folder are ignored. Examples {: #aql-reference-examples-1} Example 1: Sample module In the following example, the view TheMentions belongs to the module that is named sample. module sample; create dictionary GetTheDict as ('The', 'the'); create view TheMentions as extract dictionary 'GetTheDict' on D.text as theMention from Document D; The export statement {: #aql-export} The export statement in AQL is used to export an AQL object from the current module so that it can be imported and used in other modules. Syntax {: #aql-export-syntax} export view|dictionary|table|function <object-name>; Description {: #aql-export-desc} view|dictionary|table|function Defines the type of object to export. The object type can be a view, dictionary, table, or function. <object-name> Defines the name of the object to export. The <object-name> can be a simple identifier or a double-quoted identifier. Usage notes {: #aql-export-usage} You cannot export any of the imported AQL objects. You can create the effect of re-exporting the view or table in the current module by creating a new view: bash select * from <imported_view_name_or_table_name>; The export view and output view statements that are shown in the examples are orthogonal to each other. In other words, an output view is not automatically an exported view, but it must be explicitly exported by using an export statement. An exported view is not automatically an output view, but it must be explicitly output by using the output view statement. In the examples, the export statement attempts to export the view PersonName.FirstName, which is an imported view. This attempt causes an error, which means that the developer must copy the imported view into a new view and then export that instead. Examples {: #aql-reference-examples-2} Example 1: Creating views and dictionaries and then exporting them for use in other modules This example creates the views FirstName and NotFirstName. The view FirstName collects information about the given names that are represented in the FirstNamesDictionary dictionary. The other view collects the names that remain when you exclude the given names. Two dictionaries are necessary to make the text extraction easier. One dictionary contains all of the given names that you want to search for. The second dictionary, LastNamesDictionary, contains the surnames to search for. The FirstNamesDictionary dictionary is exported so that it can be used in other modules. The FirstName and the NotFirstName views are also exported so that they can be imported and used in other modules, such as module person in Example 2. module personName; create dictionary FirstNamesDictionary as ('Smith', 'John', 'Mary', 'Sam', 'George'); -- export dictionary statement export dictionary FirstNamesDictionary; create view FirstName as extract dictionary 'FirstNamesDictionary' on D.text as firstName from Document D; -- export view statement export view FirstName; create dictionary LastNamesDictionary as ('Stewart', 'Johnson', 'Smith', 'Hopkins', 'George'); create view NotFirstName as select F.firstName as notFirstName from FirstName F where ContainsDict('LastNamesDictionary', F.firstName); -- export view statement export view NotFirstName; Example 2: Importing views to use, but exporting inappropriately, causing an error This example shows the importing of two views. These views were exported from module personName in Example 1. Module person can now import and reference those views. However, this module attempts to export the same view that it imported, FirstName, which causes an error. module person; -- Form 1 import view FirstName from module personName as PersonFirstName; -- Form 2 import view NotFirstName from module personName; -- ERROR -- Reason: A view that is imported from one module -- cannot be exported in the current module. export view personName.FirstName; output view PersonFirstName; The reason for the error in this code is that a view that is imported from one module cannot be exported in the current module. In addition, exported views are not automatically output views unless you defined an output view with the output view statement. The import statement {: #aql-import} You can use the import statement to reference objects that are exported from other modules in the context of the current module. Syntax {: #aql-import-syntax} import view|dictionary|table|function <object-name> from module <module-name> [as <alias>]; Description {: #aql-import-desc} view\\|dictionary\\|table\\|function Identifies the type of AQL object to be imported. The type of the object is mandatory. <object-name> The <object-name> can be a simple identifier or a double-quoted identifier. <module-name> The <module-name> must be a simple identifier. <alias> This form of the import statement, also known as alias import , imports the specified AQL object under the <alias> name (not the original name) into the namespace of the current module. You can reference the imported element by using either an unqualified alias or an alias that is qualified with the current module name (the module where the AQL object was imported). You cannot use originalModule.elementName because the alias import statement imports the element under the alias name only and not under the qualified original name. Usage notes {: #aql-import-usage} An import statement without an alias specification imports the specified AQL object into the current module. It makes the AQL object accessible to AQL statements defined in the current module under its qualified name <original_module_name>.<object-name> . The import statement is only used to import AQL objects from modules other than the current module. An object that is declared in an AQL file of the module is visible to any other AQL file in that same module. An import statement puts objects in the context of the current module, and not in the context of the current file. Therefore, a view that is imported by 1.aql inside module A is made visible to 2.aql inside the same module without the need for any additional import statements. All import statements must follow immediately after the module declaration, and must precede all other types of statements. Only those AQL objects that are explicitly exported from any module can be imported in another module. If this requirement is not observed, a compilation error results. A compilation error is introduced when an import view statement introduces a naming conflict with any create view statement or other import statements within the same module (not just within the current file). This restriction applies to the import of other objects in addition to views. The AQL compiler adheres to the naming conventions that are used in previous versions of AQL: A module cannot contain a view and a table with the same name. A dictionary with the same name as a table or view is allowed. A function with the same name as a table, view, or dictionary is allowed. A compilation error is introduced when different import statements within one or multiple AQL files in the module give the same name to different AQL objects. A compilation error is introduced when an AQL file inside a module tries to reference another exported view of a module without using the import view statement. The same applies to dictionaries, tables, or functions. When two AQL files inside a module import the same view X from another module under two different aliases, for example, A and B, then the two aliases are treated synonymously. This rule applies also to tables, dictionaries, and functions. Examples {: #aql-reference-examples-3} Example 1: Create views that you export to be imported into other modules. This example creates two views, FirstName , and NotFirstName . The view FirstName collects information about the given names that are represented in the FirstNamesDictionary dictionary. The second view collects the names that are left when you exclude the given names. Two dictionaries are necessary to make the text extraction easier. One dictionary contains all of the given names that you want to search for. The second dictionary, LastNamesDictionary , contains the surnames to search for. The FirstName and the NotFirstName views are exported so that they can be imported and used in other modules, such as module person in Examples 2 and 3. module personName; create dictionary FirstNamesDictionary as ('Smith', 'John', 'Mary', 'Sam', 'George'); create view FirstName as extract dictionary 'FirstNamesDictionary' on D.text as firstName from Document D; export view FirstName; create dictionary LastNamesDictionary as ('Stewart', 'Johnson', 'Smith', 'Hopkins', 'George'); create view NotFirstName as select F.firstName as notFirstName from FirstName F where ContainsDict('LastNamesDictionary', F.firstName); export view NotFirstName; Example 2: Importing the view FirstName by using alias import This example imports one of the views that were created and exported in Example 1. Then, the PersonFirstName view is output so that its results can be viewed. The sample import statement is known as an alias import. It imports the view FirstName , with no module qualifier, to the namespace of the current module, person . The imported view can be accessed only through the alias name PersonFirstName and not through any other form. For example, you cannot refer to the imported view as personName.FirstName because it is imported only through the alias name. module person; import view FirstName from module personName as PersonFirstName; output view PersonFirstName; Example 3: Importing the view NotFirstname without using alias import This sample import statement imports the qualified name personName.NotFirstName (not the unqualified name of the view) to the namespace of the current module, person . Always refer to the imported view by using only the qualified name. Any other mode of reference is flagged as compiler error. module person; import view NotFirstName from module personName; output view personName.NotFirstName; The import module statement {: #aql-import-mod} You can use the import module statement to import and reuse existing AQL modules. Syntax {: #aql-import-mod-syntax} import module <module-name>; Description {: #aql-import-mod-desc} <module-name> Specifies the module to import. The <module-name> must be a simple identifier. Usage notes {: #aql-import-mod-usage} The import statement is only used to import AQL objects from other modules, not from the current module. All import statements must follow immediately after the module declaration and must precede all other types of statements. Only those AQL objects that are explicitly exported from any module can be imported in another module. If this requirement is not observed, a compilation error results. A compilation error is introduced when an import view statement introduces a naming conflict with any create view or other import statement within the same module (not just within the current file). This restriction applies to the import of other AQL objects in addition to views. A compilation error is introduced when an AQL file inside a module tries to reference another exported view of a module without using the import view statement. The same applies to dictionaries, tables, or functions. If two AQL files inside a module import the same view X from another module under two different aliases, for example, A and B, then the two aliases are treated synonymously. This rule applies also to tables, dictionaries, and functions. Examples {: #aql-reference-examples-4} In this example, the import statement imports the qualified name of both the exported views, personName.FirstName and personName.NotFirstName . Any view that is not exported by the module personName is not imported as a part of the import statement Example 1: Import both FirstName and NotFirstName views This example shows all of the exported views from module personName. The views FirstName and NotFirstName were created in the example section of the export statement. ```bash module personOther; -- The following statement would import both views FirstName and NotFirstName. import module personName; ``` The set default dictionary language statement {: #aql-def-dict} The set default dictionary language statement allows an extractor developer to customize the default set of dictionary-matching languages for the containing module. Syntax {: #aql-def-dict-syntax} set default dictionary language as '<language codes>'; Description {: #aql-def-dict-examp} <language codes> Specifies the language for compilation and matching for dictionaries of the module that are declared without an explicit with language as specification. The <language codes> set must be a comma-separated list, with no white spaces around each language code. Failure to observe this requirement can result in a compilation error. This statement affects the following dictionaries: Dictionaries that are explicitly declared in the current module by using the create dictionary or create external dictionary statement, and that statement does not have a with language as clause. Dictionaries from external files. In a pattern specification of an extract pattern statement, atoms of type 'string' and atoms of type <'string' [match parameters]> without an explicit with language as specification. When this statement is absent from inside a module, the runtime component defaults to a language set of German, Spanish, English, French, Italian, and the unspecified language x. It is defined as a set: [de,es,en,fr,it,x_unspecified] . Within a module, only one instance of this statement can exist. Usage notes {: #aql-def-dict-usage} The set default dictionary language statement can be updated to improve the extent of languages that are covered by the extractor. This ability to add languages promotes ease of customization and the reuse of existing extractors. Examples {: #aql-reference-examples-5} Example 1: Specifying languages to be used to match dictionary entries module Dictionaries; -- Set the default dictionary matching language -- for this module to English and French set default dictionary language as 'en,fr'; /** * Dictionary of English and French names. Because no language clause * exists in dictionary definition, module level dictionary matching * setting will be applied. */ create dictionary EnglishFrenchNames from file 'en_fr_names.dict'; /** * Dictionary of Italian names. Language clause in the dictionary * definition will override the module level dictionary matching setting. */ create dictionary ItalianNames with language as 'it' as ( 'firstEntry','secondEntry' ); /** * View to extract pattern: Phone, followed by one to three tokens, * followed by Email. Language clause at the atom level will override * the module level dictionary setting. */ create view PhoneEmailPattern as extract pattern <'Phone'[ with language as 'en']> <Token> {1,3} 'Email' as match from Document D; output view PhoneEmailPattern; The require document with columns statement {: #aql-req-doc-columns} By using the require document with columns statement, you can define the schema of the special view Document at compile time. This schema definition specifies the list of required fields and their types to be found in each tuple of the view Document . Syntax {: #aql-req-doc-syntax} require document with columns <columnName> <columnType> [and <columnName> <columnType>]*; Description {: #aql-req-doc-desc} <columnName> Specifies the name of the column to be used in the schema. The <columnName> is an attribute identifier, which is a simple identifier or a double-quoted identifier. <columnType> Specifies the type of column to be used in the schema of the Document view. The <columnType> can be one of the following data types: Integer, Float, Boolean, or Text. [ and <columnName> <columnType> ]* Specifies additional column names and types to the schema of the Document view. They follow the same rules as <columnName> and <columnType> . In earlier versions of AQL, the schema of the special view Document was predefined to consist of either a single field (text Text) or two fields (text text, label Text). The choice between these schemas was decided at run time. By using the require document with columns statement, you can override the default input document schema at compile time. Usage notes {: #aql-req-doc-usage} For modular AQL code, the scope of any require document with columns statement is the module in which it is defined. Only one require document with columns statement is allowed per AQL file. Within a single module, or a generic module, there can be zero, one, or multiple AQL files that have a require document with columns statement. All AQL files within a module merge their require document with columns statements at the level of the entire module to form a module-wide require document with columns . This statement defines the schema of the Document view for that module. If none of the AQL files of a module or a generic module contain a require statement, the module has a default schema for the view Document . This schema consists of two columns: (text Text, label Text). No default columns are established for the special view Document if at least one AQL file in the module has one require document with columns statement. When multiple modules are combined to form an extractor, the schema of the Document view of the entire extractor is defined by the duplicate-free union of Document schemas for each module. An exception is raised when any column found across the multiple require document with columns statements is found to be conflicting in its type requirements across modules. For example, a module requires a column X with type Y when another module that is being loaded with it requires a column X with type Z. An exception is raised when you run an extractor if the provided input document tuple does not contain all of the required columns. An exception is also raised if a column does not conform to its corresponding required type. When the require document with columns statement is present inside a module, every column of the special view Document that is referenced must be declared in at least one of the require document with columns statements. The statement can be found in different AQL files within the same module. However, all such require document with columns statements would be merged at the level of the module to form a module-wide require document with columns statement. Examples {: #aql-reference-examples-6} Example 1: Require document statement with similar column types The following example defines a document schema that contains four fields of the same type. This AQL sample expects each document tuple of the data collection to contain four columns as defined in the document schema. Refer to JSON document formats for details on how to create a document that conforms to a schema. module person; -- Require document statement with similar field types require document with columns inputDocumentText Text and inputDocumentLabel Text and documentAuthor Text and documentCreationDate Text; Example 2: Require document statement with varying column types The following sample defines a document schema that contains columns of varying field types. module sample; -- Require document statement with varying field types require document with columns bookText Text and purchaseCount Integer and bookPrice Float and isBookPopular Boolean; Example 3: Merging document schemas The example describes how to merge document schemas that use the files first.aql, last.aql, and socialsecurity.aql. first.aql: module person; require document with columns firstName Text; last.aql: module person; require document with columns lastName Text; socialsecurity.aql module person; require document with columns lastName Text and socialSecurityNo Integer; The merged schema is (firstName Text, lastName Text, socialSecurityNo Integer). The create view statement {: #aql-create-view} The top-level components of an AQL extractor are its views. Views are logical statements that define, but do not necessarily compute, a set of tuples. Syntax {: #aql-create-view-syntax} The create view statement can take one of three forms. The simplest form defines a logical view that is composed of the tuples of a single select or extract statement. The second is a multijoin form that defines a view that comprises the tuples that arise from the multiset union of several select or extract statements. The third form defines a new view that contains the set difference between tuples from two select or extract statements. create view <viewname> as <select or extract statement>; create view <viewname> as (<select or extract statement>) union all (<select or extract statement>)...; create view <viewname> as (<select or extract statement>) minus (<select or extract statement>); Description {: #aql-create-view-desc} <viewname> The <viewname> can be a simple identifier or a double-quoted identifier. It cannot contain the period character. <select or extract statement> The select or extract statement creates output that is used to compute the tuples of the containing view. Usage notes {: #aql-create-view-usage} View names are case-sensitive. For example, Person, PERSON, and person are different view names Two views within the same AQL module cannot share a name, which would make them duplicates. However, two views with same name can exist in two different modules, since their fully qualified names are unique. By default, a view that is defined by the create view statement is a non-output view until it is specified as an output view. The select or extract statements of the union all and the minus forms, must have a compatible output schema. Two schemas are considered compatible for a union or minus operation if they have the same number of columns, the column names are in the same order, and they have compatible data types: Fields of the same data type are union or minus compatible. The Span and Text data types are union or minus compatible. In the case of a union between a Span and a Text type, the output type is a Span. However, objects of Text type are not automatically converted into a Span type \u2013 the automatic conversion happens only when required by function calls. Two ScalarLists are union or minus compatible regardless of the underlying scalar type. Examples {: #aql-reference-examples-7} Example 1: Creating a view with a select or extract statement In the example below, the view Phone uses an extract statement to prepare its tuples. The view PhoneNumber uses a select statement to pick specific fields from the view Phone . create view Phone as extract regexes /\\+?([1-9]\\d{2}\\)\\d{3}-\\d{4}/ and /\\+?[Xx]\\.?\\d{4,5}/ on D.text as num from Document D; create view PhoneNumber as select P.num as num, LeftContextTok(P.num, 3) as lc from Phone P; Example 2: Creating a view with Union All statement The view AllPhoneNums prepares a unionized set out of the tuples of Phone and Extension views. The two views that are being unionized have the same schema. create view Phone as extract regex /\\+?([1-9]\\d{2}\\)\\d{3}-\\d{4}/ on D.text as match from Document D; create view Extension as extract regex /\\+?[Xx]\\.?\\d{4,5}/ on D.text as match from Document D; create view AllPhoneNums as (select P.match from Phone P) union all (select E.match from Extension E); Example 3: Creating a view with Minus statement The following example shows how you can use minus to filter out unwanted tuples from a set of tuples. create view Organization as (select * from OrganizationCandidates) minus (select * from InvalidOrganizations); Example 4: Schema compatibility for minus It is important to note that spans over different target text are not of the same type. Consider the following AQL example that explains this difference by using a String literal. create view OneString as select 'a string' as match from Document D; create view TheSameString as select 'a string' as match from Document D; create view Subtraction as (select R.match from OneString R) minus (select R.match from TheSameString R); Instead of the expected output of an empty tuple list, the output is a set of records that have 'a string' as a field value. Although the contents of the OneString and TheSameString views seem identical, the actual text values have different underlying AQL objects. The type of OneString.match is 'Span over OneString.match'. The type of TheSameString.match is 'Span over TheSameString.match'. Since the field types are different, they are not compatible for comparison purposes. To obtain the wanted output of the empty tuple list, you must compare values of the same type. In the following example, the GetString() function converts span objects to string objects to pass compatible types to the minus operation. create view Subtraction as (select GetString(R.match) from OneString R) minus (select GetString(R.match) from TheSameString R); Documenting the create view statement with AQL Doc The AQL Doc comment for a create view statement contains the following information: General description about the view. @field for every column name in the view. Example /** * Extracts all spans that match a phone number pattern from * the input documents. It uses a regular expression to match * phone number patterns. * @field num phone number * @field lc 3 tokens to the left of phone number*/ create view PhoneNumber as select P.num as num, LeftContextTok(P.num, 3) as lc from ( extract regexes /\\+?([1-9]\\d{2}\\)\\d{3}-\\d{4}/ and /\\+?[Xx]\\.?\\d{4,5}/ on D.text as num from Document D ) P; The output view statement {: #aql-output-view} The output view statement defines a view to be an output view. The runtime component outputs only the tuples of views that are marked as output views. The AQL compiler compiles only views that are marked as output or export, or reachable from views that are marked as output or export. Syntax {: #aql-output-view-syntax} output view <view-name> [as '<alias>']; Description {: #aql-output-view-desc} <view-name> The name of the view to output, as known in the name space of the current module. The <view-name> is a simple identifier or a double-quoted identifier. The built-in view Document cannot be output. [as '<alias>']* Defines an <alias> name for the output view. When the optional alias is not specified, the view is output under the following names: In modular AQL, the view is output under the name <module-name>.<view-name> where <module-name> is the name of the module where the view is originally defined (which can be different from the module where the view is output). In AQL 1.4 or earlier, the view is output under the name <view-name> The output ... as <alias> statement is useful when you customize an extractor for different domains. The use of the <alias> name when you define an output view ensures that output names are identical across different implementations of the customization. The <alias> name cannot be used in another select or export statement. You must enclose the <alias> with single quotation marks, and the <alias> name can contain periods. Usage notes {: #aql-output-view-usage} When an AQL extractor is run, it computes resultant tuples for each view that is defined as an output view. The tuples of any non-output view are also computed, but only when they are necessary to compute the resultant tuples of an output view. In modular AQL, the output view statement outputs the tuples of the view under the fully qualified view name that is qualified by its module name. Consider the following example where the statement output view Person; results in the output of the personModule.Person view: module personModule; create view Person as extract dictionary 'FamousPeopleDict' on D.text as match from Document D; output view Person; This behavior applies for any view output without an alias, regardless of whether the view is output in the module where it is defined, or a module where it is imported, even when it is imported by using an alias import. For example, in the output view MyPerson , this example results in the view being output under its original qualified name personModule.Person and not under its local alias MyPerson module employeeModule; import view Person from module personModule as MyPerson; output view MyPerson; The output alias statement is useful when you build libraries of extractors where the same type of entity can have many different implementations depending on the application domain or the language of the input documents. The main benefit of using an alias when you define an output view is to ensure a consistent nomenclature across output views. Consistent nomenclature is expected by the program logic of a user when you process multiple modules that each output a semantically similar view. In modular AQL, when an alias name is used in an output view statement, the tuples of a view are output under the specified alias name. For example, the following code would output the results under the alias name PersonAlias, and the alias name is not qualified with the module prefix. module personModule; create view Person as extract dictionary 'FamousPeopleDict' on D.text as match from Document D; output view Person as 'PersonAlias'; Examples {: #aql-reference-examples-8} The following examples contain two modules, personModuleFrench and personModuleEnglish. Each module outputs a view, named PersonNameFrench and PersonNameEnglish. Suppose similar modules, each of which outputs views that are semantic variants of an extractor for person names. These modules are customized for different languages with the variance in the customization of this view for a specified input language. Eventually, a user might want a program to use modules where the sought output view is named PersonName, irrespective of the modules that are processed. This expectation is normal, since each module that is customized for a language, domain, or another purpose is expected to produce various results. The consumer of these modules does not need to alter the algorithm of their program to accommodate for varying output view names when the underlying semantics are similar. In the example, because the alias PersonName is used, the consumer does not need to alter the view name that is sought. However, the results might vary depending on the modules that are processed. In the example, for instance, the resulting matches are French-based (Example 1) and English-based (Example 2). Example 1: Resulting matches are French-based The following example defines a view PersonNameFrench and outputs it under an implementation-neutral alias name, 'PersonName' . module personModuleFrench; create dictionary FrenchNames as ('Jean', 'Pierre', 'Voltaire', 'Francois', 'Marie', 'Juliette'); create view PersonNameFrench as extract dictionary 'FrenchNames' on D.text as name from Document D; output view PersonNameFrench as 'PersonName'; Example 2: Resulting matches are English-based The following example defines a view PersonNameEnglish and outputs it under an implementation-neutral alias name, 'PersonName' . module personModuleEnglish; create dictionary EnglishNames as ('John', 'Peter', 'Andrew', 'Francis', 'Mary', 'Juliet'); create view PersonNameEnglish as extract dictionary 'EnglishNames' on D.text as name from Document D; output view PersonNameEnglish as 'PersonName'; The consumer of the example modules can access the output tuples through the alias name 'PersonName' . The consumer would not need to know the actual module from which the results are fetched. The extract statement {: #aql-extract} The extract statement is used to extract basic features directly from text. Syntax {: #aql-extract-syntax} extract <select list>, <extraction specification> from <from list> [having <having clause>] [consolidate on <column> [using '<policy>']] [limit <maximum number of output tuples for each document>]; Description {: #aql-extract-desc} <select list> A comma-delimited list of output expressions. The results of these output expressions are returned as the output of the extract statement, along with the tuples that are generated by the evaluation of the extraction specification. The format for the <select list> is the same as the <select list> of a select statement. <extraction specification> Applies the extraction specification over all tuples from the views that are defined in the <from list> . It renames the columns of tuples according to their corresponding aliases that are specified in the extract statement. You can use one of the following extraction specifications: Regular expressions Dictionaries Splits Blocks Part of speech Sequence patterns <from list> A comma-delimited list that is the source of the tuples from which features are to be selected. The format of the <from list> is similar to the format of the <from list> of the select statement. However, if the extract statement does not have a pattern specification, then the <from list> can contain a single item. [having <having clause>] Specifies a filtering predicate (in the <having clause> ) that is applied to each extracted output tuple. The field names that are specified in the <having clause> refer to any aliases that are specified in the <select list> . This clause is optional. [consolidate on <column>[using '<policy>' ]] Defines how to handle overlapping spans as defined in the consolidation <policy> . In this specification, <column> must be the name of an output field, which is part of the extract statement. This clause is optional. [limit<maximum number of output tuples for each document>] Limits the number of output tuples from each document to the specified maximum. This clause is optional. Usage notes {: #aql-extract-usage} The semantics of the extract statement are as follows: Evaluate the extraction specification over each tuple of the input relation. For each result that the extraction produces, an output tuple that contains the extracted values, along with any columns of the original tuple that were specified in the <select list> , is produced. Rename the columns of the output tuple according to the aliases specified as part of the <select list> and the <extraction specification> . Apply any predicates in the optional having clause to the resulting output tuple. Consolidate tuples that pass the predicates according to the optional consolidation clause, and add the resulting tuples to the output. If the optional limit clause is present, limit the output to the specified number of tuples for each document. The semantics of the from clause of an extract pattern statement are different from other forms of extract statements that do not have a pattern specification. If at least one of the views in the <from list> does not contain any tuples on a particular document, then the output of the extract statement is empty. This output is empty because the set of all combinations of tuples in the input views is empty. In the special case of extract pattern statements, the from clause is a placeholder that declares the names of relations that are involved in the pattern specification. The semantics of the statement are driven only by the pattern specification. In particular, the output of the statement can be non-empty even when some of the input views are empty. Examples {: #aql-reference-examples-9} Example 1: Extracting phone numbers from a pre-defined view This sample extract statement evaluates a regular expression for United States phone numbers across input text that is represented by the pre-defined view Document . The output is then restricted to the first three phone numbers that are identified per each document. Field names in the having clause refer to the aliases at the beginning of the extract statement. create view PhoneNumbers as extract D.text as documentText, regex /\\d{3}-\\d{3}-\\d{4}/ on D.text as phoneNumber from Document D having MatchesRegex(/\\d{3}.*/, phoneNumber) limit 3; Example 2: Extracting blocks of capitalized words In this example, the extract statement identifies blocks of two to three capitalized words. In AQL, a block refers to a contiguous span of tokens, in this case two to three tokens. This example also uses a consolidation policy to exclude blocks that are contained within larger blocks from the output set of tuples. create view CapitalizedWord as extract regex /[A-Z][a-z]*/ with flags 'CANON_EQ' on 1 token in D.text as word from Document D; create view TwoToThreeCapitalizedWords as extract blocks with count between 2 and 3 and separation 0 tokens on CW.word as capswords from CapitalizedWord CW consolidate on capswords using 'ContainedWithin'; The consolidate clause is applied to the capswords field by the extract blocks specification. The difference is that the target field referred to by the consolidation clause is an output field of the extract statement. The target field of the select statement is an input field. This behavior is similar to that of the having clause. Example 3: A nested extract or select statement as a view name The input view for an extract statement can be either a view name, as in Example 2, or a nested extract or select statement, as in this example: create view SampleExtract as extract regex /foo/ on E.foobar as foo from (extract regex /foobar/ on D.text as foobar from Document D) E; Example 4: Extract a statement with a select list In this example, we extract matches for a pattern, and at the same time, select multiple attributes from the input views. create view Person as extract.F.first as first, M.initial as middle, L.last as last pattern ('Mr.'|'Ms.'|'Miss')? (<F.first> <M.initial>? <L.last>) return group 0 as reference and group 1 as salutation and group 2 as name from FirstName F, MiddleInitial M, LastName L; Regular expressions Use a regular expression extraction specification to identify matching patterns that are contained by the regular expression across the input text. Dictionaries Use the dictionary extraction specification to extract strings from input text that are contained in a dictionary of strings. Splits Use the split extraction specification to split a large span into several smaller spans. Blocks Use the blocks extraction specification to identify blocks of contiguous spans across input text. Part of speech Use the part-of-speech extraction specification to identify locations of different parts of speech across the input text. Sequence patterns Use the pattern extraction specification to perform pattern matching across an input document and other spans that are extracted from the input document. Regular expressions {: #aql-regx} Use a regular expression extraction specification to identify matching patterns that are contained by the regular expression across the input text. Syntax {: #aql-regx-syntax} regex[es] /<regex1>/ [and /<regex2>/ and ... and /<regex n>/] [with flags '<flags string>'] on <token spec>] <name>.<column> <grouping spec> Description {: #aql-regx-desc} regex[es] /<regex1>/ Specifies the regular expressions to be used in the extraction. By default, AQL uses Perl syntax for regular expressions, which means that regular expression literals are enclosed in two forward slash (//) characters. The regular expression syntax is the same as the syntax of Java\u2122 class java.util.regex.Pattern , except that regular expression escape sequences take precedence over other escape characters. AQL also allows regular expressions in SQL string syntax, so a regular expression for United States phone numbers can be expressed as either of the examples: bash /\\d{3}-\\d{5}/ bash '\\\\d{3}-\\\\d{5}' [and /<regex2>/ and ... and /<regex n>/ ] Lists more regular expressions to be used in the extraction. [with flags '<flags string>'] Specifies a combination of flags to control regular expression matching. This parameter is optional. These flags correspond to a subset of the flags that are defined in the Java\u2122 implementation, see class java.util.regex.Pattern . If the flags string is not provided, AQL uses only the DOTALL flag by default. To specify multiple flags, separate them with the | character. For example, to specify multiline matching, matching that is not case-sensitive, and Unicode case folding, use the flags string 'MULTILINE|CASE_INSENSITIVE|UNICODE' . [<token spec>] Indicates whether to match the regular expression only on token boundaries. bash ... [[between <number> and] <number> token[s] in] ... Token constraints are an optional part of the specification. If token constraints are omitted, AQL returns the longest non-overlapping match at each character position in the input text. If token constraints are present, the extract statement returns the longest match at every token boundary that is within the specified range of tokens in length. Each returned match must start at the beginning of a token and end at the end of a token. If multiple overlapping matches exist, the extract statement returns all of them. The locations of token boundaries depend on what tokenizer the runtime component is using to tokenize the document. If the engine is using the Standard tokenizer, then a token is defined as a sequence of word characters or a single punctuation character. For example, consider the string: bash \"The fish are pretty,\" said the boy. The token boundaries are identified at these locations: bash [\"][The] [fish] [are] [pretty][,][\"] [said] [the] [boy][.] <name>.<column> The view name and column name on which the regular expression is applied. <grouping spec> Determines how to handle the capturing of groups in the regular expression. Capturing groups are regions of the regular expression match that are identified by parentheses in the original expression. Capturing groups are numbered by counting their opening parentheses from left to right. Each group captures the portion of the regular expression match starting at the corresponding open parenthesis ( until its corresponding end parenthesis ) . For example, the expression ((fish)(cakes) and fries) has three capturing groups: Group 0 is the entire match fishcakes and fries . Group 1 is also fishcakes and fries , corresponding to the first open parenthesis ( . This group happens to encompass the entire match, since its corresponding end parenthesis ) is at the end of the expression. Group 2 is fish , corresponding to the second open parenthesis ( . Group 3 is cakes , corresponding to the third open parenthesis ( . When you specify group IDs in the return clause of the syntax, each group ID must correspond to a valid group within each regular expression specified as part of the extract regex clause in the syntax. This is the format of the grouping specification: bash return group <number> as <name> [and group <number> as <name>]* Non-capturing groups in the regular expression (groups starting with (?: ) do not count towards the group numbers. For example, in (?:fish)(cakes) there is a single group, capturing cakes . The group spanning fish is non-capturing, and does not count towards the number of groups. For more details about semantics of capturing groups, see Java\u2122 class java.util.regex.Pattern . To return only Group 0 (the entire match), you can use a shorter, alternative format as in Example 1. This format is equivalent to return group 0 as <name> . <name> can be a simple identifier or a double-quoted identifier. Usage notes {: #aql-regx-usage} In general, AQL supports the same features as the Java\u2122 8 regular expression implementation, as described in Java class java.util.regex.Pattern . The runtime component contains several regular expression engine implementations, including the built-in implementation of Java. During compilation, the Optimizer examines each regular expression and chooses the fastest engine that can run the expression. The alternative execution engines might have slightly different semantics for certain corner cases. In particular, AQL does not guarantee the order in which alternatives are evaluated. For example, suppose that an extract statement matches the regular expression /fish|fisherman/ , over the text 'fisherman'. The statement might match either 'fish' or 'fisherman', depending on which regular expression engine is used internally. AQL flag string Java flag Description CANON_EQ CANON_EQ Canonical equivalence: Different Unicode encodings of the same character are considered equivalent. CASE_INSENSITIVE CASE_INSENSITIVE Perform matching that is not case-sensitive.By default, case-insensitive matching assumes that only characters in the US-ASCII character set are matched. Unicode-aware case-insensitive matching can be enabled by specifying the UNICODE flag with this flag. UNICODE UNICODE_CASE If case-insensitive matching is specified, use Unicode case folding to determine whether two characters are equivalent in a case-insensitive comparison in a manner that is consistent with the Unicode Standard. By default, case-insensitive matching assumes that only characters in the US-ASCII character set are matched. Note: The behavior of this flag is not defined when it is used without the CASE_INSENSITIVE flag. DOTALL DOTALL Make the dot character . match all characters, including newlines. LITERAL LITERAL Treat the expression as a sequence of literal characters, ignoring the normal regular expression escape sequences. MULTILINE MULTILINE Make the characters ^ and $ match the beginning and end of any line, as opposed to the beginning and end of the entire input text. UNIX_LINES UNIX_LINES Treat only the UNIX\u2122 newline character \\n as a line break, ignoring the carriage return character \\r . Follow these guidelines to make your extractors run faster and be easier to maintain: Avoid long, complex regular expressions and use simpler, smaller regular expressions that are combined with AQL statements. Avoid unnecessary use of lookahead and lookbehind in regular expressions. You can usually achieve the same effect by adding predicates to the having clause of your extract statement. Use token constraints in your regular expression extraction specifications when possible. Examples {: #aql-reference-examples-10} Example 1: Using canonical Unicode equivalence to determine matches This example shows you how to find capitalized words that are not given names. Canonical Unicode character equivalence is used to determine matches. Notice the usage of the flag \u2018CANON_EQ\u2019 and that regex is performed on tokens: create dictionary FirstNamesDict as ( 'Aaron', 'Matthew', 'Peter' ); create view NotFirstName as extract regex /[A-Z][a-z]*/ with flags 'CANON_EQ' on 1 token in D.text as word from Document D having Not(ContainsDict('FirstNamesDict', word)); Example 2: Usage of capturing groups The following example demonstrates the usage of capturing groups in an extract regex statement. The code extracts the fields of a United States phone number by using capturing groups: create view Phone as extract regex /(\\d{3})-(\\d{3}-\\d{4})/ on between 4 and 5 tokens in D.text return group 1 as areaCode and group 2 as restOfNumber and group 0 as fullNumber from Document D; Example 3: Applying multiple regex over input text You can specify multiple regular expressions in the same extract regex statement by using the regexes syntax. create view PhoneNum as extract regexes /(\\d{3})-(\\d{3}-\\d{4})/ and /[Xx]\\d{3,5}/ on between 1 and 5 tokens in D.text as num from Document D; Example 4: Misuse of grouping specification The regular expression in this code sample does not contain group -1 or group 3000 . This results in a compilation error. create view ErrorExample as extract regex /(\\d+)/ on D.text return group -1 as wrongGroup and group 3000 as nonExistentGroup from Document D; Dictionaries {: #aql-reference-dictionaries} Use the dictionary extraction specification to extract strings from input text that are contained in a dictionary of strings. Syntax {: #aql-ref-dict-syntax} dictionar[y|ies] '<dictionary>' [and '<dictionary>' and ... and '<dictionary>'] [with flags '<flags string>'] Description {: #aql-ref-dict-desc} '<dictionary>' References a dictionary that is created by using either the create dictionary statement, create external dictionary statement, or a dictionary file on the file system. [and '<dictionary>' and ... and '<dictionary>'] References additional dictionaries to be used for extraction. [with flags'<flags string>'] Controls dictionary matching. Currently, two options are supported: Exact Provides exact, case-sensitive matching. IgnoreCase Provides matching that is not case-sensitive. If no flag is specified, the dictionary matches based on any flag that was specified when it was created. If no flag is specified during creation, it matches by using the IgnoreCase flag. Usage notes {: #aql-ref-dict-usage} Dictionaries are always evaluated on token boundaries. Specifically, a dictionary entry matches a region of text if the first token of the entry matches the first token of the text region, the second token of the entry matches the second token of the text region, and so on. Characters between two consecutive tokens are ignored. For example, assume that you are using a simple white-space based tokenization model that is appropriate for a language such as English. Also, assume that the input text is \u201cLet\u2019s go fishing!\u201d If a dictionary consists of the term go fish , no match exists in the text for Let's go fishing! . However, if the dictionary consists of the entry go fishing (notice the two white spaces between go and fishing), one match exists in the text Let's go fishing! . White space specifies that go and fishing are two distinct tokens. If one or more white-space characters exists between the two tokens go and fishing in the input text, a match is made. For each match of a dictionary entry with input text, the extract dictionary statement generates an output tuple. Examples {: #aql-reference-examples-11} Example 1: Extracting terms from dictionary files Find person names by using dictionary files of common given names and surnames with case-sensitive matching. create view Name as extract dictionaries 'first.dict' and 'last.dict' with flags 'Exact' on D.text as name from Document D; The following is sample content of last.dict : #Dictionary for surnames Anthony Aparicio Cate Lehmann Radcliff The following is sample content of first.dict : #Dictionary for given names Aaron Candra Freeman Mathew Matthew Zoraida Note: A dictionary file system that is directly referenced in an extract dictionary statement cannot be explicitly configured with a set of languages so that the dictionaries are compiled and applied at run time. Instead, the set of languages is specified with the set default dictionary language statement if the module contains this statement. Therefore, direct referencing of dictionary files in an extract dictionary statement is not recommended and might be discontinued in the future. The preferred practice is to explicitly define a dictionary object by using the statement create dictionary from file , and then use that dictionary in the extract statement. The compiler and runtime component attempt to locate dictionary files that are referenced in AQLs under the configured search path. Example 2: Extracting terms from an inline dictionary Find conjunctions by using an inline dictionary and the default matching that is not case-sensitive. create dictionary ConjunctionDict as ( 'and', 'or', 'but', 'yet' ); create view Conjunction as extract dictionary 'ConjunctionDict' on D.text as name from Document D; Splits {: #aql-splits} Use the split extraction specification to split a large span into several smaller spans. Syntax {: #aql-splits-syntax} split using <name>.<split point column> [retain [right|left|both] split point[s]] on <name>.<column to split> as <output name> Description {: #aql-splits-desc} The split extraction specification takes two arguments: A column that contains longer target spans of text. A column that contains split points. The splitting algorithm works in two passes over the input view. The first pass groups all of the input tuples by the target column. The second pass goes through the tuples in each group, splitting the target column with each value of the splitting column. <name>.<split point column> Specifies the split points for the extraction. [retain [right|left|both] split point[s]] Specifies how to treat the left and right end points of each result. This argument is optional. If retain left split point is specified, then each output span also contains the split point to its left, if such a split point exists. If retain right split point is specified, then the system makes each output span contain the split point to its right. The split extraction also accepts null values as split points. For each such value, the extraction returns a tuple that contains the entire input span. <name>.<column to split> Specifies the target column for the extraction. <output name> Defines the name of the output of the extraction. Examples {: #aql-reference-examples-12} Example 1: Split points and the retain clause If the split points are all the instances of the word fish in the phrase fish are swimming in the fish pond, then the various versions of the retain clause have the following effects: retain clause omitted \" are swimming in the \" and \" pond\" retain right split point \" are swimming in the fish\" and \" pond\" retain left split point \"fish are swimming in the \" and \"fish pond\" retain both split points \"fish are swimming in the fish\" and \"fish pond\" Example 2: Split extraction This example splits the document into sentences. It first uses a regular expression to identify sentence boundaries, then uses the split extraction specification to split the document text on the sentence boundaries. create dictionary AbbreviationsDict as ( 'Cmdr.', 'Col.', 'DR.', 'Mr.', 'Miss.'); create view Sentences as extract split using B.boundary retain right split point on B.text as sentence from ( extract D.text as text, regex /(([\\.\\?!]+\\s)|(\\n\\s*\\n))/ on D.text as boundary from Document D -- Filter the candidate boundaries. having Not(ContainsDict('AbbreviationsDict', CombineSpans(LeftContextTok(boundary, 1), boundary))) ) B; Blocks {: #aql-blocks} Use the blocks extraction specification to identify blocks of contiguous spans across input text. Syntax {: #aql-blocks-syntax} blocks with count [between <min> and] <max> and separation [between 0 and] <max> (tokens| characters) on <name>.<column containing spans> as <output name> Description {: #aql-blocks-desc} with count [between<min> and] <max> Specifies how many spans can make up a block. The <min> and <max> values specify the minimum and maximum number of spans that can make up a block. [between 0 and] <max> Specifies the separation distance that is allowed between spans before they are no longer considered to be contiguous. (tokens| characters) Specifies whether the separation distance of the span represents the number of tokens or the number of characters. <name>.<column containing spans> The view name and column name on which the block operator is to be applied. <output name> Specifies a name for the output from the block operator. Usage notes {: #aql-blocks-usage} If the input scans contain multiple overlapping blocks in the input spans, a block extraction statement returns all possible blocks. Use consolidation to filter out redundant blocks. An extract statement with block extraction specification yields blocks that each consist of an aggregation of values of a certain field from across multiple input tuples. Therefore, its select list cannot include fields from its input view. Examples {: #aql-reference-examples-13} Example 1: Extract blocks of words within a character range In the following code, the view TwoToThreeCapitalizedWords identifies blocks of two to three capitalized words within 100 characters of each other. create view CapitalizedWords as extract regex /[A-Z][a-z]*/ with flags 'CANON_EQ' on 1 token in D.text as word from Document D; create view TwoToThreeCapitalizedWords as extract blocks with count between 2 and 3 and separation between 0 and 100 characters on CW.word as capswords from CapitalizedWords CW; Example 2: Extract blocks of words within a token range The following code identifies blocks of exactly two capitalized words within five tokens of each other. create view TwoCapitalizedWords as extract blocks with count 2 and separation between 0 and 5 tokens on CW.word as capswords from CapitalizedWords CW; Part of speech {: #aql-pos} Use the part-of-speech extraction specification to identify locations of different parts of speech across the input text. Syntax {: #aql-pos-syntax} part_of_speech '<part of speech spec>' [and '<part of speech spec>']* [with language '<language code>'] [and mapping from <mapping table name>] on <input column> as <output column> from <input view> Description {: #aql-pos-desc} '<part of speech spec>' Identifies the parts of speech to extract from the input text. The '<part of speech spec>' is one of the following strings: A combination of an internal part-of-speech name and flags, as defined by a mapping table [and '<part of speech spec>']* Identifies the additional parts of speech tags for extraction. [with language '<language code>'] Specifies the language to be used in the extraction. The <language code> is a two-letter, lowercase language code, such as 'en' or 'ja'. If this argument is omitted, the language for part-of-speech extraction is assumed to be English [and mapping from <mapping table name>] Specifies the name of an AQL table that maps raw part-of-speech tags such as \"NOUN\" to combinations of high-level parts of speech and flags. While the optional mapping table can have variable names, a part-of-speech mapping table is required to have these column names: tag The column that holds a part-of-speech tag generated by your tokenizer. basetag The column that holds the corresponding internal tag. flagstr The column that holds a comma-delimited list of flags that are associated with the indicated part of speech. The mapping table must be defined by using the create table statement in the same module as the extract part_of_speech statement that uses it. It cannot be an imported table, and it cannot be an external table. bash create table POSMapping_EN(tag Text, basetag Text, flagstr Text) as values ('CCONJ','CONJ','coordinating'), ('SCONJ','CONJ','subordinating'); <input column> Specifies the column of the input view from which to extract part-of-speech information. <output column> Specifies the name of the column where the spans of the tokens with the indicated parts of speech are sent. <input view> Specifies the input view from which to extract part-of-speech information. Usage notes {: #aql-pos-usage} Part of speech extraction is not provided by the built-in tokenizer. If the system uses the Standard tokenizer, a part_of_speech extraction generates an error. Examples {: #aql-reference-examples-14} Example 1: Using a part of speech tag directly in an extract statement The view EnglishNoun extracts English nouns (singular or mass) or proper nouns (singular). create view EnglishNoun as extract parts_of_speech 'NOUN' and 'PROPN' with language 'en' on D.text as noun from Document D; Sequence patterns {: #aql-seq-patterns} Use the pattern extraction specification to perform pattern matching across an input document and other spans extracted from the input document. Syntax {: #aql-seq-ptn-syntax} The general syntax of a sequence pattern is to first specify the pattern to be matched in the text, and then to specify what is to be returned by the extractor. The final part of the sequence pattern specifies what is the input to the pattern; it might be a column from a previously defined view, or it might be the entire document text. pattern <pattern specification> [return clause] [with inline_match on <viewname.colname>] Description {: #aql-seq-ptn-desc} <pattern specification> A <pattern specification> is composed of multiple Atoms. An individual Atom can be a column from an already-defined view, a fixed string, or a regular expression. You can specify your Atoms to be optional and repeating, and specify token gaps between Atoms. The pattern specification is part of a larger AQL statement, which includes an extract clause. Here is a simple example of how to create a view that contains three adjacent matches from earlier defined views. In this example, the entire combination is returned, which is what group 0 refers to: bash create view Money as extract pattern <C.match> <N.match> <Q.match> return group 0 as match from Currency C, Number N, Quantifier Q; If your Atoms do not need to be exactly adjacent to each other, then you can use token gaps between the Atoms to allow for more matches. This example finds mentions of persons that follow within 0 to 2 tokens by a phone number. Notice the <Token>{0,2} construct, which indicates that a gap of 0 to 2 tokens between the person and phone annotations is allowed. bash create view Phone as extract regex /(\\d{3})-(\\d{3}-\\d{4})/ on between 4 and 5 tokens in D.text return group 1 as areaCode and group 2 as restOfNumber and group 0 as fullNumber from Document D; create view PersonPhone as extract pattern (<P.name>) <Token>{0,2} (<Ph.fullNumber>) return group 0 as match and group 1 as person and group 2 as phone from Person P, Phone Ph; Token gap constructs are restricted to occur within sequence expressions. Also, each token gap in a sequence must be preceded and followed by a \"non-token gap\" expression. As a result, extract pattern statements produce exceptions: bash -> pattern consisting only of a token gap is an error extract pattern <Token> as match from ... -> pattern beginning with a token gap is an error extract pattern <Token> {0,2} <Ph.phone> as match from ... -> pattern ending with a token gap is an error extract pattern <P.name> <Token> ? as match from ... -> group consisting only of a token gap is an error extract pattern <P.name> (<Token>) <Ph.phone> as match from ... Use the (min,max) syntax to indicate the number of times each Atom repeats. You can also use the ? syntax to indicate that an Atom or repeating Atom is optional. Atoms, along with their indications for repeating and optional, are combined to create sequences. Here is a more complex example that shows how to repeat elements. Find candidate hotel names by identifying occurrences of one to three capitalized words, followed by a 'Hotel' or 'hotel' token. ```bash create view CapsWord as extract regex /[A-Z][a-z]*/ on 1 token in D.text as word from Document D; create view HotelCandidate as extract pattern {1,3} /[Hh]otel/ as hotelname from CapsWord CW; ``` You can also use the | operator to indicate a choice between Atoms, as in extract pattern <A.match>| <B.match> <C.match> as match from Apple A, Bacon B, Chocolate C; . This pattern can be explained as \u201cmatch either one A.match OR a sequence of a B.match followed by a C.match . You can see a complete example that uses the | operator in Example 1. After you created your pattern, each match to your <pattern specification> constructs an output result according to the return clause of the pattern specification, as well as the optional <select list> at the beginning of the extract statement. The results are filtered and consolidated according to the having , consolidate , and limit clauses of the extract statement. For example, if multiple overlapping matches exist for the pattern specification, all of the possible matches are returned, and you can use a consolidation clause to filter out redundant outputs. Consider the previous example, but now the goal is to remove matches that contain the word 'Sheraton', and to consolidate resulting matches by removing the ones that are contained within a larger match. For example, we do not want to find \u201cBest Garden Hotel\u201d and also \u201cGarden Hotel\u201d across the same span of text. bash create view HotelCandidate as extract pattern <CW.word>{1,3} /[Hh]otel/ as hotelname from CapsWord CW having Not(ContainsRegex(/.*Sheraton.*/,hotelname)) consolidate on hotelname using 'ContainedWithin'; Now that you are familiar with the syntax and some examples, this diagram outlines the full syntax for the pattern specification. Refer to this full syntax when you start building patterns to see how to structure the pattern that you want to build. If you are familiar with POSIX character-based regular expressions, then you recognize that the syntax is similar. In this case, the syntax allows for white space between elements, and what an element can be in order to suit the AQL purposes is also defined.Note that the term Alternation in this case means choice. Using a vertical bar between elements indicates that a choice exists, which can be grouped by using ( ) . Pattern -> Alternation Alternation -> Sequence | Sequence | ... | Sequence Sequence -> Optional Optional ... Optional Optional -> Repeat | Repeat ? Repeat -> Atom | Atom { min, max } Atom -> <view_name.column_name> | 'string' | <'string' [match parameters]> | /regex/ | <Token> | Group Group -> ( Pattern ) Specifically, an Atom can have six formats: <view_name.column_name> Specifies a column from one of the views, table, or table function references that are named in the from list of the extract pattern statement. 'string' Specifies a match to the specified string by using the AQL default dictionary match semantics. <'string' [match parameters]> Specifies a match to the specified string by using the dictionary matching semantics that are specified by the [match parameters] . The format of [match parameters] is case (exact | insensitive) . This format specifies the type of case-folding that is used to determine the string matches. To specify an exact case-sensitive match, select exact. To specify a match that is not case-sensitive match, select the default value insensitive. /regex/ Specifies a character-based regular expression match with matching constrained to a single token in the document text. <token> A match for any token. In addition, the syntax allows a special token gap construct to be specified in a sequence expression to indicate a match between min and max number of tokens, using the repetition construct, for example <token>{3, 5} matches between 3 and 5 tokens. [return clause] Generates extracted values for each match of the pattern expression according to the return clause. The return clause has the same semantics as the return clause in an extract regex statement. [with inline_match on <viewname.colname>] For the Atoms such as string and regex Atoms, the with inline_match clause determines which text object the system uses for string or regex extraction. For example, if the clause is with inline_match on Email.subject , then all dictionaries and regular expressions defined inline in the pattern specification are applied to the subject field of the Email view. If the with inline_match is absent, string and regular expression extraction are run by default on the entire Document.text. In this case, viewname must be the name of a view or table that is defined in the current module, or imported from another module; references to table functions are not allowed in the with inline_match clause. [with language as <language code(s)>] Specifies a comma-delimited list of two-letter language codes, such as en (English) or zh (Chinese) for the languages on which to evaluate the string. There is no match on documents whose language code is not contained in this string. If the language parameter is omitted, the evaluation language defaults to one of the following language sets: If it is declared, the language sets that are specified through the set default language statement in the containing module. The language sets that contain German (de), Spanish (es), English (en), French (fr), Italian (it), and the unspecified language (x_unspecified) Usage notes {: #aql-seq-ptn-usage} The semantics of an extract pattern statement is driven by the pattern specification. Each match constructs an output result according to the return clause of the pattern specification and the select list at the top of the extract statement. The results are filtered and consolidated according to the having , consolidate , and limit clauses of the extract statement. If multiple overlapping matches exist for the pattern specification, a pattern extraction outputs all possible matches. Use consolidation to filter redundant outputs. The semantics of the from clause of an extract pattern statement are different from other forms of extract statements that do not have a pattern specification. The general semantics of an extract statement require that the extraction specification is evaluated over each combination of the views that are defined in the <from list> . If at least one of the views in the <from list> does not contain any results on a particular document, then the output of the extract statement is empty because the set of all combinations of results in the input views is empty. In the special case of extract pattern statements, the from clause is a placeholder that declares the names of relations that are involved in the pattern specification. The semantics of the statement are driven only by the pattern specification. In particular, the output of the statement can be non-empty even when some of the input views are empty. An extract statement that uses sequence pattern extraction can carry forward the columns of any view in the from list, but only if the view name does not appear in a repeat element of the pattern specification. For example, the statement CapsWordOneToThree results in a compilation error. The error occurs because the carried forward column CW.type at the top of the extract statement belongs to the view name CW , which is in the repeat element <CW.word>{1,3} of the pattern specification. ```bash create view CapsWord as extract 'UpperCase' as type, regex /[A-Z].*/ on 1 token in D.text as word from Document D; ---> This results in and error due to the repeating element CW.word create view CapsWordOneToThree as extract CW.type as type, pattern {1,3} as match from CapsWord CW; output view CapsWordOneToThree; ``` For columns that are carried forward from view names that appear in alternation or optional elements of the pattern specification, the value of the output column is null when the corresponding alternative or the optional element is not present in the text. An example that illustrates this point is in the Person view of Example 1. {: note} Groups that occur under a repeat element cannot be output in the return clause of the statement. For example, the following statement causes an exception: bash create view CapsWordOneToThree as extract pattern (<CW.word>){1,3} return group 0 as fullmatch and group 1 as word -- not allowed due to repeat from CapsWord CW; Examples {: #aql-reference-examples-15} Example 1: Sequence pattern with capturing groups The goal of this example is to find person names by identifying occurrences of given name, optionally followed by middle initial, followed by a surname, and the entire match that is optionally preceded by a common salutation. In addition, the extractor returns the entire match as reference, the first group as salutation, and the second group as name and carry forward the values of the given name, middle initial, and surname from the respective input views. create view MiddleInitial as extract regex /\\b([\\p{Lu}\\p{M}*]\\.\\s*){1,5}\\b/ on between 1 and 10 tokens in D.text as initial from Document D; create view Person as extract F.first as first, M.initial as middle, L.last as last, pattern ('Mr.'|'Ms.'|'Miss')? (<F.first> <M.initial>? <L.last>) return group 0 as reference and group 1 as salutation and group 2 as name from FirstName F, MiddleInitial M, LastName L; Since the subpattern expression ('Mr.'|'Ms.'|'Miss')? is optional, the value of the salutation output column is null when a salutation is not present in the text. Similarly, since the pattern subexpression <M.initial>? is optional, the value of the output column middle is null when a middle initial is not present. Example 2: Sequence pattern with string match and match parameters The goal of this example is to find occurrences of meeting notes for known projects by examining the title annotations of the document. Notice the with inline_match clause, which specifies that string matching is done over the match field of the view Title , as opposed to the entire document text. create view Project as extract regex /[Pp]roject\\s?\\w*/ on D.text as name from Document D; create view Title as extract regex /[A-z][a-z]+.*/ on between 1 and 20 tokens in D.text as match from Document D; create view MeetingNote as extract pattern <'Meeting Notes:'[with case exact]> (<P.name>) return group 0 as match and group 1 as projectname with inline_match on Title.match from Project P; Example 3: Sequence pattern that returns non-empty results even when an input view is empty The following statement generates results even when the input view LastName is empty. The second part of the pattern specification, <L.name>? contains an optional element. The semantics of the pattern specification is designed to output all spans that are composed of either a FirstName.name span or a FirstName.name span that is immediately followed by a LastName.name span. Therefore, on documents for which the view LastName is empty, the result of the statement consists of all spans that comprise a single FirstName.name span that is identified from that document. create dictionary FirstNamesDict as ( 'Aaron', 'Matthew', 'Peter' ); create dictionary LastNamesDict as ( 'Anthony', 'Lehman', 'Radcliff' ); create view LastName as extract dictionary 'LastNamesDict' on D.text as last from Document D having MatchesRegex(/((\\p{L}\\p{M}*)+\\s+)?\\p{Lu}\\p{M}*.{1,20}/, last); create view FirstName as extract dictionary 'FirstNamesDict' on D.text as first from Document D having MatchesRegex(/\\p{Lu}\\p{M}*.{1,20}/, first); create view PersonName as extract pattern <F.first> <L.last>? as fullName from FirstName F, LastName L; The select statement {: #aql-select-stmt} The select statement in AQL provides a powerful mechanism for using various specifications to construct and combine sets of tuples. Syntax {: #aql-select-stmt-syntax} The select statement is similar in structure to an SQL SELECT statement: select `<select list>` from `<from list>` [where `<where clause>`] [consolidate on `<column>` [using '`<policy>`' [with priority from `<column> ` [priority order]]]] [group by `<group by list>`] [order by `<order by list>`] [limit `<maximum number of output tuples for each document>`]; Description {: #aql-select-stmt-desc} <select list> A comma-delimited list of output expressions. <from list> A comma-delimited list that is the source of the tuples to be selected. [where <where clause>] Defines a predicate to be applied on each tuple that is generated from the Cartesian product of all of the tuples in the relations in the from clause. This clause is optional. [consolidate on<column>[using '<policy>' [with priority from <column> priority order]]] Defines a consolidation policy to manage overlapping spans. This clause is optional. [group by<group by list>] Groups the tuples that are produced from the same document by common values of a specified field. This clause is optional. [order by<order by list>] Orders the output tuples that are produced by the select statement from each document. The order is based on the values of the order-by list, a comma-delimited list of expressions. This clause is optional. [limit <maximum number of output tuples for each document>] Limits the number of output tuples for each document to the specified maximum. This clause is optional. Usage Notes {: #aql-select-stmt-usage} The semantics of the select statement are as follows: Determine the input data (in tuples) by taking the Cartesian product of relations in the from list. For each input tuple that is generated, filter it by applying the predicates in the (optional) where clause. If the optional group by clause is present, group tuples that are produced from the same document by the values that are specified in the group-by list and compute the result of the aggregate functions within the select list. Consolidate any overlapping tuples, according to the policy defined in the (optional) consolidation clause. If the optional order by clause is present, order these tuples by the values of the order-by list. Compute all expressions within the select list on each tuple, and rename the columns as specified by the as clauses. If the optional limit clause is present, limit the number of output tuples to the specified number of tuples for each document. Examples {: #aql-reference-examples-16} An example of how to use the select statement is to extract phone numbers that match a pattern. Assume that the PhoneNumbers view that extracts phone numbers of the pattern XXX-XXX-XXXX for United States is already defined. This select statement evaluates the regular expression for the pattern 444-888-XXXX across the input text. The view has the output columns documentText and phoneNumber . In addition, the output is limited to the first occurrence of this phone number pattern that is identified per document. create view PhoneNumbersPattern1 as select D.documentText, D.phoneNumber from PhoneNumbers D where MatchesRegex(/444-888-\\d{4}/,D.phoneNumber) limit 1; Another example of how you can use the select statement is to find approximate mappings of people and their corresponding phone numbers. Assume that the view Person is already defined, and that it has the columns person and the view PhoneNumbers . This select statement evaluates the where clause to find text spans that contain a person mention followed by a phone number within 1 to 3 words or tokens. The input to this statement is represented by a join of the Person and PhoneNumbers views in the from list. create view PersonPhone as select P1.documentText, P1.person, P2.phoneNumber, CombineSpans(P1.person,P2.phoneNumber) as personPhoneSpan from Person P1, PhoneNumbers P2 where FollowsTok(P1.person,P2.phoneNumber,1,3); The personPhoneSpan column will contain the matching spans that give the approximate person-phone mapping. personPhoneSpan John : 433-999-1000 Martha Mob 433-999-1001 The select list The select list in an AQL select or extract statement consists of a comma-delimited list of output expressions. The from list The second part of a select or an extract statement in AQL is the from list. The from list is a comma-separated list that is the source of the tuples to be selected or extracted. The where clause The optional where clause defines a predicate to be applied on each tuple generated from the Cartesian product of all tuples in the relations in the from clause. The consolidate on clause The optional consolidate on clause specifies how overlapping spans are resolved across tuples that are output by a select or extract statement. Tuples with non-overlapping spans are not affected when this clause is used. The group by clause The optional group by clause of a select statement directs the runtime component to group tuples that are produced from the same document by common values of a specified field. The order by clause The optional order by clause directs the runtime component to order the output tuples that are produced by the select statement from each document based on the values of the order by list, which is a comma-delimited set of expressions The limit clause The optional limit clause specifies a limit on the number of output tuples that are produced by the select statement for a document. The select... into statement The select ... into statement is useful for defining a view and specifying that it is an output view in a single statement. The select list {: #aql-select-list} The select list in an AQL select or extract statement consists of a comma-delimited list of output expressions. Syntax {: #aql-sel-list-syntax} Each select expression must be in one of the following forms: select <viewname>.<colname> as <alias> | <viewname>.* | <expr> as <alias> | case when <predfunction1()> then <expr1> when <predfunction2()> then <expr2>... when <predfunctionn()> then <exprn> [else <expr_default>] as <name> Description {: #aql-sel-list-desc} <viewname>.<colname> as <alias> <viewname> Specifies the view from which to select columns. <colname> Specifies the column in that view. <alias> Specifies the name by which the selected field is known. This field is an optional field. It is selected to be a part of each output tuple. If <alias> is not specified, the name of the column is by default the <colname> . Can be a simple identifier or a double-quoted identifier. <viewname>.* Specifies the name of a view. This syntax indicates that all columns of the specified view must be carried forward to the enclosing select or extract statement. Like SQL, AQL allows the shorthand select * statement. The effect of this statement is to select all columns from all inputs that are specified in the from clause of the select statement. However, the shorthand extract * statement is not supported. <expr> as <alias> Represents the assignment of an expression to an attribute of the encompassing view. <expr> Specifies an expression that is composed of scalar function calls, aggregate function calls, or a constant. <name> Represents the name of the column that holds the result of the expression that is specified by <expr> as <alias> . If <alias> is not specified, the name of the column is by default the <name> . Can be a simple identifier or a double-quoted identifier. when<function1()> then <expr1> when <function2()> then <expr2> ... when <functionN()> then <exprn> [else ] <expr_default> as <name> <function1()>, <function2()>, <functionN()> Specify scalar functions that return type Boolean. <expr1>, <expr2>, <exprn>, <expr_default> Specify expressions that are composed of scalar function calls and must return the same type. If the result of <function1()> is true, then the result of the case expression is the result of <expr1> , and none of the subsequent when clauses are evaluated. Otherwise, subsequent when clauses (if any) are evaluated in the same manner. When none of the conditions of the when clauses are satisfied, the result of this case expression is the result of the default expression <expr_default> . This expression is specified in the optional else clause. If the [else] clause is absent, the result of this case expression is null . Usage notes {: #aql-sel-list-usage} The following statement is not supported: bash select * from Document; The contents of the Document view might not be fully known in the current context or scope of the .aql file where this select statement is issued. The lack of information about the contents is because multiple require document with columns statements provided outside of the current .aql file might alter the eventual schema definition of this special Document view when it is used at the level of a module. The shorthand Document.* statement is not a valid AQL construct. You can explicitly select fields from the Document view. The following example shows a valid explicit selection of fields from a Document view: bash select D.label as label, D.text as text from Document D; Examples {: #aql-reference-examples-17} The following examples illustrate various forms of the select list. Example 1: Explicit value assignment using a constant This example shows the assignment of a constant value to a view attribute within the select list. The field that is called polarity indicates whether the polarity of PS.match is positive or negative (Notice the explicit assignment of a constant value to this attribute). create view PositiveSentimentsWithPolarity as select 'positive' as polarity, PS.match as sentiment from PositiveSentiments PS; create view NegativeSentimentsWithPolarity as select 'negative' as polarity, NS.match as sentiment from NegativeSentiments NS; Example 2: Explicit value assignment using function call The following example illustrates how the result of a function call is explicitly assigned to a view attribute within the select list. create view Citizenship as select P.Name as name, MatchesDict('USCities.dict', P.birthPlace) as isUSCitizen from Person P; Example 3: Select list expression from a dictionary extraction The following example illustrates how the select list expression can pick values of type Span from a result of dictionary extraction. create view PersonNames as select N.match as name from (extract dictionary 'firstNames.dict' on D.text as match from Document D )N; Example 4: Examples of case expressions This first example shows you how to specify null handling on specific fields: create view School as select case when Not(NotNull(P.education)) then 'Unknown' else GetString(P.education) as name from Person P; This example explains how to classify data: create view Company as select PM.name as productname, case when ContainsRegex (/IBM/,PM.name) then 'IBM' when ContainsDict ('OpenSourceDict',PM.name) then 'OSS' else 'Unknown' as name from ProductMatches PM; The from list {: #aql-from-list} The second part of a select or an extract statement in AQL is the from list. The from list is a comma-separated list that is the source of the tuples to be selected or extracted. Syntax {: #aql-from-list-syntax} from <from list item> <name> [, <from list item> <name>] Description {: #aql-from-list-desc} <from list item> A view, table, table function reference or nested AQL statement. All nested statements in AQL must be surrounded by parentheses. <name> Local name of the <from list item> , that is scoped within the select statement or the extract statement. A local name can be a simple identifier or a double-quoted identifier. Local names that contain spaces, punctuation characters, or AQL keywords must be enclosed in double quotation marks. Examples {: #aql-reference-examples-18} Example 1: A from list with a view and a nested statement This example shows a from list that references a view and a nested extract statement. The example assigns the result of the statement to the local name FN . The example also assigns the outputs of the LastName view to the local name Last Name . create dictionary LastNamesDict as ( 'Anthony', 'Lehman', 'Radcliff' ); create view LastName as extract dictionary 'LastNamesDict' on D.text as lastname from Document D; create view FromList as select * from (extract dictionary 'first.dict' on D.text as firstname from Document D) FN, LastName \"Last Name\" where Follows(FN.firstname, \"Last Name\".lastname, 0, 1); The following names are contained in the external dictionary first.dict: #Dictionary for given names Aaron Candra Freeman Mathew Matthew Zoraida The where clause {: #aql-where} The optional where clause defines a predicate to be applied on each tuple generated from the Cartesian product of all tuples in the relations in the from clause. Syntax {: #aql-where-syntax} select <select list> from <from list> [where <where clause>] Description {: #aql-where-desc} <where clause> Specifies one or more predicates. A join occurs when any predicate in a where clause involves fields from more than one view that belongs to the from list. This predicate must be a conjunction of a set of built-in predicate functions or other user-defined functions that return the data type Boolean: bash function1() and function2() and ... and functionn() The where clause is optional and can be omitted from a select statement if no predicates exist to apply. Examples {: #aql-reference-examples-19} Example 1: Filter out joined tuples by using a predicate in the WHERE clause This example shows a where clause that finds only phrases that consist of valid given names that are followed within 0-1 characters by valid surnames. -- a view containing words that are valid given names create view FirstName as extract dictionary 'first.dict' on D.text as firstname from Document D; -- a view containing words that are valid surnames create view LastName as extract dictionary 'last.dict' on D.text as lastname from Document D; -- a view containing phrases consisting of valid given names -- followed within 0-1 characters by valid surnames. create view FullName as select * from FirstName FN, LastName LN where Follows (FN.firstname, LN.lastname, 0, 1); The following names are contained in the external dictionary first.dict: #Dictionary for given names Aaron Candra Freeman Mathew Matthew Zoraida The following names are contained in the external dictionary last.dict: #Dictionary for surnames Anthony Lehman Radcliff The consolidate on clause {: #aql-con-on} The optional consolidate on clause specifies how overlapping spans are resolved across tuples that are output by a select or extract statement. Tuples with non-overlapping spans are not affected when this clause is used. Syntax {: #aql-con-on-syntax} The following code is an example of the general structure of this clause: consolidate on <target> [using '<policy>'[ with priority from <priority_column> [ <priority_order> ]]] Description {: #aql-con-on-desc} <target> Specifies a column in a view in the from clause, or an expression that is composed of scalar function calls involving columns of views that are specified in the from clause as arguments. '<policy>' Specifies one of the following consolidation policies that is supported by Text Analytics: ContainedWithin This policy is the default. If spans A and B overlap, and A completely contains B, this policy then removes the tuple that contains span B from the output. If A and B are the same, then it removes one of them. The choice of which tuple to remove is arbitrary. NotContainedWithin If spans A and B overlap, and A completely contains B, this policy then removes the span A from the output. If A and B are the same, then it removes one of them. The choice of which tuple to remove is arbitrary. ContainsButNotEqual This policy is the same as ContainedWithin, except that the spans that are exactly equal are retained. ExactMatch If a set of spans covers the same region of text, this policy returns exactly one of them. All other spans are left untouched. LeftToRight This policy processes the spans in order from left to right. When overlap occurs, it retains the leftmost, longest, non-overlapping span. This policy emulates the overlap-handling policy of most regular expression engines. <priority_column> Specifies a column of type Text, String, Integer, or Float. Can be specified only with the LeftToRight consolidation policy. <priority_order> Specifies either ascending or descending order. Can be specified only with the LeftToRight consolidation policy. The ascending order ensures that if a tuple T1 has priority 1 and a tuple T2 has priority 2, T1 has a higher priority than T2. By contrast, if the priority order is descending, T2 has a higher priority. The default value of the priority order is ascending. Usage notes {: #aql-con-on-usage} When the priority clause is present, the semantics of consolidation follow this order: Process spans from left to right, and when spans overlap, retain the leftmost spans. If you have multiple overlapping spans that start at the same offset, retain the ones with the highest priority according to the priority order. Break remaining ties by retaining the longest spans among those spans with the same priority. Consolidate treats nulls as identical. All inputs with a null <consolidate target> result in a single output tuple, which is chosen randomly among those inputs. This behavior is similar to how tuples are consolidated with an identical span in the target column. The exception to resulting in a single output tuple is if the policy is ContainsButNotEqual. In that case, the null <consolidate target> outputs all inputs with null consolidate target. Examples {: #aql-reference-examples-20} Example 1: Consolidate on single column This example directs the system to examine the field Person.name of all output tuples and to use the ContainedWithin consolidation policy to resolve overlap. consolidate on Person.name using 'ContainedWithin' Example 2: Consolidate on expression, involving multiple columns This example directs the system to examine the result of applying the CombineSpans scalar function to fields Person.firstname and Person.lastname in each output tuple. It resolves overlap by using the ContainedWithin consolidation policy. consolidate on CombineSpans(Person.firstname, Person.lastname) using 'ContainedWithin' Example 3: Consolidate by using LeftToRight policy and priority order Suppose that the following Term tuples are extracted from the input text John Doe: match: `John`, priority: `1` and match: `John Doe`, priority: `2` Both spans have the same begin offset. When you consolidate by using the LeftToRight policy for ascending priority order, the tuple ( match: John, priority: 1 ) is retained because it has the higher priority. When you consolidate by using the descending priority order, the tuple ( match: John Doe, priority: 2 ) is retained, as in the following example: create view ConsolidatePeopleWithPrioritiesAscending as select P.match as match, P.weight as weight from People P consolidate on P.match using 'LeftToRight' with priority from P.weight ascending; The group by clause {: #aql-group-by} The optional group by clause of a select statement directs the runtime component to group tuples that are produced from the same document by common values of a specified field. Syntax {: #aql-group-by-syntax} select <select list> from <from list> [where <where clause>] ... [group by <group by list>] Description {: #aql-group-by-desc} <group by list> Specifies a comma-delimited list of expressions that involves columns of the views in the from clause and scalar function calls. When you apply the group by clause, each group of tuples that shares common values for all group by expressions produces a single output tuple that is representative for the entire group. A field or expression that does not appear in the group by clause cannot appear in the select list, unless it is used in an aggregate function call. The order of expressions in the list does not matter. The group by clause treats all nulls as identical. Group by on a column with null values results in a single group. Examples {: #aql-reference-examples-21} Example 1: Computing aggregate values Use the group by clause to compute aggregate values. This example counts the number of occurrences of each given name in the document. In this example, Count is an aggregate function. create view SampleView as select GetText(P.firstname) as name, Count(GetText(P.firstname)) as occurrences from (extract dictionary 'first.dict' on D.text as firstname from Document D ) P group by GetText(P.firstname); In this case, first.dict is an external dictionary that contains the following entries: #Dictionary for given names Aaron Candra Freeman Matthew Zoraida The following steps describe semantics of this statement: Group tuples that are produced by the subquery in the from clause by the text content of their firstname field. For each group, count the number of tuples with a non-null firstname value. Produce a single output tuple for each such group, with two values, the name and the number of tuples in that group. Example 2: Issues with grouping dissimilar fields This example illustrates a statement that is not valid. select GetText(P.firstname) as first, GetText(P.lastname) as last, Count(P.firstname) as occurrences from Person P group by GetText(P.firstname); The inclusion of GetText(P.lastname) in the select list is not accepted, since tuples with the same firstname values might have different lastname values, which leads to ambiguity. The order by clause {: #aql-order-by} The optional order by clause directs the runtime component to order the output tuples that are produced by the select statement from each document based on the values of the order by list, which is a comma-delimited set of expressions Syntax {: #aql-order-by-syntax} select ... [order by <order by list>] Description {: #aql-order-by-desc} <order by list> Specifies a comma-delimited list of expressions. The order is based on the values of a comma-delimited list of expressions. The order by clause supports expressions that return numeric (Integer or Float), Text, or Span data types. If an expression in the order by clause returns a type Span, the result tuples are compared by comparing the relevant span values. In the following example, the span values of the person field are compared. ```bash order by P.person ``` The order by clause treats nulls as unordered (amongst themselves). Nulls are ordered lower than other objects. Examples {: #aql-reference-examples-22} Example 1: Order by multiple expressions Assume that person is a field of type Span. The following order by clause specifies that the statement returns tuples within each document. They are ordered lexicographically by the text of the person field and then by the beginning of the person field. order by GetText(P.person), GetBegin(P.person) The limit clause {: #aql-limit} The optional limit clause specifies a limit on the number of output tuples that are produced by the select statement for a document. Syntax {: #aql-limit-syntax} select <select list> from <from list> ... [limit <maximum number of output tuples for each document>]; Description {: #aql-limit-desc} <maximum number of output tuples for each document> Specifies the maximum number of output tuples for each document. If the limit value is greater than or equal to the total number of tuples that could be returned, all of the tuples are returned. Examples {: #aql-reference-examples-23} Example 1: Limiting the number of returns This example returns the first three person names in each document: create view SampleView as select * from Person P order by GetBegin(P.name) limit 3; The select... into statement {: #aql-select-into} The select ... into statement is useful for defining a view and specifying that it is an output view in a single statement. Syntax {: #aql-select-into-syntax} select <select list> into <output view name> from <from list> [where <where clause>] [consolidate on <column> [using '<policy>' [with priority from <column> [priority order]]]] [group by <group by list>] [order by <order by list>] [limit <maximum number of output tuples for each document>]; Description {: #aql-select-into-desc} <output view name> Specifies the name of the output view that is defined by the statement. The select ... into statement is identical to the select statement, except for the additional into <output view name> clause. Examples {: #aql-reference-examples-24} Example 1: Defining a view This example defines a view that is called PersonPhone and also specifies this view as an output view. select P.name as name, Ph.number as phoneNumber into PersonPhone from Person P, Phone Ph; This example is equivalent to the following two statements: create view PersonPhone as select P.name as name, Ph.number as phoneNumber from Person P, Phone Ph; output view PersonPhone; The detag statement {: #aql-detag} The detag statement in AQL provides the function to detag or remove all the markup from HTML or XML documents before you run AQL extractors. The detag statement can also retain the original locations of tags and any values that are stored in these tags. When a detag statement removes tags from a document, the runtime component remembers the mapping between offsets from the detagged text and the original markup source. The Remap function, a special built-in function that maps spans from the detagged text back to their equivalent spans from the original source. Syntax {: #aql-detag-syntax} detag <input view name>.<text column> as <output view name> [detect content_type (always|never)] [annotate element '<element name>' as <auxiliary view name> [with attribute '<attribute name>' as <column name>] [and attribute '<attribute name>' as <column name>] [, element ...]]; Description {: #aql-detag-desc} <input view name>.<text column> <input view name> Specifies the name of the input view on which to do the detag process. The <input view name> can be a simple identifier or a double-quoted identifier. <text column> Specifies the text field of the input view on which to do the detag process. The <text column> can be a simple identifier or a double-quoted identifier. <output view name> Specifies the name of the output view that contains the detagged text. The output view contains a single column that is called text , which holds the detagged text. The <output view name> can be a simple identifier or a double-quoted identifier. always|never Specifies whether to verify that the contents are HTML or XML before the detag statement is processed. When you run non-HTML and non-XML text through a detagger, problems can occur if the text contains XML special characters such as < , > , or & . If the detect content_type clause is missing, the default value is always and the system always detects content. always Specifies that verification always occurs before the operation is attempted to avoid problems with parsing documents that are not HTML or XML. If the value of the <text column> does not appear to contain a markup, the system skips detagging for the current document. never Specifies that verification never occurs before the detag operation is attempted. The system attempts to detag the target text, even if the text does not contain any HTML or XML content. <element name> Specifies the name of the HTML or XML element to be annotated. The optional annotate clause can direct the runtime component to remember information about the removed tags by creating one or more views. <auxiliary view name> Specifies the name of the view that is created to hold the original tags and their attributes. Can be a simple identifier or a double-quoted identifier. <attribute name> Name of an attribute of the HTML or XML element. <column name> The name of the column in the <auxiliary view name> that is used to store the values of <attribute name> . Can be a simple identifier or a double-quoted identifier. Examples {: #aql-reference-examples-25} Example 1: Specifying the detag output view and an auxiliary view In this example, the DetaggedDoc view is created to hold the detagged version of the original text in the text attribute of the view Document . In addition to creating a DetaggedDoc view, the annotate clause creates an auxiliary view called Anchor . This auxiliary view has two columns. One column, called match , contains the anchor text. The other column, called linkTarget , contains the actual target of the link as text. The spans in the match column are over the text value of the DetaggedDoc view. detag Document.text as DetaggedDoc annotate 'a' as Anchor with attribute 'href' as linkTarget; Example 2: Using the Remap function The following example illustrates how the Remap function is used to map spans from the detagged text back to their equivalents in the original source. -- Strip out tags from each document, provided that the document -- is in HTML or XML format. -- Remember the locations and content of all <A> and <META> tags -- in the original source document. detag Document.text as DetaggedDoc detect content_type always annotate element 'a' as Anchor with attribute 'href' as target, element 'meta' as Meta with attribute 'name' as name and attribute 'content' as content; output view DetaggedDoc; -- Create a view containing all lists of keywords in the -- document META tags. create view MetaKeywordsLists as select M.content as list from Meta M where MatchesRegex(/keywords/, 'CASE_INSENSITIVE', M.name) and NotNull(M.content); -- Create a dictionary of \"interesting\" web sites create dictionary InterestingSitesDict as ( 'ibm.com', 'slashdot.org' ); -- Create a view containing all anchor tags whose targets contain -- a match of the \"interesting sites\" dictionary. create view InterestingLinks as select A.match as anchortext, A.target as href from Anchor A where ContainsDict('InterestingSitesDict', A.target); -- Find all capitalized words in the anchor text of links to -- \"interesting\" web sites. create view InterestingWords as extract I.href as href, regex /[A-Z][a-z]+/ on 1 token in I.anchortext as word from InterestingLinks I; -- Map spans in the InterestingWords view back to the original -- HTML or XML source of the document. create view InterestingWordsHTML as select I.href as href, Remap(I.word) as word from InterestingWords I; Documenting the detag statement with AQL Doc The AQL Doc comment for a detag statement contains the following information: General description about the function of the statement. @field for each text field of the input view on which to do the detag process. @auxView specifies the view name. @auxViewField specifies the fully qualified column name of the view. /** * Detags the input document * @field text the detagged text of the document * @auxView Anchor stores the anchor points from tagged doc * @auxViewField Anchor.linkTarget stores the href attribute of anchor tag */ detag Document.text as DetaggedDoc annotate element 'a' as Anchor with attribute 'href' as linkTarget; The create dictionary and create external dictionary statements {: #aql-create-dict} The create dictionary and create external dictionary statements are used to define dictionaries of words or phrases to identify matching terms across input text through extract statements or predicate functions. The create dictionary statement allows the specification of dictionary content in source AQL code, and the dictionary content is serialized inside the compiled representation of the module (the .tam file). The create external dictionary statement allows you to specify the dictionary content when the extractor is instantiated, instead of in source AQL code, and you do not have to recompile the module. Therefore, external dictionaries are powerful constructs that allow the AQL developer to expose customization points in a compiled module. Dictionaries can be created from three sources: Dictionary files Inline dictionary declarations Tables that are created with the create table statement and the create external table statement. Syntax {: #aql-create-dict-syntax} The internal create dictionary statement has three syntactical forms, from file , from table , and an inline format. Internal dictionary From file: ```bash create dictionary from file ' ' [with language as ' '] [and case (exact | insensitive)] [and lemma_match]; ``` From table: ```bash create dictionary from table with entries from [and language as ' '] [and case (exact | insensitive)] [and lemma_match]; ``` Inline format bash create dictionary <dictionary name> [with language as '<language code(s)>'] [and case (exact | insensitive)] [and lemma_match] as ( '<entry 1>', '<entry 2>', ... , '<entry n>' ) ; External dictionary bash create external dictionary <dictionary-name> required [true|false] [with language as '<language codes>'] [and case (exact | insensitive )] [and lemma_match]; Description {: #aql-create-dict-desc} <dictionary name> Specifies a name for the new internal or external dictionary. Can be a simple identifier or double-quoted identifier. '<file name>' Specifies the name of the file that contains dictionary entries. Dictionary files are carriage-return-delimited text files with one dictionary entry per line. Entries in a dictionary file can consist of multiple tokens. <table name> Specifies the name of the table from which to add dictionary entries. Dictionaries cannot be created from a table that is imported from another module. <column name> Specifies the name of the column in the table from which to add dictionary entries. required [true|false] Specifies whether external content for the external dictionary is required to run the module. true If the clause is required true , you must supply a URI to the location of the file that contains the external content. The specified file must contain content. If the URI is not supplied, or if the file does not contain content, the runtime component throws an exception. false If the clause is required false , the module can be run successfully even if a URI to the external content is not supplied for this dictionary. If a URI is not supplied, the runtime component treats it as an empty dictionary. The use of create external dictionary <dictionary-name> allow_empty is now deprecated and results in a compiler warning. {: deprecated} '<language code(s)>' Specifies a comma-delimited list of two-letter language codes, such as en (English) or zh (Chinese) for the languages, or for the document languages for external dictionaries, on which to evaluate the dictionary. The dictionary produces no results on documents whose language code is not contained in this string. If the language parameter is omitted, the dictionary language defaults to one of the following language sets: The language sets that are specified through the set default language statement, if it is declared, in the containing module. The language sets that contain German (de), Spanish (es), English (en), French (fr), Italian (it), and the unspecified language (x_unspecified). lemma_match Use lemmatization to find matches for similar words to a dictionary term in your documents. Lemmatization is the process of determining the lemma for a given word. A lemma is a word that can be used as a match for a single given term. For example, the term \u201cgo\u201d can be matched to the terms \u201cgoes\u201d, \u201cgoing\u201d, \u201cgone\u201d, or \u201cwent\u201d. This process involves complex tasks such as understanding context and determining the part of speech of a word in a sentence. Lemmatization is not provided by the built-in tokenizer. Lemma match is performed only for dictionaries declared with the lemma match clause. The semantics for dictionary extraction with the lemma_match clause are as follows: The lemmatized form for each token of the input document is computed. The dictionary is evaluated against the lemmatized document. You cannot use the lemma_match option with the case exact option. If both are used, a compiler error is returned. {: important} case (exact | insensitive) Specifies the type of case-folding that the dictionary performs when it determines whether a specific region of the document matches. exact Specifies an exact case-sensitive match. insensitive Specifies a match that is not case-sensitive match. This option is the default value. '<entry 1>', '<entry 2>', ... , '<entry n>' Specifies the strings that you want to include in the inline dictionary. Entries in an inline dictionary can consist of one or more tokens. Usage notes {: #aql-create-dict-usage} The from file and from table formats are recommended, especially when you anticipate modifying the entries, or when you have many entries. By using these formats, you can modify the contents of the dictionary without modifying the code. When the create dictionary statement is processed by the modular AQL compiler, references to dictionary file locations that are specified in the create dictionary ... from file syntax must be relative to the root of the module in which this create dictionary statement is issued. You can specify comments in a dictionary file by preceding the comment with the character # . Comments can start anywhere in a line. If you want to specify comments that span multiple lines, you must precede each line with the comment character. If the comment character is part of a dictionary entry, you must escape it using the backslash character (\\), as in \\# . If the backslash character is part of the dictionary entry, it must be escaped with itself, as in \\\\ . For external dictionaries, when modules are being loaded, you must specify a list of URIs to external dictionaries, as required by the modules that are being loaded. Lemmatization of dictionaries : The primary difference between the existing dictionary matching semantics and the lemmatized semantics is that the match is performed against the lemmatized form of the document, instead of the original form of the document. Dictionary entries that belong to a dictionary with lemma match enabled have these prerequisites: A dictionary entry can contain one or more tokens, where each entry token is a lemma. To create a dictionary of lemmas, you can use the [GetLemma scalar function. The tokens of a dictionary entry should be separated by a white space. If the token consists of white spaces, then the white spaces should be escaped by using the backslash (\\) character. The following table shows the differences between the create external dictionary statement and the create dictionary statement: create external dictionary create dictionary Defines a placeholder for a dictionary whose content is supplied at initialization time. Requires that the content of the dictionary is available at compile time. Serialized in the compiled representation (.tam) of a module. Examples {: #aql-reference-examples-26} Example 1: Creating an external dictionary The external dictionary, PersonPositiveClues, expects to be populated with values from an external file at load-time. It also expects to be matched against some Western languages as specified by its flags. module PersonModuleEnglish; create external dictionary PersonPositiveClues allow_empty false with case exact; export dictionary PersonPositiveClues; Example 2: Lemmatization Consider a dictionary that has lemma match enabled and contains two entries: go shop and went shopping. The document contains the text Anna went shopping . The lemmatized form of the input document is Anna go shop . Lemma match returns went shopping as a match for the entry go shop. The original document text is not compared to the dictionary entries, only the lemmatized document text. Therefore, no matches exist in the document for the entry went shopping . Documenting the create dictionary and create external dictionary statements with AQL Doc The AQL Doc comment for a create dictionary statement contains the following information: General description about the dictionary. /** * A dictionary of terms used to greet people. * */ create dictionary GreetingDict as ( 'regards', 'regds', 'hello', 'hi', 'thanks', 'best', 'subj', 'to', 'from' ); The AQL Doc comment for a create external dictionary statement contains the general description about the dictionary that is created. The following string illustrates the format: /** * Customizable dictionary of given names. * Evaluated on English, French, Italian, German, Portuguese, Spanish text. */ create external dictionary CustomFirstNames_WesternEurope allow_empty true; The create table statement {: #aql-create-table} The create table statement creates an AQL table. The create table statement in AQL is used to define static lookup tables to augment annotations with more information. Syntax {: #aql-cr-tab-syntax} create table <table name> ( <colname> <type> [, <colname> <type>]* ) as values ( <value> [, <value>]*), ... ( <value> [, <value>]*); Description {: #aql-cr-tab-desc} <table name> Specifies the name of the table to create. The <table name> can be a simple identifier or a double-quoted identifier. <colname> Specifies the name of the column to create. <type> Specifies the AQL data type for the associated column. All columns must be of type Text , Integer, Float or Boolean. <value> Specifies the tuples to be populated in the created table. Examples {: #aql-reference-examples-27} Example 1: Creating a table of company names In this example, the create table statement adds more location metadata to company name annotations: -- Create a dictionary of company names create dictionary CompanyNames as ('IBM', 'BigCorp', 'Initech'); -- Find all matches of the company names dictionary. create view Company as extract dictionary 'CompanyNames' on D.text as company from Document D; -- Create a table that maps company names to locations of -- corporate headquarters. create table NameToLocation (name Text, location Text) as values ('IBM', 'USA'), ('BigCorp', 'Apex'), ('Initech', 'Dallas'), ('Acme Fake Company Names', 'Somewhere'); -- Use the table to augment the Company view with location -- information. create view CompanyLoc as select N2C.location as loc, C.company as company from Company C, NameToLocation N2C where Equals(GetText(C.company), GetText(N2C.name)); output view CompanyLoc; Documenting the create table statement with AQL Doc The AQL Doc comment for a create table statement contains the following information: General description about the table. @field for every column name in the schema of this table. /** Create a table that maps company names to locations /** of corporate headquarters. * @field name name of the company * @field location location of corporate headquarters */ create table NameToLocation (name Text, location Text) as values ('IBM', 'USA'), ('Enron', 'UK'), ('Initech', 'Dallas'), ('Acme Fake Company Names', 'Somewhere'); The create external table statement {: #aql-create-ext-table} You can use the create external table statement to specify a table with established content when a compiled module is run across all input documents. You supply the table content during load time instead of in the source AQL code, and you do not have to recompile the module. External tables are powerful constructs that allow the AQL developer to expose customization points in a compiled module. Syntax {: #aql-cr-ext-tab-syntax} create external table <table-name> (<colname> <type> [, <colname> <type>]* ) allow_empty <true|false>; Description {: #aql-cr-ext-tab-desc} <table-name> Specifies name of the external table to create. The <table-name> can be a simple identifier or a double-quoted identifier. <colname> Specifies the name of the column to create. <type> Specifies the AQL data type for the associated column. All columns must be of type Text , Integer, Float or Boolean. [, <colname> <type>]* Specifies additional columns and AQL objects to be used in the external table. allow_empty [true|false] Specifies the value for the allow_empty clause. true If the clause is allow_empty true , the module can be run successfully even if a URI to the external content is not supplied for this table. If the URI is not supplied, the runtime component treats it as an empty table. false If the clause is allow_empty false , then you must supply a URI to the location of the file that contains the external content. The specified file must contain content. If the URI is not supplied, or if the file does not contain content, the runtime component throws an exception. Usage notes {: #aql-cr-ext-tab-usage} The compiled representation of the module contains metadata about the external objects (views, dictionaries, and tables) that the module defines. When modules are being loaded, you must specify a list of URIs to external tables, as required by the modules that are being loaded. The supported format for the content of an external table is one CSV (.csv) file with header. The following table shows the differences between the create external table statement and the create table statement: create external table create table Defines a placeholder for a table whose content is supplied at initialization time. Requires that the content of the table is available at compile time. Serialized in the compiled representation (.tam) of a module. Examples {: #aql-reference-examples-28} Example 1: Creating an external table that is populated at load time The external table, PersonNegativeClues, expects to be populated at load-time because of the flag, allow_empty false . module PersonModuleFrench; create external table PersonNegativeClues (name Text) allow_empty false; export table PersonNegativeClues; Example 2: Creating a dictionary with an external table Dictionaries can also be created from external tables, similar to being created from inline tables that are declared with the create table statement. create external table Product (nickName Text, formalName Text) allow_empty false; /** * Dictionary of product nicknames, from the nickName field * of the customizable external table Product. */ create dictionary ProductDict from table Product with entries from nickName; Documenting the create external table statement with AQL Doc The AQL Doc comment for a create external table statement contains the following information: General description about the table. @field for every column name in the schema of this table. /** Create a table that maps company names to locations of corporate headquarters. * @field name name of the company * @field location location of corporate headquarters */ create external table Company2Location (name Text, location Text) allow_empty false; The create external view statement {: #aql-create-ext-view} The create external view statement in AQL allows specification of more metadata about a document as a new view, in addition to the predefined Document view that holds the textual and label content. Syntax {: #aql-cr-x-view-syntax} create external view <view_name> ( <colname> <type> [, <colname> <type>]* ) external_name '<view_external_name>'; Description {: #aql-cr-x-view-desc} <view_name> Specifies the internal name of the external view. The external view is referred by this name in the AQL rules. A <view_name> can be a simple identifier or a double-quoted identifier. A <view_name> cannot contain the period character. <colname> Specifies the name of the column to define in the external view. <type> Specifies the data type for the associated column. The data types that are supported for external view columns are Text, Span, Integer, and Float. '<view_external_name>' Specifies the external name of the external view. External systems that populate tuples into the external view refer to the external view by the external name. The '<view_external_name>' must be a String constant that is encased in single quotation marks ('ExternalName'). Examples {: #aql-reference-examples-29} To illustrate external views, consider an example application that requires that you identify the names of persons in email messages. Example 1: Identifying the names of persons in email messages Assume that the text of an email message is \"Ena, please send me the document ASAP\". While a human might be able to understand that Ena is a name of a person based on the text of the email, AQL rules that are written to identify names of persons with high precision from general text might be too conservative and not able to draw the same conclusion with high confidence, based on the fact that Ena is a capitalized word. One way to boost the coverage of the rules is to use words from the From , To , and CC fields of the email as more evidence. If the email is addressed to \"Ena Smith,\" and the application makes this information available to the extractor, then the extractor developer can write more AQL rules to boost the coverage of the extractor that is based on the domain knowledge that emails are usually addressed to people. For example, you can write AQL rules to identify person tokens from the email metadata fields. You then use that information as strong clues when you decide whether a capitalized token in the email text is a person name. In general, the email metadata is not part of the actual email message, but the application can make this metadata available to the extractor by using an external view. For each email that must be processed, the application can pass the email text as document text (to populate the view Document ) at run time. It can also pass the extra metadata by using an appropriately defined external view. The following statement defines an external view that is named EmailMetadata . The external view has a schema that contains three fields of type Text. At run time, the view EmailMetadata is automatically populated from an external type named EmailMetadataSrc . You can then reference the EmailMetadata view in your AQL rules, similarly to how you would reference any other view. create external view EmailMetadata (fromAddress Text, toAddress Text, ccAddress Text) external_name 'EmailMetadataSrc'; Documenting the create external view statement with AQL Doc The AQL Doc comment for a create external view statement contains the following information: General description about the view. @field for every column name in the view. /** * The external view named EmailMetadata, containing three fields * of type Text. At run time, the view EmailMetadata is * automatically populated from an external type named * EmailMetadataSrc. * * @field from the fromAddress field of the email * @field to the toAddress field of the email * @field cc the ccAddress field of the email */ create external view EmailMetadata(fromAddress Text, toAddress Text, ccAddress Text) external_name 'EmailMetadataSrc'; File formats for external artifacts {: #aql-format-ext} Three types of external artifacts are supported: external views, external dictionaries, and external tables. External dictionary The format for the file that contains entries for an external dictionary is defined here: Carriage-return-delimited text file. One dictionary entry per line. Recommended file extension is .dict, but other file extensions can be supported. Entries in the dictionary can consist of multiple tokens. Comments can be specified by preceding the content of the comment with the character, # . Comments can start anywhere in a line. Multi-line comments must contain the # character at the start of each line. Dictionary entries can contain comment characters, if each comment character is escaped with a backslash character. For example, \\# . External table The supported file format for the content of an external table is a .csv file with header. The following example shows a create external table statement, and the .csv file that specifies the content of this external table. create external table Company2Location (name Text, location Text) allow_empty false; The first line of the .csv file contains the header. The remaining lines contain the data. name,location IBM,USA Infosys,India LG,Korea Vodafone,UK External view The content of external views can be specified in the following ways: When you run an extractor, you can specify external view content for a data collection only when you are using the JSON input format. Built-in functions {: #aql-inc-func} AQL has a collection of built-in functions for use in extraction rules. Aggregate functions Aggregate functions are used to implement operations (such as counting, mathematical operations, and other operations) across a set of input values. These functions return only one result. Predicate functions Predicate functions test a certain predicate over its input arguments and return a corresponding Boolean value. Scalar functions Scalar functions perform an operation over the values of a field across a set of input tuples and return a non-Boolean value, such as a Span, Text, or Integer. These functions can be used within the select list of a select statement or an extract statement. They can also be used as inputs to predicate functions. Aggregate functions {: #aql-aggr-func} Aggregate functions are used to implement operations (such as counting, mathematical operations, and other operations) across a set of input values. These functions return only one result. These functions can be used within the select list of a select statement but not within an extract statement. The following example is the general form of an aggregate function call: Aggregate_Function_Name(argument) The argument can be: An expression that consists of a column of a view in the from clause, or a combination of scalar functions that involve columns of the views in the from clause. In most cases, except as noted, null values for argument are ignored. The character * in the special case of the Count(*) aggregate function. In this case, all rows of the output are counted, including null values. Aggregate function Argument type Return type Return value Avg(expression) Integer, Float Float The average of all input values or null if no rows are selected Count(*) Integer The number of all input rows Count(expression) Any Integer The number of all non-null input values List(expression) Integer, Float, Text, Span List of scalar values of the same type as the input argument An unordered list of non-null input values: a bag, not a set, hence might contain duplicates. An empty list if only null values are selected Max(expression) Integer, Float, Text, Span Same as the argument type The maximum element across all input values or null if no rows are selected Min(expression) Integer, Float, Text, Span Same as the argument type The minimum element across all input values or null if no rows are selected Sum(expression) Integer, Float Same as the argument type The sum of all input values or null if no rows are selected Limitations of the current version: The current version of AQL supports the creation of scalar values through the aggregate function List. Examples {: #aql-reference-examples-30} The following example illustrates how aggregate functions can count the number of person name annotations, or compute sets of given names that are associated with each distinct surname that is identified in a document: -- identify occurrences of given names in the document create view FirstName as extract dictionary 'firstNames.dict' on D.text as name from Document D; -- identify occurrences of surnames in the document create view LastName as extract dictionary 'lastNames.dict' on D.text as name from Document D; -- identify complete person names in the document create view Person as select F.name as firstname, L.name as lastname from FirstName F, LastName L where FollowsTok(F.name, L.name, 0, 0); -- count the number of person annotations in the document create view CountPerson as select Count(*) from Person; -- for each distinct surname, output a list of given names associated with it in the document create view FamilyMembers as select GetText(P.lastname) as lastname, List(GetText(P.firstname)) as firstnames from Person P group by GetText(P.lastname); The following example illustrates the use of the Min and Max functions: -- Extract stop words from input text create view StopWords as extract regex /\\s(the|in|a|an|as|to|from)\\s/ on D.text as match from Document D; -- Count the number of times each stop word matched above, was used in the text create view StopWordsCount as select GetText(S.match) as stopword, Count(S.match) as stopwordcount from StopWords S group by GetText(S.match); -- Retrieve the most used and least used stop word count create view StopWordUsageCount as select Min(S.stopwordcount) as least, Max(S.stopwordcount) as most from StopWordsCount S; Predicate functions {: #aql-pred-func} Predicate functions test a certain predicate over its input arguments and return a corresponding Boolean value. Input arguments to predicate functions include return values of other scalar or aggregate functions in addition to dictionaries, regular expressions, and more. These functions can be employed within the where clause of a select statement, and the having clause of an extract statement. And The And function accepts a variable number of Boolean arguments and returns the results of a logical AND operation across all the input arguments. The AQL optimizer does not attempt to optimize the order of evaluation of the arguments to this function as part of the logical AND operation. If any input is null , the result is null . Consider this query format: select ... from ... where And(predicate1, predicate2); As a result, a query format that uses the AND operation often runs considerably slower than the same query in the form of: select ... from ... where predicate1 and predicate2; When possible, use the SQL-style and keyword instead of this function. Contains The Contains function takes two spans as arguments: Contains(<span1>, <span2>) This function returns TRUE if span1 completely contains span2 . If span2 starts at or after the beginning of span1 , and ends at or before the end of span1 , span2 is completely contained. If either argument is null , the function returns null . ContainsDict The ContainsDict function checks if the text of a span contains any entry from a given dictionary. This function accepts as input arguments a dictionary, an optional flag specification, and a span to evaluate. ContainsDict('<dictionary>', ['<flags>', ]<span>) The ContainsDict function returns TRUE if the span contains one or more matches of the dictionary. The flags can be Exact or IgnoreCase . If Exact is used, a case-sensitive match is performed against each term in the dictionary. If IgnoreCase is used, the match that is performed against each term in the dictionary is not case-sensitive. If no flag is specified, the dictionary matches based on any flag that was specified when it was created. If no flag was specified during creation, it matches by using the IgnoreCase flag. If the span is null , the function returns null . The following example illustrates the use of the ContainsDict function: create dictionary EmployeePhoneDict as ( '121-222-2346', '121-234-1198', '121-235-8891' ); create view PhoneNum as extract regex /(\\d{3})-(\\d{3}-\\d{4})/ on between 4 and 5 tokens in D.text return group 1 as areaCode and group 2 as restOfNumber and group 0 as number from Document D; create view PhoneNumbers as select P.number as number from PhoneNum P where ContainsDict('EmployeePhoneDict',P.number); Dictionaries are always evaluated on token boundaries. For example, if a dictionary consists of the term fish, no match exists in the text Let's go fishing!. ContainsDicts The ContainsDicts function checks if the text of a span contains any entry from any given dictionaries. This function accepts as input arguments two or more dictionaries, an optional flag specification, and a span to evaluate. ContainsDicts('<dictionary>','<dictionary>','<dictionary>', ['<flags>', ]<span>) The ContainsDicts function returns TRUE if the span contains one or more matches from at least one of the specified dictionaries. The flags can be Exact or IgnoreCase . If Exact is used, a case-sensitive match is performed against each of the terms in the dictionaries. If IgnoreCase is used, the match that is performed against each of the terms in the dictionaries is not case-sensitive. If no flag is specified, the dictionary matches based on any flag that was specified when it was created. If no flag was specified during creation, it matches by using the IgnoreCase flag. If either or both arguments are null, then the function returns null . The following example illustrates the use of the ContainsDicts function with the Exact flag: create view PersonWithFirstName as select P.reference as reference from Person P where ContainsDicts( 'FirstNamesUsedGlobally', 'FirstNamesUsedInGermanyLong', 'NickNamesUsedGlobally', 'FirstNamesUsedInGermanyShort', 'FirstNamesUsedInItaly', 'FirstNamesUsedInFrance', 'Exact', P.reference); ContainsRegex The ContainsRegex function checks whether the text of a span matches a given regular expression. This function accepts a regular expression with which to match, an optional flag specification, and the input span against which to match. ContainsRegex(/<regular expression>/, ['<flags>', ]<span>) The function returns TRUE if the text of the span, which is taken as a separate Java\u2122 string, contains one or more matches of the regular expression. The function returns null if the span is null . The optional flags affect the matching behavior, similarly to flags used in Java regular expressions. The flags string is formed by combining one or more of the following flags by using | as the separator: CANON_EQ CASE_INSENSITIVE DOTALL LITERAL MULTILINE UNICODE (meaningless without CASE_INSENSITIVE) UNIX_LINES An example of a flags string is 'UNICODE | CASE_INSENSITIVE' Consider this example where ContainsRegex identifies product names along with their version number mentions on either sides. Unlike the example for MatchesRegex , a version number match is not strictly identified by using the regex , but by the context around a product name mention containing a token that matches against the regex . -- dictionary of product names create dictionary ProductNamesDict as ( 'IBM WebSphere Application Server', 'Microsoft Windows', 'Apple Mac OS', 'IBM Rational Application Developer', 'Apache HTTP Server', 'Eclipse', 'Google Android' ); -- extract product names from input text create view ProductNames as extract dictionary 'ProductNamesDict' on D.text as name from Document D; -- gather context around product name mention create view ProductNamesWithContext as select P.name as name, LeftContext(P.name, 5) as leftctxt, RightContext(P.name, 5) as rightctxt from ProductNames P; -- use a regex to identify products with version number mentions on either sides of the product mention create view ProductsWithVersionNumbers as ( select P.name as productname, P.leftctxt as productversion from ProductNamesWithContext P where ContainsRegex (/v\\d((\\.\\d)+)?/, P.leftctxt) ) union all ( select P.name as productname, P.rightctxt as productversion from ProductNamesWithContext P where ContainsRegex (/v\\d((\\.\\d)+)?/, P.rightctxt) ); Equals The Equals function takes two arguments of arbitrary type: Equals(<arg1>, <arg2>) Two spans are considered equal if they both begin and end at the same offsets and contain the same text. If either or both arguments are null, the function returns null . The following example illustrates the use of the Equals function. -- Select phone number spans whose text is equal to 001-543-2217 create view PhoneNumber as select P.number as number from PhoneNum P where Equals('001-543-2217',GetText(P.number)); Follows The Follows predicate function takes two span arguments and two integer arguments: Follows(<span1>, <span2>, <minchar>, <maxchar>) The function returns TRUE if the number of characters between the end of span1 and the beginning of span2 is between minchar and maxchar , inclusive. If any argument is null , the function returns null . FollowsTok The FollowsTok predicate function is a version of Follows ; however, the FollowsTok distance arguments are in terms of tokens instead of characters: FollowsTok(<span1>, <span2>, <mintok>, <maxtok>) The FollowsTok function returns TRUE if the number of tokens between the end of span1 and the beginning of span2 is between mintok and maxtok , inclusive. If any argument is null , the function returns null . GreaterThan The GreaterThan predicate function takes two arguments of arbitrary type: GreaterThan(<arg1>, <arg2>) The function returns TRUE if <arg1> is greater than <arg2> . The function returns FALSE if either argument is null . IsNull The IsNull function tests whether data is, or is not, null. It takes a single argument of any type and returns TRUE if the single argument is null , and FALSE otherwise. The behavior of this predicate and the already defined NotNull predicate is different from all other predicates that return null on null input. MatchesDict The MatchesDict function takes a dictionary (as in a dictionary extraction), an optional flag specification, and a span as arguments: MatchesDict('<dictionary>', ['<flags>', ]<span>) The MatchesDict function returns TRUE if the span exactly matches one or more of the terms in the dictionary. The flags can be Exact or IgnoreCase . If Exact is used, a case-sensitive match is performed against each term in the dictionary. If IgnoreCase is used, the match that is performed against each term in the dictionary is not case-sensitive. If no flag is specified, the dictionary matches based on any flag that was specified when it was created. If no flag was specified during creation, it matches by using the IgnoreCase flag. If any argument is null , the function returns null . Dictionaries are always evaluated on token boundaries. For example, if a dictionary consists of the term fish, no match exists in the text Let's go fishing!. MatchesRegex The MatchesRegex function has a similar syntax to ContainsRegex . Unlike ContainsRegex function, the MatchesRegex function returns TRUE only if the entire text of the span, which is taken as a separate Java string, matches the regular expression. If any argument is null , the function returns null . The optional flags affect the matching behavior similar to flags used in Java regular expressions. MatchesRegex(/<regular expression>/, ['<flags>', ]<span>) The flags string is formed by combining a subset of these flags by using | as the separator: CANON_EQ CASE_INSENSITIVE DOTALL LITERAL MULTILINE UNICODE (meaningless without CASE_INSENSITIVE) UNIX_LINES An example of a flags string is 'UNICODE | CASE_INSENSITIVE' Consider this example where MatchesRegex is used to identify product names along with their version number mentions to the right. Unlike the example in ContainsRegex section, the exact version number is identified as the token immediately following the product name mention. -- gather right context around product name mention create view ProductNamesWithContext as select P.name as name, RightContext(P.name, 5) as rightctxt from ProductNames P; -- use a regex to identify products with version number mentions to the right create view ProductsWithVersionNumbers as select P.name as productname, P.rightctxt as productversion from ProductNamesWithContext P where MatchesRegex (/v\\d((\\.\\d)+)?/, P.rightctxt); Not The Not function takes a single Boolean argument and returns its complement. If the argument is null , the function returns null . NotNull The NotNull function takes a single argument of any type. As its name suggests, the NotNull function returns TRUE if the value of the argument is not null, and FALSE if the argument is null . Or The Or function takes a variable number of non-null Boolean arguments. If any argument is null , the function returns null . The Or function returns TRUE if any of them evaluates to TRUE . Overlaps The Overlaps function takes two span arguments: Overlaps(<span1>, <span2>) The function returns TRUE if the two input spans overlap in the document text. The function returns null if either argument is null . Scalar functions {: #aql-scalar-func} Scalar functions perform an operation over the values of a field across a set of input tuples and return a non-Boolean value, such as a Span, Text, or Integer. These functions can be used within the select list of a select statement or an extract statement. They can also be used as inputs to predicate functions. If a Text object is provided where a Span object is required, a converted Span object is automatically generated, which is based on this Text object, with begin and end offsets covering the whole length of the Text object. If a Span object is provided where a Text object is required, a converted Text object is automatically generated from the text value of the Span object. Chomp The Chomp function is similar to the Chomp operator in Perl, except that Chomp operates over spans instead of strings: Chomp(<span1>) The following example illustrates the use of the Chomp function. detag Document.text as DetaggedDoc annotate element 'a' as Anchor with attribute 'href' as linkTarget; create view Links as select Chomp(A.linkTarget) as link from Anchor A; If the input span contains any white space at the beginning or end, the Chomp function shrinks the span enough to remove the white space. The function then returns a new span with no leading or trailing white space. If the input span has no leading or trailing white space, then the Chomp function returns the same span. If the input span is null , then Chomp returns null . CombineSpans The CombineSpans function takes two spans as input and returns the shortest span that completely covers both input spans if the spans are based on the same text object. CombineSpans(['IgnoreOrder',] <span1>, <span2>) The CombineSpans function is sensitive to the order of its input spans, unless you use the IgnoreOrder parameter. When the optional IgnoreOrder parameter is used, the order of the two spans is ignored. The following example illustrates the use of the CombineSpans function. create view FullName as select CombineSpans('IgnoreOrder',F.name, L.name) as fullName from FirstName F, LastName L where FollowsTok(F.name, L.name, 0,0); The semantics of the function are as follows: If either span1 or span2 is null , or the two spans are over different Text objects, the function returns null . Otherwise, if span1 is smaller than span2 or the IgnoreOrder parameter is used, the function returns the shortest span that covers both span1 and span2 . Otherwise ( span1 is larger than span2 and the IgnoreOrder is not used), the function returns a runtime error. Based on the definition of Span, the different scenarios of arguments to the CombineSpans function are as follows: Span 2 is always after span 1. In other words, left-to-right order is maintained: bash CombineSpans([0,7], [3,7]) returns the span [0,7] CombineSpans([0,7], [8,10]) returns the span [0,10] CombineSpans([0,7], [3,6]) returns the span [0,7] CombineSpans([0,7], [0,7]) returns the span [0,7] Span 2 is not after span 1. In other words, left-to-right order is not maintained: bash CombineSpans(\u2018IgnoreOrder\u2019, [0,10], [0,7]) returns the span [0,10] CombineSpans(\u2018IgnoreOrder\u2019, [3,6], [0,7]) returns the span [0,7] CombineSpans(\u2018IgnoreOrder\u2019, [3,7], [0,7]) returns the span [0,7] CombineSpans(\u2018IgnoreOrder\u2019, [8,10], [0,7]) returns the span [0,10] CombineSpans([3,6], [0,7]) will result in Runtime error as the IgnoreOrder flag has not been specified. GetBegin and GetEnd The GetBegin function takes a single span argument and returns the begin offset of the input span. For example, GetBegin([5, 10]) returns the value 5 . Likewise, the GetEnd function returns the end offset of its input span. The following example illustrates the use of the GetBegin and GetEnd function. create view PersonOffsets as select GetBegin(P.name) as offsetBegin, GetEnd(P.name) as offsetEnd from Person P; For both of these functions, if the argument is null , the function returns null . GetLanguage The GetLanguage function takes a single span argument and returns the two-letter language code of the source text of the span. If the argument is null , the function returns null . This statement produces meaningful results only if the data source is tagging text fields with the appropriate languages. GetLemma The GetLemma function takes a single Span or Text object as an argument and returns a string that contains the lemmatized form of the input span. If the argument is null , the function returns null . With dictionary entries for lemma match, this function can determine the lemmatized form of various tokens as returned by the tokenizer. For example, for the span went shopping GetLemma returns the lemma string go shop . The results of this function follow these rules: If the input span starts at the beginning of a token and ends at the end of a token, the result contains the sequence of lemmas that begins with the lemma of the first token, followed by a white space, followed by the lemma of the second token, followed by a white space, and so on (for example, dog cat fish bird ...). If the lemma for a token consists of white spaces, escape the white space by using the backslash character ( \\ ). If the input span starts or ends with white space (for example, it starts between two tokens or ends between two tokens), the function ignores the beginning and trailing white space. If the input span starts in the middle of a token or ends in the middle of a token, then the output consists of the following content, in this order, and separated by a white space: The surface form of the first partial token if it exists. The sequence of lemmas that correspond to the first to last complete tokens. If the lemma for any of the complete tokens consists of white spaces, escape the white spaces by using the backslash character (\\). The surface form of the last partial token if it exists. This function returns an error if the tokenizer that is being used is not capable of producing lemmas. You can use the GetLemma() function to create dictionaries of lemmatized forms. Call GetLemma() on an input that contains the terms whose lemmatized form you want to include in the dictionary. GetLength The GetLength function takes a single span argument and returns the length of the input span. If the argument is null , the function returns null . For example, GetLength([5, 12]) returns a value of 7. GetLengthTok The GetLengthTok function takes a single span argument and returns the length of the input span in tokens. If the input argument is null , the function returns null . GetString The GetString function takes a single AQL object as its argument and returns a Text object that is formed from the string representation of the object. For span and text arguments, the values that are returned are different from those returned by GetText() . For Text objects, the returned value includes single quotation marks surrounding the text string. For span objects, the returned value includes in addition offsets in brackets. For scalar lists, this function returns the GetString() values of the elements of the list, concatenated with semicolons. For Integer, Float, Boolean, and String arguments, this function returns the value of the argument as a string. For null arguments, this function returns null . GetText The GetText function takes a single span or text as an argument. For span input, it returns the text object based on the actual text string that the span marks. For text input, it returns the input text object. If the input is null , then this function returns null . For example: GetText([5, 12]) The span returns the substring of the document from character position 5 - 12. The GetText function has two primary uses. Testing for string equality between the text marked by two spans. -- Create a dictionary of company names create dictionary CompanyNames as ('IBM', 'BigCorp', 'Initech'); -- Find all matches of the company names dictionary. create view Company as extract dictionary 'CompanyNames' on D.text as company from Document D; -- Create a table that maps company names to locations of -- corporate headquarters. create table NameToLocation (name Text, location Text) as values ('IBM', 'USA'), ('BigCorp', 'Apex'), ('Initech', 'Dallas'), ('Acme Fake Company Names', 'Somewhere'); -- Use the table to augment the Company view with location -- information. create view CompanyLoc as select N2C.location as loc, C.company as company from Company C, NameToLocation N2C where Equals(GetText(C.company), GetText(N2C.name)); output view CompanyLoc; Splitting a document into smaller subdocuments. For example, if the main document is a blog that consists of multiple blog entries, you can use GetText to create a subdocument for each blog entry. detag Document.text as DetaggedBlog annotate element 'blog' as Blog with attribute 'name' as title; create view BlogEntry as select B.match as entry, B.title as title from Blog B; -- Turn each tuple in the BlogEntry view into a sub-document create view BlogEntryDoc as select GetText(B.title) as title, GetText(B.entry) as body from BlogEntry B; output view BlogEntryDoc; --Dictionary for Companies create dictionary CompanyNameDict as ( 'A Corporation', 'B Corporation' ); -- Run an extraction over the sub-documents. -- The spans that this \"extract\" statement creates will have -- offsets relative to the blog entries themselves, as opposed -- to the original multi-entry document. create view CompanyName as extract dictionary 'CompanyNameDict' on B.body as name from BlogEntryDoc B; output view CompanyName; LeftContext and RightContext The LeftContext function takes a Span and an Integer as input: LeftContext(<input span>, <nchars>) The LeftContext(<input span>, <nchars>) function returns a new span that contains the nchars characters of the document immediately to the left of <input span> . If the input span starts less than <nchars> characters from the beginning of the document, then LeftContext() returns a span that starts at the beginning of the document and continues until the beginning of the input span. For example, LeftContext([20, 30], 10) returns the span [10, 20]. The span LeftContext([5, 10], 10) returns [0, 5]. If the input starts on the first character of the document, LeftContext() returns a zero-length span. Similarly, the RightContext function returns the text to the right of its input span. For both functions, if either argument is null , the function returns null . LeftContextTok and RightContextTok The LeftContextTok and RightContextTok functions are versions of LeftContext and RightContext that take distances in terms of tokens: LeftContextTok(<input span>, <num tokens>) RightContextTok(<input span>, <num tokens>) The following example illustrates the use of the RightContextTok function. create view Salutation as extract regex /Mr\\.|Ms\\.|Miss/ on D.text as salutation from Document D; --Select the token immediately following a Salutation span create view NameCandidate as select RightContextTok(S.salutation, 1) as name from Salutation S; For both functions, if either argument is null , the function returns null . Remap The Remap function takes a single span argument: Remap(<span>) If the input span is over a text object that was produced by transforming another text object , the Remap function converts the span into a span over the original \"source\" text. For example, if the span N.name is over a detagged document that is produced by running HTML through the AQL detag statement, then Remap(<N.name>) returns an equivalent span over the original HTML. If the span argument was produced by running the detag statement over an empty document, the function remaps the spans to the beginning of the document (in other words, Document.text[0-0]). Also, if the detag statement produces an empty string, the function remaps the spans to the beginning of the document. The only part of AQL that produces such derived text object is the detag statement. The following example illustrates the use of the Remap function: -- Detag the HTML document and annotate the anchor tags detag Document.text as DetagedDoc annotate element 'a' as Anchor; -- Remap the Anchor Tags create view AnchorTag as select Remap(A.match) as anchor from Anchor A; If the argument to Remap is not a derived text object or a span over a derived text object , the function generates an error. If the argument is null , the function returns null . SpanBetween The SpanBetween function takes two spans as input and returns the span that exactly covers the text between the two spans if the spans are based on the same text object, and returns null if they are based on different text objects: SpanBetween(['IgnoreOrder',] <span1>, <span2>) When the optional IgnoreOrder parameter is used, the order of the two spans is ignored by the AQL compiler. If no text exists between the two spans, then SpanBetween returns an empty span that starts at the end of <span1> . Like CombineSpans ,\u001d SpanBetween is sensitive to the order of its inputs, unless you use the IgnoreOrder parameter. So SpanBetween([5, 10], [50, 60]) returns the span [10, 50] , while SpanBetween([50, 60], [5, 10]) returns the span [60, 60] . If the argument to SpanBetween is null , then the function returns null . SpanIntersection The SpanIntersection function takes two spans as input and returns a span that covers the text that both inputs cover if the spans are based on the same text object, and returns null if they are based on different text objects: SpanIntersection(<span1>, <span2>) For example, SpanIntersection([5, 20], [10, 60]) returns the span [10, 20] , while SpanIntersection([5, 20], [7, 10]) returns the span [7, 10] . If the two spans do not overlap, then SpanIntersection returns null . If either span input is null , the function returns null . SubSpanTok The SubSpanTok function takes as input a span and a pair of offsets into the span: SubSpanTok(<span>, <first_tok>, <last_tok>) As the name of the function suggests, the <first_tok> and <last_tok> arguments are distances in tokens, according to whatever tokenizer the system is configured to use. The SubSpanTok function returns a new span that covers the indicated range of tokens, inclusive, within the input span. If the specified range starts inside the span and goes beyond the end of the span, then the portion of the range that is contained is returned. If <first_tok> represents a distance beyond the target span, then SubSpanTok returns a zero-length span that starts at the beginning of the input span. If any input is null , the function returns null . ToLowerCase The ToLowerCase function takes a single object as its argument and returns a lowercase string representation of the object . The conversion to a string occurs in the same way that the GetString() function performs the conversion. The primary use of this function is to perform equality joins that are not case-sensitive: where Equals(ToLowerCase(C.company), ToLowerCase(N2C.name)) If the input object is null , the function returns null . The create function statement {: #aql-create-func} To perform operations on extracted values that are not supported by AQL, you can define custom functions to use in extraction rules called user-defined functions (UDFs). AQL supports user-defined scalar functions and user-defined table functions. Java\u2122 and PMML are the only supported implementation language for UDFs. A scalar function returns a single scalar value, and a table function returns one or more tuples, in other words, a table. You implement user-defined functions (UDFs) by following four steps: Implementing the function AQL supports user-defined functions (UDFs) that are implemented in Java or PMML. 2. Declaring it in AQL. You can make the user-defined scalar functions and machine learning models from PMML files available to AQL by using the create function statement. 3. Using it in AQL. User-defined functions work with AQL statements and clauses. 4. Debugging the UDF. Because Java based user-defined functions (UDFs) are implemented as public methods in Java classes, you debug your UDFs in the same way that you debug your Java programs. Implementing user-defined functions {: #aql-user-def-imp} AQL supports user-defined functions (UDFs) that are implemented in Java\u2122 or PMML. This section specifically focuses on UDFs that are implemented in Java. For UDFs implemented in PMML, the machine learning model is stored inside the PMML XML file. Refer to the documentation of PMML for how to create these models: [http://dmg.org/pmml/v4-1/GeneralStructure.html] You can implement a Scalar UDF as a public method in a Java class. You can implement a table UDF as a public method in a Java class that extends the API com.ibm.avatar.api.udf.TableUDFBase. In addition, a table UDF can optionally override the method initState() of the superclass com.ibm.avatar.api.udf.TableUDFBase. If the UDF has an input parameter of type Span, Text, or ScalarList, or the return type of the UDF is Span, Text, or ScalarList, the Java class must import the classes com.ibm.avatar.algebra.datamodel.Span, com.ibm.avatar.algebra.datamodel.Text, or com.ibm.avatar.algebra.datamodel.ScalarList to compile. Table functions require extra APIs to provide output schema information. These APIs belong to the base class com.ibm.systemt.api.udf.TableUDFbase. If a subclass contains more than one table function, a separate Java object is created for each instance. For the UDF code to retrieve non-class resources from its JAR file, the only supported method is getResourceAsStream() . Other methods for accessing resources (such as getResource() , getResources() , getSystemResource() ) are not supported. For example, a UDF JAR file that contains the properties file my.properties in the package com.ibm.myproject.udfs, can access it with the following Java statement: InputStream in = this.getClass().getClassLoader(). getResourceAsStream(\u201ccom/ibm/myproject/udfs/my.properties\u201d); Lifecycle of UDFs implemented in Java The following operations are performed when the extractor is compiled (the CompileAQL.compile() API), instantiated ( OperatorGraph.createOG() API) and validated ( OperatorGraph.validateOG() API), exactly once for each create function statement in AQL: Load the Java class that contains the UDF method, using a fresh new class loader. The class is searched inside the UDF JAR specified in the corresponding create function statement. Any other classes that are required when loading this class are also searched inside the same UDF JAR, and if not found, the search is delegated to the classloader that instantiated the Runtime. Create an instance of that class. For table UDFs, invoke the method initState() . Since those steps are performed for each create function statement, if a Java class contains multiple UDF methods (and all methods are used in AQL in different create function statements), then the class is loaded multiple times, each time in a separate class loader (Step 1). Furthermore, multiple instances of the class are created (Step 2) and the method initState() is called once for each instance (Step 3). The reason every UDF results in a separate class loader is to allow different UDFs to use different versions of the same class (or library). At runtime ( OperatorGraph.execute() API), the UDF class is not loaded again because the class is loaded during extractor instantiation. The Java method that implements the UDF is invoked when necessary to compute pieces of the output. When the extractor is used within a single thread, that means anywhere from zero to possibly many invocations for each input document exist (and most likely multiple invocations throughout the life of the Operator Graph object). When the extractor is shared across multiple threads, different threads can hit the method around the same time (with different inputs). If you require a specific part of the UDF code to be evaluated exactly once, for example, to initialize data structures needed by the UDF method across invocations, that code should not be placed in the UDF evaluation method, since that method is most likely executed multiple times throughout the life of the extractor (like the OperatorGraph object), and can be hit almost simultaneously when the extractor is shared across multiple threads. Instead: For scalar UDFs, follow standard Java programming principles. For example, place the code in a static block. For table UDFs, place the code in the initState() method, or follow standard Java programming principles. For example, place the code in a static block. When doing so, remember that the class might be loaded multiple times during extractor compilation, instantiation and validation, as explained in Steps 1-3 above. If the code that initializes the UDF is placed in a static block, that code is executed each time that the class is loaded (potentially multiple times), therefore, introducing an overhead during compilation and operator graph instantiation. If the overhead is large, follow these best practices: To minimize overhead during compilation time, the best practice is to place compile-time heavy resources like large dictionaries or UDFs with large initialization time in a separate AQL module and export them. Try compiling only the AQL modules that changed and other modules that depending on them. To minimize overhead during operator graph instantiation and validation: If the initialization code is required by a single UDF method, then place that UDF method in a separate Java class (and place the heavy instantiation code in a static initializer as before, or use some other mechanism that ensures the code is executed once). If the initialization code is shared by multiple UDF methods, then the Runtime and AQL do not provide an explicit mechanism to ensure that initialization code is executed exactly once. In such a situation, if the initialization time is prohibitive, the only solution is to place the shared resources and initialization code on the system class path. In other words, place the initialization code in a new class MySeparateClass.java, and store the initialization result in a static variable of this class, such as MySeparateClass.myVar. Package MySeparateClass.java along with any resources that are needed during initialization in a JAR file and place that JAR on the system class path. The UDF methods can refer to the initialized model using the MySeparateClass.myVar static variable. The initialization code is now executed exactly once - when the class MySeparateClass.java is loaded by the system classloader. Tip: The compiled representation of a module (the .tam file) contains the serialized binary code of the UDF. Therefore, if the same UDF code is referenced by create function statements in two different modules, the UDF code is serialized in the compiled representation of both modules. In other words, the UDF code is serialized twice. In such cases, you can avoid the redundancy by creating a separate module that acts as a library of UDFs, and reuse that library in other modules. To create a library of UDFs, follow these steps: Create one module. Place all UDF JAR files inside that module. Define all necessary functions with the create function statements. Export them all with export function statements so that they can be imported and used in other modules. Examples {: #aql-reference-examples-31} Example 1: Implementing a scalar UDF This example shows the implementation of a scalar UDF named toUpperCase. This UDF takes as input a value of type Span and outputs a string value that consists of the text content of the input Span, converted to uppercase. package com.ibm.biginsights.textanalytics.udf; import com.ibm.avatar.algebra.datamodel.Span; /** * @param s input Span * @return all upper-case version of the text of the input Span */ public String toUpperCase(Span s) { return s.getText().toUpperCase(); } Example 2: Implementing a scalar UDF that uses a table as input This example shows the implementation of a scalar UDF named TableConsumingScalarFunc. This UDF takes as input two lists of tuples and outputs a string value that concatenates the content of the two input tuple lists. import com.ibm.avatar.algebra.datamodel.TupleList; /** * Example implementation of a user-defined scalar function using a table as input */ public class TableConsumingScalarFunc { /** * Main entry point to the `scalar` function. This function takes two lists of tuples and concatenates them into a single string. * * @param arg1 first set of tuples to merge * @param arg2 second set of tuples to merge * @return the two sets of tuples, concatenated */ public String eval (TupleList arg1, TupleList arg2) { StringBuilder sb = new StringBuilder (); sb.append(\"Input1: \"); sb.append(arg1.toPrettyString ()); sb.append(\"\\nInput2: \"); sb.append(arg2.toPrettyString ()); return sb.toString (); } } Example 3: Implementing a table UDF that uses a table as input This example shows the implementation of a table UDF named TableConsumingTableFunc. This UDF takes as input two lists of tuples and outputs a single list of tuples that contains tuples from the first input interspersed with tuples from the second input. Notice that the implementation extends from the base class com.ibm.avatar.api.udf.TableUDFBase, which provides APIs for obtaining the input and output schemas. package com.ibm.test.udfs; import java.lang.reflect.Method; import com.ibm.avatar.algebra.datamodel.AbstractTupleSchema; import com.ibm.avatar.algebra.datamodel.FieldCopier; import com.ibm.avatar.algebra.datamodel.Tuple; import com.ibm.avatar.algebra.datamodel.TupleList; import com.ibm.avatar.algebra.datamodel.TupleSchema; import com.ibm.avatar.api.exceptions.TableUDFException; import com.ibm.avatar.api.udf.TableUDFBase; /** * Example implementation of a user-defined table function */ public class TableConsumingTableFunc extends TableUDFBase { /** Accessors for copying fields from input tuples to output tuples. */ private FieldCopier arg1Copier, arg2Copier; /** * Main entry point to the `table` function. This function takes two lists of tuples and generates a new list of wide * tuples, where element i of the returned list is created by joining element i of the first input with element i of * the second input. * * @param arg1 first set of tuples to merge * @param arg2 second set of tuples to merge * @return the two sets of tuples, interleaved */ public TupleList eval (TupleList arg1, TupleList arg2) { TupleSchema retSchema = getReturnTupleSchema (); TupleList ret = new TupleList (retSchema); // We skip any records that go off the end int numRecs = Math.min (arg1.size (), arg2.size ()); for (int i = 0; i < numRecs; i++) { Tuple retTuple = retSchema.createTup (); Tuple inTup1 = arg1.getElemAtIndex (i); Tuple inTup2 = arg2.getElemAtIndex (i); arg1Copier.copyVals (inTup1, retTuple); arg2Copier.copyVals (inTup2, retTuple); // System.err.printf (\"%s + %s = %s\\n\", inTup1, inTup2, retTuple); ret.add (retTuple); } return ret; } /** * Initialize the internal state of the `table` function. In this case, we create accessors to copy fields from input * tuples to output tuples. * * @see com.ibm.avatar.api.udf.TableUDFBase#initState() for detailed description */ @Override public void initState () throws TableUDFException { // Create accessors to do the work of copying fields from input tuples to output tuples AbstractTupleSchema arg1Schema = getRuntimeArgSchema ().getFieldTypeByIx (0).getRecordSchema (); AbstractTupleSchema arg2Schema = getRuntimeArgSchema ().getFieldTypeByIx (1).getRecordSchema (); TupleSchema retSchema = getReturnTupleSchema (); // Create offsets tables for a straightforward copy. String[] srcs1 = new String[arg1Schema.size ()]; String[] dests1 = new String[arg1Schema.size ()]; String[] srcs2 = new String[arg2Schema.size ()]; String[] dests2 = new String[arg2Schema.size ()]; for (int i = 0; i < srcs1.length; i++) { srcs1[i] = arg1Schema.getFieldNameByIx (i); dests1[i] = retSchema.getFieldNameByIx (i); } for (int i = 0; i < srcs2.length; i++) { srcs2[i] = arg2Schema.getFieldNameByIx (i); dests2[i] = retSchema.getFieldNameByIx (i + srcs1.length); } arg1Copier = retSchema.fieldCopier (arg1Schema, srcs1, dests1); arg2Copier = retSchema.fieldCopier (arg2Schema, srcs2, dests2); } /** * Check the validity of tuple schemas given in the AQL \u201ccreate function\u201d. * * @see com.ibm.avatar.api.udf.TableUDFBase#validateSchema(TupleSchema, TupleSchema, TupleSchema, Method, Boolean) for * description of arguments */ @Override public void validateSchema (TupleSchema declaredInputSchema, TupleSchema runtimeInputSchema, TupleSchema returnSchema, Method methodInfo, boolean compileTime) throws TableUDFException { // The output schema should contain the columns of the two input schemas in order. AbstractTupleSchema arg1Schema = declaredInputSchema.getFieldTypeByIx (0).getRecordSchema (); AbstractTupleSchema arg2Schema = declaredInputSchema.getFieldTypeByIx (1).getRecordSchema (); System.err.printf (\"TableConsumingTableFunc: Input schemas are %s and %s\\n\", arg1Schema, arg2Schema); // First check sizes if (returnSchema.size () != arg1Schema.size () + arg2Schema.size ()) { throw new TableUDFException ( \"Schema sizes don't match (%d + %d != %d)\", arg1Schema.size (), arg2Schema.size (), returnSchema.size ()); } // Then check types for (int i = 0; i < arg1Schema.size (); i++) { if (false == (arg1Schema.getFieldTypeByIx (i).equals (returnSchema.getFieldTypeByIx (i)))) { throw new TableUDFException ( \"Field type %d of output doesn't match corresponding field of first input arg (%s != %s)\", i, returnSchema.getFieldTypeByIx (i), arg1Schema.getFieldTypeByIx (i)); } } for (int i = 0; i < arg2Schema.size (); i++) { if (false == (arg2Schema.getFieldTypeByIx (i).equals (returnSchema.getFieldTypeByIx (i + arg1Schema.size ())))) { throw new TableUDFException ( \"Field type %d of output doesn't match corresponding field of first input arg (%s != %s)\", i + arg1Schema.size (), returnSchema.getFieldTypeByIx (i + arg1Schema.size ()), arg2Schema.getFieldTypeByIx (i)); } } } } Declaring user-defined functions {: #aql-user-def-dec} You can make the user-defined scalar functions and machine learning models from PMML files available to AQL by using the create function statement. Syntax {: #aql-declare-syntax} The general syntax of the create function statement is as follows: create function <function-name>(<input-schema-definition>) return <return-type> [like <column-name>] | table ( <output-schema-definition) external_name <ext-name> language [java | pmml] [deterministic | not deterministic] [return null on null input | called on null input]; <input-schema-definition> <column-name> <data-type> | table (<output-schema-definition>) as locator [,<column-name> <data-type> | table (<output-schema-definition>) as locator ]* <output-schema-definition> <column-name> <data-type> [,<column-name> <data-type>]* Description {: #aql-declare-desc} <function-name> The <function-name> declares the AQL name of the UDF. The UDF is referred to in the AQL code with this name <input-schema-definition> Specifies the input parameters of the UDF. An input parameter has a name, which is specified as <column-name> , and can be either a scalar type or a table locator. When the language is PMML, the function must take a single table that is called params as the argument. <column-name> Specifies the name of a column in the input or the output of the UDF. <data-type> The type of an input scalar parameter to the UDF, the type of a column in the schema of an input table to the UDF or in the schema of the output table of the UDF. Possible values for <data-type> are Integer, Float, String, Text, Span, Boolean, or ScalarList. table (<output-schema-definition>) as locator This input type allows a function to take as input the entire contents of a given AQL view as computed on the current document. The locator parameter references views or tables as arguments. <return-type> For scalar UDFs, the <return-type> specifies the type of the scalar value that is returned by the UDF. Possible values for the return type are: Integer, Float, String, Text, Span, Boolean, or ScalarList. If the return type is Integer, the Java\u2122 function that implements the UDF returns objects of type Integer. The UDF implementation cannot return the primitive int type. If the return type is Text or Span, specify the input parameter from which the return type is derived. You can specify the input parameter by using the optional like specification, since spans are always from an underlying column. If the return type is ScalarList, specify the input parameter from which the scalar type inside the ScalarList is to be inferred. When the language is PMML, the function must return a table. <output-schema-definition> For table UDFs, the <output-schema-definition> specifies the output schema of the table that is returned by the UDF, including the column names and their types. <ext-name> The external_name specifies where to find the implementation of the UDF. When the language is Java, it is a string of the form '<jar-file-name>:<fully-qualified-class-name>!<method-name>' , which consists of three parts: JAR file name: When you compile modular AQL, the location references of UDF JAR files must be relative to the root of the module in which the reference is made. Class name: Fully qualified class name that contains the UDF implementation Method name: The method must be a public method of the class. The method signature must match the parameter types that are specified in the create function statement. Automatic type conversion is not done by the runtime component. When the language is PMML, the external_name clause specifies a string that is the location of the PMML file relative to the module\u2019s root directory. The current implementation supports models that are expressed by using the PMML standard version 4.1 and evaluates them using the version 1.0.22 org.jpmml library. language The language specification points to the language of implementation of the UDF. The runtime component supports only UDFs that are implemented in Java\u2122. deterministic/not deterministic The optional deterministic/not deterministic specifies whether the function is stateless. A deterministic function always returns the same value for the same set of input values. return null on null input The optional return null on null input specifies the function behavior when one or more of the input values are null. If return null on null input is specified, the system returns null on null input without invoking the UDF. If called on null input is specified, the UDF is invoked even for null input values. Usage notes for UDFs implemented in PMML {: #aql-declare-usage} Functions that are created from PMML files take a single table that is called params as their argument and output a table. The implementation of the function maps input fields between the input and output schema declared in the create function statement and the schema that is specified in the PMML file. In other words, the DataDictionary section that describes the names and types of fields that can appear in the input and output records that the model produces and consumes, the MiningSchema section that tells which named files defined in the DataDictionary section are in each tuple that represents a feature vector ,and the Output section that tells which named fields defined in the DataDictionary section are present in each tuple of the external representation of the output of the model. These functions must be table functions; each row in the table is passed to the PMML model and produce an output row. This schema information is necessary because the PMML and AQL type systems do not match perfectly. For example, PMML has several types to represent timestamps, while AQL currently requires users to encode timestamps as string values. The schema information also allows developers who know AQL but not PMML to understand the AQL rule set. The AQL compiler checks the input schema against the input schema of the PMML file to ensure that the two schemas are compatible. If the two schemas contain fields with the same name but incompatible types, compilation fails with an appropriate error message. If the function\u2019s input or output schemas contain extra or missing columns, the resulting function ignores these columns and does not generate an error. The order of column names can be different between the AQL definition and the PMML file. If a column name appears in both input and output schemas but not in the PMML schema, then the values of that column are passed to the output of the function. Examples {: #aql-reference-examples-32} Example 1: Declaring scalar UDFs with scalar values as input using Java The following example shows a create function statement that declares a function that is named udfCombineSpans . The user-defined function takes in two spans as input and returns a merged span similar to the first input span. The actual UDF Java\u2122 function is packaged in a JAR file that is named udfs.jar , under the udfjars directory. The method that implements the function is combineSpans in class com.ibm.test.udfs.MiscScalarFunc . The method also declares a function udfSpanGreaterThan that returns true if the span is greater than the specified size. /** * A function to combine two spans and return the resultant span from beginOffset of first span, to endOffset of second span * @param p1 first input span * @param p2 second input span * @return a span from beginOffset of p1 till endOffset of p2 */ create function udfCombineSpans(p1 Span, p2 Span) return Span like p1 external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!combineSpans' language java deterministic return null on null input; /** * A function to compare an input Span's size against an input size * @param span input span * @param size input size to be compared against * @return Boolean true if input span's size is greater than input argument size, else false */ create function udfSpanGreaterThan(span Span, size Integer) return Boolean external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!spanGreaterThan' language java deterministic; The next example shows how to declare a function that is named udfCompareNames that is given a list of names nameList and a single name myName. The output is a list similar to the input list, which contains only the entries from nameList similar to myName. /** * A function to compare an input string against a list of names * and returns a list of entries from nameList similar to myName. * @param nameList a list of names * @param myName a name to compare against the list * @return a list of entries from nameList similar to myName */ create function udfCompareNames(nameList ScalarList, myName String) return ScalarList like nameList external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!compareNames' language java deterministic; Example 2: Declaring scalar UDFs with tables as input using Java The following example illustrates the use of the table as locator input type. It shows a create function statement that declares a function that is called MyScalarFunc. The Java\u2122 implementation of the UDF is packaged inside the class com.ibm.test.udfs.TableConsumingScalarFunc. This UDF takes as input two lists of tuples and outputs a string value that concatenates the content of the two input tuple lists. The Java\u2122 implementation of the UDF is included in Implementing user-defined functions . -- Declare a simple scalar function that turns two tables into a big string create function MyScalarFunc( firstArg table( spanVal Span ) as locator, secondArg table( spanVal Span, strVal Text ) as locator ) return String external_name 'udfjars/udfs.jar:com.ibm.test.udfs.TableConsumingScalarFunc!eval' language java deterministic called on null input; Example 3: Declaring table UDFs using Java The following example illustrates the use of the return table clause. The example shows a create function statement that declares a table function called MyScalarFunc. The Java\u2122 implementation of the UDF is packaged inside the class com.ibm.test.udfs.TableConsumingTableFunc. This UDF takes as input two lists of tuples and outputs a list of tuples that merges together the input lists. The Java\u2122 implementation of the UDF is included in Implementing user-defined functions . -- Declare a simple table function that \"zips together\" two input tables create function MyTableFunc( firstArg table( spanVal Span ) as locator, secondArg table( spanVal Span, strVal Text ) as locator ) return table( outSpan1 Span, outSpan2 Span, outStr Text) external_name 'udfjars/udfs.jar:com.ibm.test.udfs.TableConsumingTableFunc!eval' language java deterministic called on null input; Example 4: Declaring a table UDF implemented in PMML The following example shows a create function statement that declares a table function named IrisDecisionTree using PMML language. The PMML implementation of the UDF is specified in the IrisTree.xml file, which is shown in the following example. The model that is stored in the IrisTree.xml is a decision tree model. -- Scoring function based on a decision tree model stored in IrisTree.xml create function IrisDecisionTree( params table( sepal_length Float, sepal_width Float, petal_length Float, petal_width Float ) as locator ) return table( class Text, actual_class Text, -- This PMML file also defines additional output fields \"Probability_Iris-setosa\" Float, \"Probability_Iris-versicolor\" Float, \"Probability_Iris-virginica\" Float) external_name 'IrisTree.xml' language pmml; IrisTree.xml <?xml version=\"1.0\"?> <PMML version=\"3.2\" xmlns=\"http://www.dmg.org/PMML-3_2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.dmg.org/PMML-3_2 http://www.dmg.org/v3-2/pmml-3-2.xsd\"> <Header copyright=\"Copyright (c) 2012 DMG\" description=\"RPart Decision Tree Model\"> <Extension name=\"user\" value=\"DMG\" extender=\"Rattle/PMML\"/> <Application name=\"Rattle/PMML\" version=\"1.2.29\"/> <Timestamp>2012-09-27 12:46:08</Timestamp> </Header> <DataDictionary numberOfFields=\"5\"> <DataField name=\"class\" optype=\"categorical\" dataType=\"string\"> <Value value=\"Iris-setosa\"/> <Value value=\"Iris-versicolor\"/> <Value value=\"Iris-virginica\"/> </DataField> <DataField name=\"sepal_length\" optype=\"continuous\" dataType=\"double\"> <Interval closure=\"closedClosed\" leftMargin=\"4.3\" rightMargin=\"7.9\"/> </DataField> <DataField name=\"sepal_width\" optype=\"continuous\" dataType=\"double\"> <Interval closure=\"closedClosed\" leftMargin=\"2\" rightMargin=\"4.4\"/> </DataField> <DataField name=\"petal_length\" optype=\"continuous\" dataType=\"double\"> <Interval closure=\"closedClosed\" leftMargin=\"1\" rightMargin=\"6.91\"/> </DataField> <DataField name=\"petal_width\" optype=\"continuous\" dataType=\"double\"> <Interval closure=\"closedClosed\" leftMargin=\"0.1\" rightMargin=\"2.5\"/> </DataField> </DataDictionary> <TreeModel modelName=\"RPart_Model\" functionName=\"classification\" algorithmName=\"rpart\" splitCharacteristic=\"binarySplit\" missingValueStrategy=\"defaultChild\"> <MiningSchema> <MiningField name=\"class\" usageType=\"predicted\"/> <MiningField name=\"sepal_length\" usageType=\"supplementary\"/> <MiningField name=\"sepal_width\" usageType=\"supplementary\"/> <MiningField name=\"petal_length\" usageType=\"active\"/> <MiningField name=\"petal_width\" usageType=\"supplementary\"/> </MiningSchema> <Output> <OutputField name=\"class\" optype=\"categorical\" dataType=\"string\" feature=\"predictedValue\"/> <OutputField name=\"Probability_Iris-setosa\" optype=\"continuous\" dataType=\"double\" feature=\"probability\" value=\"Iris-setosa\"/> <OutputField name=\"Probability_Iris-versicolor\" optype=\"continuous\" dataType=\"double\" feature=\"probability\" value=\"Iris-versicolor\"/> <OutputField name=\"Probability_Iris-virginica\" optype=\"continuous\" dataType=\"double\" feature=\"probability\" value=\"Iris-virginica\"/> </Output> <Node id=\"1\" score=\"Iris-virginica\" recordCount=\"105\" defaultChild=\"3\"> <True/> <ScoreDistribution value=\"Iris-setosa\" recordCount=\"33\" confidence=\"0.314285714285714\"/> <ScoreDistribution value=\"Iris-versicolor\" recordCount=\"35\" confidence=\"0.333333333333333\"/> <ScoreDistribution value=\"Iris-virginica\" recordCount=\"37\" confidence=\"0.352380952380952\"/> <Node id=\"2\" score=\"Iris-setosa\" recordCount=\"33\"> <SimplePredicate field=\"petal_length\" operator=\"lessThan\" value=\"2.6\"/> <ScoreDistribution value=\"Iris-setosa\" recordCount=\"33\" confidence=\"1\"/> <ScoreDistribution value=\"Iris-versicolor\" recordCount=\"0\" confidence=\"0\"/> <ScoreDistribution value=\"Iris-virginica\" recordCount=\"0\" confidence=\"0\"/> </Node> <Node id=\"3\" score=\"Iris-virginica\" recordCount=\"72\" defaultChild=\"7\"> <SimplePredicate field=\"petal_length\" operator=\"greaterOrEqual\" value=\"2.6\"/> <ScoreDistribution value=\"Iris-setosa\" recordCount=\"0\" confidence=\"0\"/> <ScoreDistribution value=\"Iris-versicolor\" recordCount=\"35\" confidence=\"0.486111111111111\"/> <ScoreDistribution value=\"Iris-virginica\" recordCount=\"37\" confidence=\"0.513888888888889\"/> <Node id=\"6\" score=\"Iris-versicolor\" recordCount=\"37\"> <SimplePredicate field=\"petal_length\" operator=\"lessThan\" value=\"4.85\"/> <ScoreDistribution value=\"Iris-setosa\" recordCount=\"0\" confidence=\"0\"/> <ScoreDistribution value=\"Iris-versicolor\" recordCount=\"34\" confidence=\"0.918918918918919\"/> <ScoreDistribution value=\"Iris-virginica\" recordCount=\"3\" confidence=\"0.0810810810810811\"/> </Node> <Node id=\"7\" score=\"Iris-virginica\" recordCount=\"35\"> <SimplePredicate field=\"petal_length\" operator=\"greaterOrEqual\" value=\"4.85\"/> <ScoreDistribution value=\"Iris-setosa\" recordCount=\"0\" confidence=\"0\"/> <ScoreDistribution value=\"Iris-versicolor\" recordCount=\"1\" confidence=\"0.0285714285714286\"/> <ScoreDistribution value=\"Iris-virginica\" recordCount=\"34\" confidence=\"0.971428571428571\"/> </Node> </Node> </Node> </TreeModel> </PMML> Documenting the create function statement with comments The AQL Doc comment for a create function statement contains the following information: A general description about the function. The @param description that specifies each parameter name that is used in the function. Indicate the type of the parameter, whether it is a scalar type or a table. If the parameter is a table, describe the schema of the table, including column names and types in the order in which they appear in the schema of the table expected as input. The @return description that specifies the information that is returned by the function. If the function returns a table, specify the output schema of the table, including column names and types in the order in which they appear in the schema of the output table. /** * A function to compare an input string against a list of names * and returns a list of entries from nameList similar to myName. * @param nameList a list of names * @param myName a name to compare against the list * @return a list of entries from nameList similar to myName */ create function udfCompareNames(nameList ScalarList, myName String) return ScalarList like nameList external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!compareNames' language java deterministic; Using user-defined functions {: #aql-user-def-use} User-defined functions work with AQL statements and clauses. User-defined scalar functions work with AQL statements and clauses, similar to built-in functions. Specifically, scalar UDFs can be used in the select , where , having , group by , and order by clauses in the same way as built-in scalar functions, such as GetBegin and LeftContext . User-defined scalar functions that return a Boolean type can be used as predicates in where and having clauses. User-defined table functions (table UDFs) can be used within the from clause of a select statement or an extract statement. Examples {: #aql-reference-examples-33} Example 1: Using scalar UDFs implemented in Java with scalar values as input This example demonstrates how to use the udfCombineSpans and udfSpanGreaterThan functions that are declared in Declaring user-defined functions . create function udfCombineSpans(p1 Span, p2 Span) return Span like p1 external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!combineSpans' language java deterministic return null on null input; create function udfSpanGreaterThan(span Span, size Integer) return Boolean external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!spanGreaterThan' language java deterministic; -- identify occurrences of given names in the document create dictionary FirstNamesDict from file 'firstNames.dict'; create view FirstName as extract dictionary 'FirstNamesDict' on D.text as name from Document D; -- Use a UDF to merge the name that is longer than 7 characters with the text to its right in a -- way that is appropriate to the application. create view NameAndContext as select udfCombineSpans(F.name, RightContext(F.name, 50)) as name from FirstName F where udfSpanGreaterThan(F.name, 7); Example 2: Using Scalar UDFs implemented in Java with tables as input The following example shows how to use a scalar function that uses tables as input. This example illustrates the usage of the MyScalarFunc table UDF function, whose Java\u2122 implementation is included in Implementing user-defined functions . They are the same column types, but have different column names. -- Declare a simple scalar function that turns two tables into a big string create function MyScalarFunc( firstArg table( spanVal Span ) as locator, secondArg table( spanVal Span, strVal Text ) as locator ) return String external_name 'udfjars/udfs.jar:com.ibm.test.udfs.TableConsumingScalarFunc!eval' language java deterministic called on null input; -- Create two views to serve as inputs to the `table` function. -- Note that column names don't match, but types do. create view FirstInputView as extract regex /\\d+/ on 1 token in D.text as match from Document D; create view SecondInputView as select S.match as spanCol, 'Dummy string' as textCol from (extract regex /[A-Z][a-z]+/ on 1 token in D.text as match from Document D) S; -- Call the `scalar` function defined above from the select list. create view ScalarFuncOutput as select MyScalarFunc(FirstInputView, SecondInputView) as func from Document D; Example 3: Using Table UDFs implemented in Java This example illustrates the usage of the MyTableFunc table UDF function, whose Java\u2122 implementation is included in Implementing user-defined functions . -- Declare a simple table function that \"zips together\" two input tables create function MyTableFunc( firstArg table( spanVal Span ) as locator, secondArg table( spanVal Span, strVal Text ) as locator ) return table( outSpan1 Span, outSpan2 Span, outStr Text) external_name 'udfjars/udfs.jar:com.ibm.test.udfs.TableConsumingTableFunc!eval' language java deterministic called on null input; -- Create two views to serve as inputs to the `table` function. -- Note that column names don't match, but types do. create view FirstInputView as extract regex /\\d+/ on 1 token in D.text as match from Document D; create view SecondInputView as select S.match as spanCol, 'Dummy string' as textCol from (extract regex /[A-Z][a-z]+/ on 1 token in D.text as match from Document D) S; -- Use the `table` function create view TabFuncOutput as select T.outSpan1 as span1, T.outSpan2 as span2 from MyTableFunc(FirstInputView, SecondInputView) T; As in Example 2, The example defines two views: FirstInputView with schema (match Span) and SecondInputView with schema (spanCol Span, textCol Text) . The last view in the example, TabFuncOutput invokes the table function MyTableFunc in the from clause, with the two input views. As explained in Example 2, a view is compatible with a table locator argument of a function, as long as the number of columns and the column types in the schema of the input view and table locator match. However, it is not necessary that the column names match. Finally, the select clause discards column outStr from the output table of MyTableFunc, keeping only the first two columns outSpan1 and outSpan2 . Example 4: Using a table UDF implemented in PMML This example illustrates the usage of the IrisDecisionTree table UDF function that is declared in example 4 of Declaring user-defined functions . -- External view to hold the input records create external view IrisData( sepal_length Float, sepal_width Float, petal_length Float, petal_width Float ) external_name 'IrisData'; --Invoke the function on the input data records create view IrisDecisionTreeOutput as select * from IrisDecisionTree(IrisData); output view IrisDecisionTreeOutput;","title":"AQL Reference"},{"location":"aql-ref-guide/#annotation-query-language-reference","text":"Annotation Query Language (AQL) is the primary language that is used to create {{site.data.keyword.knowledgestudiofull}} advanced rules extractors. Data model : The data model for AQL is similar to the standard relational model that is used by an SQL database such as DB2\u00ae. All data in AQL is stored in tuples, data records of one or more columns, or fields. A collection of tuples forms a view. All tuples in a view must have the same schema that is the names and types of the fields across all tuples. Execution model : The runtime component has a document-at-a-time execution model. The runtime component receives a collection of documents and runs the extractor on each document to extract information from that document. AQL statements : By using AQL statements, you can create and then use modules, views, tables, dictionaries, and functions. Built-in functions : AQL has a collection of built-in functions for use in extraction rules. The create function statement : To perform operations on extracted values that are not supported by AQL, you can define custom functions to use in extraction rules called user-defined functions (UDFs). Table of Contents {:toc}","title":"Annotation Query Language reference"},{"location":"aql-ref-guide/#annotation-query-language-aql-syntax","text":"{: #aql-syntax} Like many programming languages, AQL is based on common syntax and grammar. The lexical structure of a programming language is the set of elementary rules that define the tokens or basic components of that language such as its reserved words, identifiers, constants, and more. The syntax of AQL is similar to that of SQL, but several important differences exist: AQL is case-sensitive. AQL currently does not support advanced SQL features such as correlated subqueries and recursive queries. AQL has a new statement type, extract , which is not present in SQL. AQL does not allow keywords (reserved words) as view, column, or function names. AQL allows, but does not require, regular expressions to be expressed in Perl syntax. Regular expressions begin with a slash ( / ) and end with a slash ( / ), as in Perl syntax. AQL also allows regular expressions that start with a single quotation mark ( ' ) and end with a single quotation mark ( ' ). For example, you can use /regex/ instead of 'regex' as the regular expression in AQL. Identifiers : Identifiers are used to define the names of AQL objects, including names of modules, views, tables, dictionaries, functions, attributes, and function parameters. Reserved words : Reserved words are words that have a fixed meaning within the context of the AQL structure and cannot be redefined. Keywords are reserved words that have special meanings within the language syntax. Constants : Constants are fixed values that can be one of these data types: String, Integer, Float, or Boolean. Comments \" Use comments to augment AQL code with basic descriptions to help others understand the code and to generate self-describing compiled modules. Expressions : An AQL expression is a combination of one or more scalar values and functions that evaluates to a single scalar value.","title":"Annotation Query Language (AQL) syntax"},{"location":"aql-ref-guide/#identifiers","text":"{: #aql-identifiers} Identifiers are used to define the names of AQL objects, including names of modules, views, tables, dictionaries, functions, attributes, and function parameters. Two types of case-sensitive AQL identifiers exist: Simple identifier A simple identifier must start with a lowercase ( a-z ) or uppercase ( A-Z ) letter or the underscore character ( _ ). Subsequent characters can be lowercase or uppercase letters, the underscore character, or digits ( 0-9 ). A simple identifier must be different from any AQL key word. Double-quoted identifier A double-quoted identifier starts and ends with a double quotation mark character ( \" ). You can use any character between the beginning and ending double quotation mark characters. Double-quoted identifiers cannot contain a period ( . ) character. If a double quotation mark character occurs within name, it must be escaped by prefixing it with the backslash character (\\), for example \\\u201d .","title":"Identifiers"},{"location":"aql-ref-guide/#reserved-words","text":"{: #aql-reserved} Reserved words are words that have a fixed meaning within the context of the AQL structure, and cannot be redefined. Keywords are reserved words that have special meanings within the language syntax. The following AQL-reserved keywords cannot be used as identifiers because each has a well-defined purpose within the language: all allow allow_empty always and annotate as ascending ascii attribute between blocks both by called case cast ccsid character characters columns consolidate content_type count create default descending detag detect deterministic dictionary dictionaries document element else empty_fileset entries exact export external external_name extract fetch file first flags folding from function group having import in include infinity inline_match input into insensitive java language left lemma_match like limit mapping matching_regex minus module name never not null on only order output part_of_speech parts_of_speech parameter pattern point points priority regex regexes retain required return right rows select separation set specific split table tagger then token Token tokens unicode union up using values view views when where with The following reserved words are the names of built-in scalar types: Text Span Integer Float String Boolean ScalarList The following other reserved names cannot be used as identifiers: Dictionary Regex Consolidate Block BlockTok Sentence Tokenize RegexTok PosTag","title":"Reserved words"},{"location":"aql-ref-guide/#constants","text":"{: #aql-constants} Constants are fixed values that can be one of these data types: String , Integer , Float , or Boolean . Constants are used in the select list of a select or extract clause, or as arguments in built-in or UDF functions and predicates. AQL supports the following types of constants: String constant A string that is enclosed in single quotation marks (\u2018), for example \u2018a string\u2019. Integer constant A 32-bit signed integer value that is not enclosed in quotation marks, for example 10 or -1. Float constant A single-precision 32-bit floating point value, not enclosed in quotation marks, for example 3.14 or -1.2. Boolean constant The value true or false not enclosed in quotation marks. Null constant The value null that is not enclosed in quotation marks.","title":"Constants"},{"location":"aql-ref-guide/#comments","text":"{: #aql-comments} Use comments to augment AQL code with basic descriptions to help others understand the code and to generate self-describing compiled modules. Comments allow AQL developers to augment AQL code with basic descriptions, for ease of understanding of AQL source code, and they generate self-describing compiled AQL modules. Three kinds of comments are supported in AQL: Single-line comments Single-line comments begin with double hyphens ( -- ). Mulitple-line comments Multiple-line comments begin with a slash and an asterisk ( /* ) and end with an asterisk and a slash ( * ). Multiple-line comments cannot be nested. For example, the following nested mulitple-line comment is not allowed: bash /* A comment with a /*nested comment*/ */ AQL Doc comments AQL Doc comments provide a way to describe a module or an AQL object in plain language, and in an aspect-rich manner for contextual comprehension by other users. Unlike single-line comments and multiple line comments, which are ignored by the AQL Compiler, AQL Doc comments are serialized in the metadata of compiled modules (.tam files) and available for external consumption. All comments in AQL Doc for statements and modules have the following format: The comment is in plain text (no HTML tags are supported). The comment begins with a forward slash followed by two asterisks ( /** ) and ends with an asterisk forward slash ( */ ). Optionally, each line can start with an asterisk ( * ). Any number of white spaces can be used before the asterisk. Special tags that are prefixed by an at symbol ( @ ) can be used at the beginning of each line or after the optional asterisk. AQL Doc comments cannot be nested. Two levels of granularity are supported within the AQL Doc system. The format for documenting each artifact is explained in detail in the topic that describes its statement and syntax. Module-level comments Module level comments are contained inside a special file that is named module.info and located directly under the module folder. The comments are expected to describe the semantics of the module, and the schema of the view Document of the module. Statement-level comments Statement level comments are contained in the source AQL file, immediately preceding the statement that creates an AQL object. Single-line comments and multiple-line comments are allowed between the AQL Doc comment of a statement and the statement itself. The following top-level AQL statements can be documented by using AQL Doc comments: The create external view statement The create external table statement The create external dictionary statement The create function The detag statement The select... into statement AQL Doc comments are serialized inside the compiled representation of a module.","title":"Comments"},{"location":"aql-ref-guide/#expressions","text":"{: #aql-expr} An AQL expression is a combination of one or more scalar values and functions that evaluates to a single scalar value. Expressions can be one of four types: a constant a column reference a scalar function call an aggregate function call","title":"Expressions"},{"location":"aql-ref-guide/#constant","text":"An expression can be a constant of type Integer , Float , or String , as in the following example: select 'positive' as polarity The expression is the string constant positive .","title":"Constant"},{"location":"aql-ref-guide/#column-reference","text":"An expression can be a column reference, as in the following example: create view Person as select F.name as firstname, L.name as lastname from FirstName F, LastName L where FollowsTok(F.name, L.name, 0, 0); This view identifies text that can be interpreted as a person\u2019s full name (for example, \u201cSamuel Davis\u201d, \u201cVicky Rosenberg\u201d). The expressions F.name and L.name are column-reference expressions that return the name column of the views F and L , respectively. The from statement defines views F and L as the local names for the views FirstName and LastName (which define valid given names and surnames, and are not shown in this example).","title":"Column reference"},{"location":"aql-ref-guide/#scalar-function-call","text":"An expression can be composed of one or more scalar function calls, each of which might contain arguments that are also expressions of type constant, column reference, or scalar function call. A scalar function call can be one of the built-in scalar functions, or a scalar user-defined function. Consider the following example: create view FamilyMembers as select ToLowerCase(GetText(P.lastname)) as lastname, List( (GetText(P.firstname))) as firstnames from Person P group by GetText(P.lastname); This view identifies potential family members by grouping people with the same surnames together, printing all of the names, with the surnames in lowercase characters (for example, lastname keaton, firstnames (Elyse, Alex, Diane)). The outputs of the GetText function calls are used as arguments to the ToLowerCase function call to display the names in lowercase. The scalar function call expressions in this example are: ToLowerCase , (GetText(P.lastname) , ToLowerCase(GetText(P.firstname)) , and GetText(P.lastname) .","title":"Scalar function call"},{"location":"aql-ref-guide/#aggregate-function-call","text":"An expression can be an aggregate function call. This expression type can have as arguments another expression of type column reference or type scalar function call. The following example shows an aggregate function call with expression type column reference: create view CountLastNames as select Count(Person.lastname) from Person; The expression is simply Count(Person.lastname) and counts the number of Person.lastname annotations in the document. An example of an aggregate function call with expression type scalar function call exists in the previous example as List(GetText(P.firstname)) with the List aggregate function taking a GetText scalar function as an argument to generate a list of given names. Aggregate function call expressions are only allowed as expressions in the select list of a select statement. Aggregate function call expressions are not allowed in the select list of an extract statement, or as arguments to a scalar or aggregate function call.","title":"Aggregate function call"},{"location":"aql-ref-guide/#data-model","text":"{: #aql-datamodel} The data model for AQL is similar to the standard relational model that is used by an SQL database such as DB2\u00ae. All data in AQL is stored in tuples, data records of one or more columns, or fields. A collection of tuples forms a view. All tuples in a view must have the same schema that is the names and types of the fields across all tuples. The content of the input document is represented as a special view called Document. The fields of a tuple must belong to one of the built-in data types of AQL: Boolean A data type that has a true or false value. Float A single-precision floating-point number. Integer A 32-bit signed integer. ScalarList A collection of values of the same scalar type (Integer, Float, Text, or Span). A value of data type ScalarList can be obtained as a result of the AQL built-in aggregate function List() , or as the result of a UDF. Span A Span is a contiguous region of a Text object, which is identified by its beginning and ending offsets in the Text object. Assume that your input text is: bash Amelia Earhart is a pilot. The text across Span [0-6] is Amelia . This Span can be visualized as: bash A m e l i a ^ ^ ^ ^ ^ ^ ^ 0 1 2 3 4 5 6 Likewise, the text across Span [20-25] is pilot . A Span of [x-x] represents the span between the end of a character and the beginning of the next character. In the previous example, [0-0] is an empty string before the character A . Likewise, a Span of [3-3] is an empty string between the characters e and l . {: note} A value of type Span can be obtained as a result of an extract statement or a built-in scalar function, or a UDF. Text The AQL data type to represent a character sequence. A text object contains a Unicode string, called its string value. When a string is formed as the concatenation of disjoint substrings of another Text object, it also contains reference to the original Text object and relevant offset mapping information. Two Text objects are considered equal to each other if all their components are correspondingly equal to each other. Comparing values of type Span and Text Prioritization factors affect comparisons between values of type Span and type Text.","title":"Data model"},{"location":"aql-ref-guide/#comparing-values-of-type-span-and-text","text":"{: #aql-comparing} Prioritization factors affect comparisons between values of type Span and type Text. Values of type Span and type Text compare against each other in these ways: A null span value is always sorted lower than other values. A text value is always sorted higher than a span value. Values of type Text are sorted first by the lexical order of their string values, then by their original Text orders and offset mapping information if applicable. Values of type Span are sorted first by their underlying text objects, then by their begin offset, and then by their end offset. The span with a smaller begin offset is sorted lower. Between two spans that start on the same offset, the one with the smaller end offset sorts lower.","title":"Comparing values of type Span and Text"},{"location":"aql-ref-guide/#execution-model","text":"{: #aql-execmodel} The runtime component has a document-at-a-time execution model. The runtime component receives a collection of documents and runs the extractor on each document to extract information from that document. An extractor consists of one or more AQL modules to create a collection of views that each defines a relation. Some of these views are designated as output views, while others are non-output views. Non-output views can include some views that are imported into or exported from a module. It is important to note that output views and exported views are orthogonal. For example, a view that is exported does not qualify as a view that is output. In addition to these views, the unique Document view represents the input document that is annotated by this extractor.","title":"Execution model"},{"location":"aql-ref-guide/#document-view","text":"At the AQL module level, Document view is a special view that represents the current document that is being annotated by that module. When two or more modules are combined to form an extractor, the duplicate-free union of Document schemas of all modules is the required Document view. Use the require document with columns statement to specify the schema of the Document view. If this statement is absent from a module, the default schema that is assumed for the Document view is (text Text, label Text): text The textual content of the current document. label The label of the current document that is being annotated. The keyword Document is reserved as an identifier for the Document view, which is automatically populated during execution. Therefore, you cannot define another view or table with the same name. However, you can use the word Document as an identifier for attribute names and aliases.","title":"Document view"},{"location":"aql-ref-guide/#aql-statements","text":"{: #aql-statements} By using AQL statements, you can create and then use modules, views, tables, dictionaries, and functions. The following statements are supported in AQL: Statements for creating modules and declaring the interaction between them The module statement The export statement The import statement Statements for creating AQL objects: views, dictionaries, tables, or functions The create dictionary and create external dictionary statements The create table statement The create view statement The create external view statement The detag statement The extract statement The select statement The require document with columns statement The set default dictionary language statement The create function statement The syntax specifications in the AQL statements contain brackets ( [ ] ). These specifications indicate that the brackets and the constructs that they hold are optional when the corresponding syntax is used in writing a statement. In addition, they stand as place holders to define how to specify additional instances of a construct or argument. {: note}","title":"AQL statements"},{"location":"aql-ref-guide/#the-module-statement","text":"{: #aql-module} Use the module statement to create a module that has self-contained required resources. You can export and import these resources as AQL objects to and from other modules.","title":"The module statement"},{"location":"aql-ref-guide/#syntax","text":"{: #aql-module-syntax} module <module-name>;","title":"Syntax"},{"location":"aql-ref-guide/#description","text":"{: #aql-module-desc} <module-name> Declares that the current file is part of the module that is named <module-name> . The module name must be a simple identifier. Double-quoted identifiers are not allowed as module names. Each AQL file inside the module must have exactly one module declaration, and this declaration must be the first statement in each file. This declaration establishes a namespace identical to the module-name. All views (and other objects such as dictionaries, tables, and functions) declared in AQL files inside this module are located inside this single namespace. They are accessible to all AQL files in this namespace. All of the files that are declared as part of the module <module-name> must be inside a folder that is called <module-name> to enforce this namespace. No ordering of the AQL files within this module folder exists. The AQL compiler examines all AQL files of the module and determines the right order to compile all views, dictionaries, tables, and functions declared in the module.","title":"Description"},{"location":"aql-ref-guide/#usage-notes","text":"{: #aql-module-usage} When the underlying AQL file is not located as part of a module (modular AQL) and is compiled in compatibility mode, this statement is not supported. Circular dependencies between views are not allowed. AQL modules do not support submodules. AQL files within subfolders of the top-level module folder are ignored.","title":"Usage notes"},{"location":"aql-ref-guide/#examples","text":"{: #aql-reference-examples-1} Example 1: Sample module In the following example, the view TheMentions belongs to the module that is named sample. module sample; create dictionary GetTheDict as ('The', 'the'); create view TheMentions as extract dictionary 'GetTheDict' on D.text as theMention from Document D;","title":"Examples"},{"location":"aql-ref-guide/#the-export-statement","text":"{: #aql-export} The export statement in AQL is used to export an AQL object from the current module so that it can be imported and used in other modules.","title":"The export statement"},{"location":"aql-ref-guide/#syntax_1","text":"{: #aql-export-syntax} export view|dictionary|table|function <object-name>;","title":"Syntax"},{"location":"aql-ref-guide/#description_1","text":"{: #aql-export-desc} view|dictionary|table|function Defines the type of object to export. The object type can be a view, dictionary, table, or function. <object-name> Defines the name of the object to export. The <object-name> can be a simple identifier or a double-quoted identifier.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_1","text":"{: #aql-export-usage} You cannot export any of the imported AQL objects. You can create the effect of re-exporting the view or table in the current module by creating a new view: bash select * from <imported_view_name_or_table_name>; The export view and output view statements that are shown in the examples are orthogonal to each other. In other words, an output view is not automatically an exported view, but it must be explicitly exported by using an export statement. An exported view is not automatically an output view, but it must be explicitly output by using the output view statement. In the examples, the export statement attempts to export the view PersonName.FirstName, which is an imported view. This attempt causes an error, which means that the developer must copy the imported view into a new view and then export that instead.","title":"Usage notes"},{"location":"aql-ref-guide/#examples_1","text":"{: #aql-reference-examples-2} Example 1: Creating views and dictionaries and then exporting them for use in other modules This example creates the views FirstName and NotFirstName. The view FirstName collects information about the given names that are represented in the FirstNamesDictionary dictionary. The other view collects the names that remain when you exclude the given names. Two dictionaries are necessary to make the text extraction easier. One dictionary contains all of the given names that you want to search for. The second dictionary, LastNamesDictionary, contains the surnames to search for. The FirstNamesDictionary dictionary is exported so that it can be used in other modules. The FirstName and the NotFirstName views are also exported so that they can be imported and used in other modules, such as module person in Example 2. module personName; create dictionary FirstNamesDictionary as ('Smith', 'John', 'Mary', 'Sam', 'George'); -- export dictionary statement export dictionary FirstNamesDictionary; create view FirstName as extract dictionary 'FirstNamesDictionary' on D.text as firstName from Document D; -- export view statement export view FirstName; create dictionary LastNamesDictionary as ('Stewart', 'Johnson', 'Smith', 'Hopkins', 'George'); create view NotFirstName as select F.firstName as notFirstName from FirstName F where ContainsDict('LastNamesDictionary', F.firstName); -- export view statement export view NotFirstName; Example 2: Importing views to use, but exporting inappropriately, causing an error This example shows the importing of two views. These views were exported from module personName in Example 1. Module person can now import and reference those views. However, this module attempts to export the same view that it imported, FirstName, which causes an error. module person; -- Form 1 import view FirstName from module personName as PersonFirstName; -- Form 2 import view NotFirstName from module personName; -- ERROR -- Reason: A view that is imported from one module -- cannot be exported in the current module. export view personName.FirstName; output view PersonFirstName; The reason for the error in this code is that a view that is imported from one module cannot be exported in the current module. In addition, exported views are not automatically output views unless you defined an output view with the output view statement.","title":"Examples"},{"location":"aql-ref-guide/#the-import-statement","text":"{: #aql-import} You can use the import statement to reference objects that are exported from other modules in the context of the current module.","title":"The import statement"},{"location":"aql-ref-guide/#syntax_2","text":"{: #aql-import-syntax} import view|dictionary|table|function <object-name> from module <module-name> [as <alias>];","title":"Syntax"},{"location":"aql-ref-guide/#description_2","text":"{: #aql-import-desc} view\\|dictionary\\|table\\|function Identifies the type of AQL object to be imported. The type of the object is mandatory. <object-name> The <object-name> can be a simple identifier or a double-quoted identifier. <module-name> The <module-name> must be a simple identifier. <alias> This form of the import statement, also known as alias import , imports the specified AQL object under the <alias> name (not the original name) into the namespace of the current module. You can reference the imported element by using either an unqualified alias or an alias that is qualified with the current module name (the module where the AQL object was imported). You cannot use originalModule.elementName because the alias import statement imports the element under the alias name only and not under the qualified original name.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_2","text":"{: #aql-import-usage} An import statement without an alias specification imports the specified AQL object into the current module. It makes the AQL object accessible to AQL statements defined in the current module under its qualified name <original_module_name>.<object-name> . The import statement is only used to import AQL objects from modules other than the current module. An object that is declared in an AQL file of the module is visible to any other AQL file in that same module. An import statement puts objects in the context of the current module, and not in the context of the current file. Therefore, a view that is imported by 1.aql inside module A is made visible to 2.aql inside the same module without the need for any additional import statements. All import statements must follow immediately after the module declaration, and must precede all other types of statements. Only those AQL objects that are explicitly exported from any module can be imported in another module. If this requirement is not observed, a compilation error results. A compilation error is introduced when an import view statement introduces a naming conflict with any create view statement or other import statements within the same module (not just within the current file). This restriction applies to the import of other objects in addition to views. The AQL compiler adheres to the naming conventions that are used in previous versions of AQL: A module cannot contain a view and a table with the same name. A dictionary with the same name as a table or view is allowed. A function with the same name as a table, view, or dictionary is allowed. A compilation error is introduced when different import statements within one or multiple AQL files in the module give the same name to different AQL objects. A compilation error is introduced when an AQL file inside a module tries to reference another exported view of a module without using the import view statement. The same applies to dictionaries, tables, or functions. When two AQL files inside a module import the same view X from another module under two different aliases, for example, A and B, then the two aliases are treated synonymously. This rule applies also to tables, dictionaries, and functions.","title":"Usage notes"},{"location":"aql-ref-guide/#examples_2","text":"{: #aql-reference-examples-3} Example 1: Create views that you export to be imported into other modules. This example creates two views, FirstName , and NotFirstName . The view FirstName collects information about the given names that are represented in the FirstNamesDictionary dictionary. The second view collects the names that are left when you exclude the given names. Two dictionaries are necessary to make the text extraction easier. One dictionary contains all of the given names that you want to search for. The second dictionary, LastNamesDictionary , contains the surnames to search for. The FirstName and the NotFirstName views are exported so that they can be imported and used in other modules, such as module person in Examples 2 and 3. module personName; create dictionary FirstNamesDictionary as ('Smith', 'John', 'Mary', 'Sam', 'George'); create view FirstName as extract dictionary 'FirstNamesDictionary' on D.text as firstName from Document D; export view FirstName; create dictionary LastNamesDictionary as ('Stewart', 'Johnson', 'Smith', 'Hopkins', 'George'); create view NotFirstName as select F.firstName as notFirstName from FirstName F where ContainsDict('LastNamesDictionary', F.firstName); export view NotFirstName; Example 2: Importing the view FirstName by using alias import This example imports one of the views that were created and exported in Example 1. Then, the PersonFirstName view is output so that its results can be viewed. The sample import statement is known as an alias import. It imports the view FirstName , with no module qualifier, to the namespace of the current module, person . The imported view can be accessed only through the alias name PersonFirstName and not through any other form. For example, you cannot refer to the imported view as personName.FirstName because it is imported only through the alias name. module person; import view FirstName from module personName as PersonFirstName; output view PersonFirstName; Example 3: Importing the view NotFirstname without using alias import This sample import statement imports the qualified name personName.NotFirstName (not the unqualified name of the view) to the namespace of the current module, person . Always refer to the imported view by using only the qualified name. Any other mode of reference is flagged as compiler error. module person; import view NotFirstName from module personName; output view personName.NotFirstName;","title":"Examples"},{"location":"aql-ref-guide/#the-import-module-statement","text":"{: #aql-import-mod} You can use the import module statement to import and reuse existing AQL modules.","title":"The import module statement"},{"location":"aql-ref-guide/#syntax_3","text":"{: #aql-import-mod-syntax} import module <module-name>;","title":"Syntax"},{"location":"aql-ref-guide/#description_3","text":"{: #aql-import-mod-desc} <module-name> Specifies the module to import. The <module-name> must be a simple identifier.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_3","text":"{: #aql-import-mod-usage} The import statement is only used to import AQL objects from other modules, not from the current module. All import statements must follow immediately after the module declaration and must precede all other types of statements. Only those AQL objects that are explicitly exported from any module can be imported in another module. If this requirement is not observed, a compilation error results. A compilation error is introduced when an import view statement introduces a naming conflict with any create view or other import statement within the same module (not just within the current file). This restriction applies to the import of other AQL objects in addition to views. A compilation error is introduced when an AQL file inside a module tries to reference another exported view of a module without using the import view statement. The same applies to dictionaries, tables, or functions. If two AQL files inside a module import the same view X from another module under two different aliases, for example, A and B, then the two aliases are treated synonymously. This rule applies also to tables, dictionaries, and functions.","title":"Usage notes"},{"location":"aql-ref-guide/#examples_3","text":"{: #aql-reference-examples-4} In this example, the import statement imports the qualified name of both the exported views, personName.FirstName and personName.NotFirstName . Any view that is not exported by the module personName is not imported as a part of the import statement Example 1: Import both FirstName and NotFirstName views This example shows all of the exported views from module personName. The views FirstName and NotFirstName were created in the example section of the export statement. ```bash module personOther; -- The following statement would import both views FirstName and NotFirstName. import module personName; ```","title":"Examples"},{"location":"aql-ref-guide/#the-set-default-dictionary-language-statement","text":"{: #aql-def-dict} The set default dictionary language statement allows an extractor developer to customize the default set of dictionary-matching languages for the containing module.","title":"The set default dictionary language statement"},{"location":"aql-ref-guide/#syntax_4","text":"{: #aql-def-dict-syntax} set default dictionary language as '<language codes>';","title":"Syntax"},{"location":"aql-ref-guide/#description_4","text":"{: #aql-def-dict-examp} <language codes> Specifies the language for compilation and matching for dictionaries of the module that are declared without an explicit with language as specification. The <language codes> set must be a comma-separated list, with no white spaces around each language code. Failure to observe this requirement can result in a compilation error. This statement affects the following dictionaries: Dictionaries that are explicitly declared in the current module by using the create dictionary or create external dictionary statement, and that statement does not have a with language as clause. Dictionaries from external files. In a pattern specification of an extract pattern statement, atoms of type 'string' and atoms of type <'string' [match parameters]> without an explicit with language as specification. When this statement is absent from inside a module, the runtime component defaults to a language set of German, Spanish, English, French, Italian, and the unspecified language x. It is defined as a set: [de,es,en,fr,it,x_unspecified] . Within a module, only one instance of this statement can exist.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_4","text":"{: #aql-def-dict-usage} The set default dictionary language statement can be updated to improve the extent of languages that are covered by the extractor. This ability to add languages promotes ease of customization and the reuse of existing extractors.","title":"Usage notes"},{"location":"aql-ref-guide/#examples_4","text":"{: #aql-reference-examples-5} Example 1: Specifying languages to be used to match dictionary entries module Dictionaries; -- Set the default dictionary matching language -- for this module to English and French set default dictionary language as 'en,fr'; /** * Dictionary of English and French names. Because no language clause * exists in dictionary definition, module level dictionary matching * setting will be applied. */ create dictionary EnglishFrenchNames from file 'en_fr_names.dict'; /** * Dictionary of Italian names. Language clause in the dictionary * definition will override the module level dictionary matching setting. */ create dictionary ItalianNames with language as 'it' as ( 'firstEntry','secondEntry' ); /** * View to extract pattern: Phone, followed by one to three tokens, * followed by Email. Language clause at the atom level will override * the module level dictionary setting. */ create view PhoneEmailPattern as extract pattern <'Phone'[ with language as 'en']> <Token> {1,3} 'Email' as match from Document D; output view PhoneEmailPattern;","title":"Examples"},{"location":"aql-ref-guide/#the-require-document-with-columns-statement","text":"{: #aql-req-doc-columns} By using the require document with columns statement, you can define the schema of the special view Document at compile time. This schema definition specifies the list of required fields and their types to be found in each tuple of the view Document .","title":"The require document with columns statement"},{"location":"aql-ref-guide/#syntax_5","text":"{: #aql-req-doc-syntax} require document with columns <columnName> <columnType> [and <columnName> <columnType>]*;","title":"Syntax"},{"location":"aql-ref-guide/#description_5","text":"{: #aql-req-doc-desc} <columnName> Specifies the name of the column to be used in the schema. The <columnName> is an attribute identifier, which is a simple identifier or a double-quoted identifier. <columnType> Specifies the type of column to be used in the schema of the Document view. The <columnType> can be one of the following data types: Integer, Float, Boolean, or Text. [ and <columnName> <columnType> ]* Specifies additional column names and types to the schema of the Document view. They follow the same rules as <columnName> and <columnType> . In earlier versions of AQL, the schema of the special view Document was predefined to consist of either a single field (text Text) or two fields (text text, label Text). The choice between these schemas was decided at run time. By using the require document with columns statement, you can override the default input document schema at compile time.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_5","text":"{: #aql-req-doc-usage} For modular AQL code, the scope of any require document with columns statement is the module in which it is defined. Only one require document with columns statement is allowed per AQL file. Within a single module, or a generic module, there can be zero, one, or multiple AQL files that have a require document with columns statement. All AQL files within a module merge their require document with columns statements at the level of the entire module to form a module-wide require document with columns . This statement defines the schema of the Document view for that module. If none of the AQL files of a module or a generic module contain a require statement, the module has a default schema for the view Document . This schema consists of two columns: (text Text, label Text). No default columns are established for the special view Document if at least one AQL file in the module has one require document with columns statement. When multiple modules are combined to form an extractor, the schema of the Document view of the entire extractor is defined by the duplicate-free union of Document schemas for each module. An exception is raised when any column found across the multiple require document with columns statements is found to be conflicting in its type requirements across modules. For example, a module requires a column X with type Y when another module that is being loaded with it requires a column X with type Z. An exception is raised when you run an extractor if the provided input document tuple does not contain all of the required columns. An exception is also raised if a column does not conform to its corresponding required type. When the require document with columns statement is present inside a module, every column of the special view Document that is referenced must be declared in at least one of the require document with columns statements. The statement can be found in different AQL files within the same module. However, all such require document with columns statements would be merged at the level of the module to form a module-wide require document with columns statement.","title":"Usage notes"},{"location":"aql-ref-guide/#examples_5","text":"{: #aql-reference-examples-6} Example 1: Require document statement with similar column types The following example defines a document schema that contains four fields of the same type. This AQL sample expects each document tuple of the data collection to contain four columns as defined in the document schema. Refer to JSON document formats for details on how to create a document that conforms to a schema. module person; -- Require document statement with similar field types require document with columns inputDocumentText Text and inputDocumentLabel Text and documentAuthor Text and documentCreationDate Text; Example 2: Require document statement with varying column types The following sample defines a document schema that contains columns of varying field types. module sample; -- Require document statement with varying field types require document with columns bookText Text and purchaseCount Integer and bookPrice Float and isBookPopular Boolean; Example 3: Merging document schemas The example describes how to merge document schemas that use the files first.aql, last.aql, and socialsecurity.aql. first.aql: module person; require document with columns firstName Text; last.aql: module person; require document with columns lastName Text; socialsecurity.aql module person; require document with columns lastName Text and socialSecurityNo Integer; The merged schema is (firstName Text, lastName Text, socialSecurityNo Integer).","title":"Examples"},{"location":"aql-ref-guide/#the-create-view-statement","text":"{: #aql-create-view} The top-level components of an AQL extractor are its views. Views are logical statements that define, but do not necessarily compute, a set of tuples.","title":"The create view statement"},{"location":"aql-ref-guide/#syntax_6","text":"{: #aql-create-view-syntax} The create view statement can take one of three forms. The simplest form defines a logical view that is composed of the tuples of a single select or extract statement. The second is a multijoin form that defines a view that comprises the tuples that arise from the multiset union of several select or extract statements. The third form defines a new view that contains the set difference between tuples from two select or extract statements. create view <viewname> as <select or extract statement>; create view <viewname> as (<select or extract statement>) union all (<select or extract statement>)...; create view <viewname> as (<select or extract statement>) minus (<select or extract statement>);","title":"Syntax"},{"location":"aql-ref-guide/#description_6","text":"{: #aql-create-view-desc} <viewname> The <viewname> can be a simple identifier or a double-quoted identifier. It cannot contain the period character. <select or extract statement> The select or extract statement creates output that is used to compute the tuples of the containing view.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_6","text":"{: #aql-create-view-usage} View names are case-sensitive. For example, Person, PERSON, and person are different view names Two views within the same AQL module cannot share a name, which would make them duplicates. However, two views with same name can exist in two different modules, since their fully qualified names are unique. By default, a view that is defined by the create view statement is a non-output view until it is specified as an output view. The select or extract statements of the union all and the minus forms, must have a compatible output schema. Two schemas are considered compatible for a union or minus operation if they have the same number of columns, the column names are in the same order, and they have compatible data types: Fields of the same data type are union or minus compatible. The Span and Text data types are union or minus compatible. In the case of a union between a Span and a Text type, the output type is a Span. However, objects of Text type are not automatically converted into a Span type \u2013 the automatic conversion happens only when required by function calls. Two ScalarLists are union or minus compatible regardless of the underlying scalar type.","title":"Usage notes"},{"location":"aql-ref-guide/#examples_6","text":"{: #aql-reference-examples-7} Example 1: Creating a view with a select or extract statement In the example below, the view Phone uses an extract statement to prepare its tuples. The view PhoneNumber uses a select statement to pick specific fields from the view Phone . create view Phone as extract regexes /\\+?([1-9]\\d{2}\\)\\d{3}-\\d{4}/ and /\\+?[Xx]\\.?\\d{4,5}/ on D.text as num from Document D; create view PhoneNumber as select P.num as num, LeftContextTok(P.num, 3) as lc from Phone P; Example 2: Creating a view with Union All statement The view AllPhoneNums prepares a unionized set out of the tuples of Phone and Extension views. The two views that are being unionized have the same schema. create view Phone as extract regex /\\+?([1-9]\\d{2}\\)\\d{3}-\\d{4}/ on D.text as match from Document D; create view Extension as extract regex /\\+?[Xx]\\.?\\d{4,5}/ on D.text as match from Document D; create view AllPhoneNums as (select P.match from Phone P) union all (select E.match from Extension E); Example 3: Creating a view with Minus statement The following example shows how you can use minus to filter out unwanted tuples from a set of tuples. create view Organization as (select * from OrganizationCandidates) minus (select * from InvalidOrganizations); Example 4: Schema compatibility for minus It is important to note that spans over different target text are not of the same type. Consider the following AQL example that explains this difference by using a String literal. create view OneString as select 'a string' as match from Document D; create view TheSameString as select 'a string' as match from Document D; create view Subtraction as (select R.match from OneString R) minus (select R.match from TheSameString R); Instead of the expected output of an empty tuple list, the output is a set of records that have 'a string' as a field value. Although the contents of the OneString and TheSameString views seem identical, the actual text values have different underlying AQL objects. The type of OneString.match is 'Span over OneString.match'. The type of TheSameString.match is 'Span over TheSameString.match'. Since the field types are different, they are not compatible for comparison purposes. To obtain the wanted output of the empty tuple list, you must compare values of the same type. In the following example, the GetString() function converts span objects to string objects to pass compatible types to the minus operation. create view Subtraction as (select GetString(R.match) from OneString R) minus (select GetString(R.match) from TheSameString R);","title":"Examples"},{"location":"aql-ref-guide/#documenting-the-create-view-statement-with-aql-doc","text":"The AQL Doc comment for a create view statement contains the following information: General description about the view. @field for every column name in the view. Example /** * Extracts all spans that match a phone number pattern from * the input documents. It uses a regular expression to match * phone number patterns. * @field num phone number * @field lc 3 tokens to the left of phone number*/ create view PhoneNumber as select P.num as num, LeftContextTok(P.num, 3) as lc from ( extract regexes /\\+?([1-9]\\d{2}\\)\\d{3}-\\d{4}/ and /\\+?[Xx]\\.?\\d{4,5}/ on D.text as num from Document D ) P;","title":"Documenting the create view statement with AQL Doc"},{"location":"aql-ref-guide/#the-output-view-statement","text":"{: #aql-output-view} The output view statement defines a view to be an output view. The runtime component outputs only the tuples of views that are marked as output views. The AQL compiler compiles only views that are marked as output or export, or reachable from views that are marked as output or export.","title":"The output view statement"},{"location":"aql-ref-guide/#syntax_7","text":"{: #aql-output-view-syntax} output view <view-name> [as '<alias>'];","title":"Syntax"},{"location":"aql-ref-guide/#description_7","text":"{: #aql-output-view-desc} <view-name> The name of the view to output, as known in the name space of the current module. The <view-name> is a simple identifier or a double-quoted identifier. The built-in view Document cannot be output. [as '<alias>']* Defines an <alias> name for the output view. When the optional alias is not specified, the view is output under the following names: In modular AQL, the view is output under the name <module-name>.<view-name> where <module-name> is the name of the module where the view is originally defined (which can be different from the module where the view is output). In AQL 1.4 or earlier, the view is output under the name <view-name> The output ... as <alias> statement is useful when you customize an extractor for different domains. The use of the <alias> name when you define an output view ensures that output names are identical across different implementations of the customization. The <alias> name cannot be used in another select or export statement. You must enclose the <alias> with single quotation marks, and the <alias> name can contain periods.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_7","text":"{: #aql-output-view-usage} When an AQL extractor is run, it computes resultant tuples for each view that is defined as an output view. The tuples of any non-output view are also computed, but only when they are necessary to compute the resultant tuples of an output view. In modular AQL, the output view statement outputs the tuples of the view under the fully qualified view name that is qualified by its module name. Consider the following example where the statement output view Person; results in the output of the personModule.Person view: module personModule; create view Person as extract dictionary 'FamousPeopleDict' on D.text as match from Document D; output view Person; This behavior applies for any view output without an alias, regardless of whether the view is output in the module where it is defined, or a module where it is imported, even when it is imported by using an alias import. For example, in the output view MyPerson , this example results in the view being output under its original qualified name personModule.Person and not under its local alias MyPerson module employeeModule; import view Person from module personModule as MyPerson; output view MyPerson; The output alias statement is useful when you build libraries of extractors where the same type of entity can have many different implementations depending on the application domain or the language of the input documents. The main benefit of using an alias when you define an output view is to ensure a consistent nomenclature across output views. Consistent nomenclature is expected by the program logic of a user when you process multiple modules that each output a semantically similar view. In modular AQL, when an alias name is used in an output view statement, the tuples of a view are output under the specified alias name. For example, the following code would output the results under the alias name PersonAlias, and the alias name is not qualified with the module prefix. module personModule; create view Person as extract dictionary 'FamousPeopleDict' on D.text as match from Document D; output view Person as 'PersonAlias';","title":"Usage notes"},{"location":"aql-ref-guide/#examples_7","text":"{: #aql-reference-examples-8} The following examples contain two modules, personModuleFrench and personModuleEnglish. Each module outputs a view, named PersonNameFrench and PersonNameEnglish. Suppose similar modules, each of which outputs views that are semantic variants of an extractor for person names. These modules are customized for different languages with the variance in the customization of this view for a specified input language. Eventually, a user might want a program to use modules where the sought output view is named PersonName, irrespective of the modules that are processed. This expectation is normal, since each module that is customized for a language, domain, or another purpose is expected to produce various results. The consumer of these modules does not need to alter the algorithm of their program to accommodate for varying output view names when the underlying semantics are similar. In the example, because the alias PersonName is used, the consumer does not need to alter the view name that is sought. However, the results might vary depending on the modules that are processed. In the example, for instance, the resulting matches are French-based (Example 1) and English-based (Example 2). Example 1: Resulting matches are French-based The following example defines a view PersonNameFrench and outputs it under an implementation-neutral alias name, 'PersonName' . module personModuleFrench; create dictionary FrenchNames as ('Jean', 'Pierre', 'Voltaire', 'Francois', 'Marie', 'Juliette'); create view PersonNameFrench as extract dictionary 'FrenchNames' on D.text as name from Document D; output view PersonNameFrench as 'PersonName'; Example 2: Resulting matches are English-based The following example defines a view PersonNameEnglish and outputs it under an implementation-neutral alias name, 'PersonName' . module personModuleEnglish; create dictionary EnglishNames as ('John', 'Peter', 'Andrew', 'Francis', 'Mary', 'Juliet'); create view PersonNameEnglish as extract dictionary 'EnglishNames' on D.text as name from Document D; output view PersonNameEnglish as 'PersonName'; The consumer of the example modules can access the output tuples through the alias name 'PersonName' . The consumer would not need to know the actual module from which the results are fetched.","title":"Examples"},{"location":"aql-ref-guide/#the-extract-statement","text":"{: #aql-extract} The extract statement is used to extract basic features directly from text.","title":"The extract statement"},{"location":"aql-ref-guide/#syntax_8","text":"{: #aql-extract-syntax} extract <select list>, <extraction specification> from <from list> [having <having clause>] [consolidate on <column> [using '<policy>']] [limit <maximum number of output tuples for each document>];","title":"Syntax"},{"location":"aql-ref-guide/#description_8","text":"{: #aql-extract-desc} <select list> A comma-delimited list of output expressions. The results of these output expressions are returned as the output of the extract statement, along with the tuples that are generated by the evaluation of the extraction specification. The format for the <select list> is the same as the <select list> of a select statement. <extraction specification> Applies the extraction specification over all tuples from the views that are defined in the <from list> . It renames the columns of tuples according to their corresponding aliases that are specified in the extract statement. You can use one of the following extraction specifications: Regular expressions Dictionaries Splits Blocks Part of speech Sequence patterns <from list> A comma-delimited list that is the source of the tuples from which features are to be selected. The format of the <from list> is similar to the format of the <from list> of the select statement. However, if the extract statement does not have a pattern specification, then the <from list> can contain a single item. [having <having clause>] Specifies a filtering predicate (in the <having clause> ) that is applied to each extracted output tuple. The field names that are specified in the <having clause> refer to any aliases that are specified in the <select list> . This clause is optional. [consolidate on <column>[using '<policy>' ]] Defines how to handle overlapping spans as defined in the consolidation <policy> . In this specification, <column> must be the name of an output field, which is part of the extract statement. This clause is optional. [limit<maximum number of output tuples for each document>] Limits the number of output tuples from each document to the specified maximum. This clause is optional.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_8","text":"{: #aql-extract-usage} The semantics of the extract statement are as follows: Evaluate the extraction specification over each tuple of the input relation. For each result that the extraction produces, an output tuple that contains the extracted values, along with any columns of the original tuple that were specified in the <select list> , is produced. Rename the columns of the output tuple according to the aliases specified as part of the <select list> and the <extraction specification> . Apply any predicates in the optional having clause to the resulting output tuple. Consolidate tuples that pass the predicates according to the optional consolidation clause, and add the resulting tuples to the output. If the optional limit clause is present, limit the output to the specified number of tuples for each document. The semantics of the from clause of an extract pattern statement are different from other forms of extract statements that do not have a pattern specification. If at least one of the views in the <from list> does not contain any tuples on a particular document, then the output of the extract statement is empty. This output is empty because the set of all combinations of tuples in the input views is empty. In the special case of extract pattern statements, the from clause is a placeholder that declares the names of relations that are involved in the pattern specification. The semantics of the statement are driven only by the pattern specification. In particular, the output of the statement can be non-empty even when some of the input views are empty.","title":"Usage notes"},{"location":"aql-ref-guide/#examples_8","text":"{: #aql-reference-examples-9} Example 1: Extracting phone numbers from a pre-defined view This sample extract statement evaluates a regular expression for United States phone numbers across input text that is represented by the pre-defined view Document . The output is then restricted to the first three phone numbers that are identified per each document. Field names in the having clause refer to the aliases at the beginning of the extract statement. create view PhoneNumbers as extract D.text as documentText, regex /\\d{3}-\\d{3}-\\d{4}/ on D.text as phoneNumber from Document D having MatchesRegex(/\\d{3}.*/, phoneNumber) limit 3; Example 2: Extracting blocks of capitalized words In this example, the extract statement identifies blocks of two to three capitalized words. In AQL, a block refers to a contiguous span of tokens, in this case two to three tokens. This example also uses a consolidation policy to exclude blocks that are contained within larger blocks from the output set of tuples. create view CapitalizedWord as extract regex /[A-Z][a-z]*/ with flags 'CANON_EQ' on 1 token in D.text as word from Document D; create view TwoToThreeCapitalizedWords as extract blocks with count between 2 and 3 and separation 0 tokens on CW.word as capswords from CapitalizedWord CW consolidate on capswords using 'ContainedWithin'; The consolidate clause is applied to the capswords field by the extract blocks specification. The difference is that the target field referred to by the consolidation clause is an output field of the extract statement. The target field of the select statement is an input field. This behavior is similar to that of the having clause. Example 3: A nested extract or select statement as a view name The input view for an extract statement can be either a view name, as in Example 2, or a nested extract or select statement, as in this example: create view SampleExtract as extract regex /foo/ on E.foobar as foo from (extract regex /foobar/ on D.text as foobar from Document D) E; Example 4: Extract a statement with a select list In this example, we extract matches for a pattern, and at the same time, select multiple attributes from the input views. create view Person as extract.F.first as first, M.initial as middle, L.last as last pattern ('Mr.'|'Ms.'|'Miss')? (<F.first> <M.initial>? <L.last>) return group 0 as reference and group 1 as salutation and group 2 as name from FirstName F, MiddleInitial M, LastName L; Regular expressions Use a regular expression extraction specification to identify matching patterns that are contained by the regular expression across the input text. Dictionaries Use the dictionary extraction specification to extract strings from input text that are contained in a dictionary of strings. Splits Use the split extraction specification to split a large span into several smaller spans. Blocks Use the blocks extraction specification to identify blocks of contiguous spans across input text. Part of speech Use the part-of-speech extraction specification to identify locations of different parts of speech across the input text. Sequence patterns Use the pattern extraction specification to perform pattern matching across an input document and other spans that are extracted from the input document.","title":"Examples"},{"location":"aql-ref-guide/#regular-expressions","text":"{: #aql-regx} Use a regular expression extraction specification to identify matching patterns that are contained by the regular expression across the input text.","title":"Regular expressions"},{"location":"aql-ref-guide/#syntax_9","text":"{: #aql-regx-syntax} regex[es] /<regex1>/ [and /<regex2>/ and ... and /<regex n>/] [with flags '<flags string>'] on <token spec>] <name>.<column> <grouping spec>","title":"Syntax"},{"location":"aql-ref-guide/#description_9","text":"{: #aql-regx-desc} regex[es] /<regex1>/ Specifies the regular expressions to be used in the extraction. By default, AQL uses Perl syntax for regular expressions, which means that regular expression literals are enclosed in two forward slash (//) characters. The regular expression syntax is the same as the syntax of Java\u2122 class java.util.regex.Pattern , except that regular expression escape sequences take precedence over other escape characters. AQL also allows regular expressions in SQL string syntax, so a regular expression for United States phone numbers can be expressed as either of the examples: bash /\\d{3}-\\d{5}/ bash '\\\\d{3}-\\\\d{5}' [and /<regex2>/ and ... and /<regex n>/ ] Lists more regular expressions to be used in the extraction. [with flags '<flags string>'] Specifies a combination of flags to control regular expression matching. This parameter is optional. These flags correspond to a subset of the flags that are defined in the Java\u2122 implementation, see class java.util.regex.Pattern . If the flags string is not provided, AQL uses only the DOTALL flag by default. To specify multiple flags, separate them with the | character. For example, to specify multiline matching, matching that is not case-sensitive, and Unicode case folding, use the flags string 'MULTILINE|CASE_INSENSITIVE|UNICODE' . [<token spec>] Indicates whether to match the regular expression only on token boundaries. bash ... [[between <number> and] <number> token[s] in] ... Token constraints are an optional part of the specification. If token constraints are omitted, AQL returns the longest non-overlapping match at each character position in the input text. If token constraints are present, the extract statement returns the longest match at every token boundary that is within the specified range of tokens in length. Each returned match must start at the beginning of a token and end at the end of a token. If multiple overlapping matches exist, the extract statement returns all of them. The locations of token boundaries depend on what tokenizer the runtime component is using to tokenize the document. If the engine is using the Standard tokenizer, then a token is defined as a sequence of word characters or a single punctuation character. For example, consider the string: bash \"The fish are pretty,\" said the boy. The token boundaries are identified at these locations: bash [\"][The] [fish] [are] [pretty][,][\"] [said] [the] [boy][.] <name>.<column> The view name and column name on which the regular expression is applied. <grouping spec> Determines how to handle the capturing of groups in the regular expression. Capturing groups are regions of the regular expression match that are identified by parentheses in the original expression. Capturing groups are numbered by counting their opening parentheses from left to right. Each group captures the portion of the regular expression match starting at the corresponding open parenthesis ( until its corresponding end parenthesis ) . For example, the expression ((fish)(cakes) and fries) has three capturing groups: Group 0 is the entire match fishcakes and fries . Group 1 is also fishcakes and fries , corresponding to the first open parenthesis ( . This group happens to encompass the entire match, since its corresponding end parenthesis ) is at the end of the expression. Group 2 is fish , corresponding to the second open parenthesis ( . Group 3 is cakes , corresponding to the third open parenthesis ( . When you specify group IDs in the return clause of the syntax, each group ID must correspond to a valid group within each regular expression specified as part of the extract regex clause in the syntax. This is the format of the grouping specification: bash return group <number> as <name> [and group <number> as <name>]* Non-capturing groups in the regular expression (groups starting with (?: ) do not count towards the group numbers. For example, in (?:fish)(cakes) there is a single group, capturing cakes . The group spanning fish is non-capturing, and does not count towards the number of groups. For more details about semantics of capturing groups, see Java\u2122 class java.util.regex.Pattern . To return only Group 0 (the entire match), you can use a shorter, alternative format as in Example 1. This format is equivalent to return group 0 as <name> . <name> can be a simple identifier or a double-quoted identifier.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_9","text":"{: #aql-regx-usage} In general, AQL supports the same features as the Java\u2122 8 regular expression implementation, as described in Java class java.util.regex.Pattern . The runtime component contains several regular expression engine implementations, including the built-in implementation of Java. During compilation, the Optimizer examines each regular expression and chooses the fastest engine that can run the expression. The alternative execution engines might have slightly different semantics for certain corner cases. In particular, AQL does not guarantee the order in which alternatives are evaluated. For example, suppose that an extract statement matches the regular expression /fish|fisherman/ , over the text 'fisherman'. The statement might match either 'fish' or 'fisherman', depending on which regular expression engine is used internally. AQL flag string Java flag Description CANON_EQ CANON_EQ Canonical equivalence: Different Unicode encodings of the same character are considered equivalent. CASE_INSENSITIVE CASE_INSENSITIVE Perform matching that is not case-sensitive.By default, case-insensitive matching assumes that only characters in the US-ASCII character set are matched. Unicode-aware case-insensitive matching can be enabled by specifying the UNICODE flag with this flag. UNICODE UNICODE_CASE If case-insensitive matching is specified, use Unicode case folding to determine whether two characters are equivalent in a case-insensitive comparison in a manner that is consistent with the Unicode Standard. By default, case-insensitive matching assumes that only characters in the US-ASCII character set are matched. Note: The behavior of this flag is not defined when it is used without the CASE_INSENSITIVE flag. DOTALL DOTALL Make the dot character . match all characters, including newlines. LITERAL LITERAL Treat the expression as a sequence of literal characters, ignoring the normal regular expression escape sequences. MULTILINE MULTILINE Make the characters ^ and $ match the beginning and end of any line, as opposed to the beginning and end of the entire input text. UNIX_LINES UNIX_LINES Treat only the UNIX\u2122 newline character \\n as a line break, ignoring the carriage return character \\r . Follow these guidelines to make your extractors run faster and be easier to maintain: Avoid long, complex regular expressions and use simpler, smaller regular expressions that are combined with AQL statements. Avoid unnecessary use of lookahead and lookbehind in regular expressions. You can usually achieve the same effect by adding predicates to the having clause of your extract statement. Use token constraints in your regular expression extraction specifications when possible.","title":"Usage notes"},{"location":"aql-ref-guide/#examples_9","text":"{: #aql-reference-examples-10} Example 1: Using canonical Unicode equivalence to determine matches This example shows you how to find capitalized words that are not given names. Canonical Unicode character equivalence is used to determine matches. Notice the usage of the flag \u2018CANON_EQ\u2019 and that regex is performed on tokens: create dictionary FirstNamesDict as ( 'Aaron', 'Matthew', 'Peter' ); create view NotFirstName as extract regex /[A-Z][a-z]*/ with flags 'CANON_EQ' on 1 token in D.text as word from Document D having Not(ContainsDict('FirstNamesDict', word)); Example 2: Usage of capturing groups The following example demonstrates the usage of capturing groups in an extract regex statement. The code extracts the fields of a United States phone number by using capturing groups: create view Phone as extract regex /(\\d{3})-(\\d{3}-\\d{4})/ on between 4 and 5 tokens in D.text return group 1 as areaCode and group 2 as restOfNumber and group 0 as fullNumber from Document D; Example 3: Applying multiple regex over input text You can specify multiple regular expressions in the same extract regex statement by using the regexes syntax. create view PhoneNum as extract regexes /(\\d{3})-(\\d{3}-\\d{4})/ and /[Xx]\\d{3,5}/ on between 1 and 5 tokens in D.text as num from Document D; Example 4: Misuse of grouping specification The regular expression in this code sample does not contain group -1 or group 3000 . This results in a compilation error. create view ErrorExample as extract regex /(\\d+)/ on D.text return group -1 as wrongGroup and group 3000 as nonExistentGroup from Document D;","title":"Examples"},{"location":"aql-ref-guide/#dictionaries","text":"{: #aql-reference-dictionaries} Use the dictionary extraction specification to extract strings from input text that are contained in a dictionary of strings.","title":"Dictionaries"},{"location":"aql-ref-guide/#syntax_10","text":"{: #aql-ref-dict-syntax} dictionar[y|ies] '<dictionary>' [and '<dictionary>' and ... and '<dictionary>'] [with flags '<flags string>']","title":"Syntax"},{"location":"aql-ref-guide/#description_10","text":"{: #aql-ref-dict-desc} '<dictionary>' References a dictionary that is created by using either the create dictionary statement, create external dictionary statement, or a dictionary file on the file system. [and '<dictionary>' and ... and '<dictionary>'] References additional dictionaries to be used for extraction. [with flags'<flags string>'] Controls dictionary matching. Currently, two options are supported: Exact Provides exact, case-sensitive matching. IgnoreCase Provides matching that is not case-sensitive. If no flag is specified, the dictionary matches based on any flag that was specified when it was created. If no flag is specified during creation, it matches by using the IgnoreCase flag.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_10","text":"{: #aql-ref-dict-usage} Dictionaries are always evaluated on token boundaries. Specifically, a dictionary entry matches a region of text if the first token of the entry matches the first token of the text region, the second token of the entry matches the second token of the text region, and so on. Characters between two consecutive tokens are ignored. For example, assume that you are using a simple white-space based tokenization model that is appropriate for a language such as English. Also, assume that the input text is \u201cLet\u2019s go fishing!\u201d If a dictionary consists of the term go fish , no match exists in the text for Let's go fishing! . However, if the dictionary consists of the entry go fishing (notice the two white spaces between go and fishing), one match exists in the text Let's go fishing! . White space specifies that go and fishing are two distinct tokens. If one or more white-space characters exists between the two tokens go and fishing in the input text, a match is made. For each match of a dictionary entry with input text, the extract dictionary statement generates an output tuple.","title":"Usage notes"},{"location":"aql-ref-guide/#examples_10","text":"{: #aql-reference-examples-11} Example 1: Extracting terms from dictionary files Find person names by using dictionary files of common given names and surnames with case-sensitive matching. create view Name as extract dictionaries 'first.dict' and 'last.dict' with flags 'Exact' on D.text as name from Document D; The following is sample content of last.dict : #Dictionary for surnames Anthony Aparicio Cate Lehmann Radcliff The following is sample content of first.dict : #Dictionary for given names Aaron Candra Freeman Mathew Matthew Zoraida Note: A dictionary file system that is directly referenced in an extract dictionary statement cannot be explicitly configured with a set of languages so that the dictionaries are compiled and applied at run time. Instead, the set of languages is specified with the set default dictionary language statement if the module contains this statement. Therefore, direct referencing of dictionary files in an extract dictionary statement is not recommended and might be discontinued in the future. The preferred practice is to explicitly define a dictionary object by using the statement create dictionary from file , and then use that dictionary in the extract statement. The compiler and runtime component attempt to locate dictionary files that are referenced in AQLs under the configured search path. Example 2: Extracting terms from an inline dictionary Find conjunctions by using an inline dictionary and the default matching that is not case-sensitive. create dictionary ConjunctionDict as ( 'and', 'or', 'but', 'yet' ); create view Conjunction as extract dictionary 'ConjunctionDict' on D.text as name from Document D;","title":"Examples"},{"location":"aql-ref-guide/#splits","text":"{: #aql-splits} Use the split extraction specification to split a large span into several smaller spans.","title":"Splits"},{"location":"aql-ref-guide/#syntax_11","text":"{: #aql-splits-syntax} split using <name>.<split point column> [retain [right|left|both] split point[s]] on <name>.<column to split> as <output name>","title":"Syntax"},{"location":"aql-ref-guide/#description_11","text":"{: #aql-splits-desc} The split extraction specification takes two arguments: A column that contains longer target spans of text. A column that contains split points. The splitting algorithm works in two passes over the input view. The first pass groups all of the input tuples by the target column. The second pass goes through the tuples in each group, splitting the target column with each value of the splitting column. <name>.<split point column> Specifies the split points for the extraction. [retain [right|left|both] split point[s]] Specifies how to treat the left and right end points of each result. This argument is optional. If retain left split point is specified, then each output span also contains the split point to its left, if such a split point exists. If retain right split point is specified, then the system makes each output span contain the split point to its right. The split extraction also accepts null values as split points. For each such value, the extraction returns a tuple that contains the entire input span. <name>.<column to split> Specifies the target column for the extraction. <output name> Defines the name of the output of the extraction.","title":"Description"},{"location":"aql-ref-guide/#examples_11","text":"{: #aql-reference-examples-12} Example 1: Split points and the retain clause If the split points are all the instances of the word fish in the phrase fish are swimming in the fish pond, then the various versions of the retain clause have the following effects: retain clause omitted \" are swimming in the \" and \" pond\" retain right split point \" are swimming in the fish\" and \" pond\" retain left split point \"fish are swimming in the \" and \"fish pond\" retain both split points \"fish are swimming in the fish\" and \"fish pond\" Example 2: Split extraction This example splits the document into sentences. It first uses a regular expression to identify sentence boundaries, then uses the split extraction specification to split the document text on the sentence boundaries. create dictionary AbbreviationsDict as ( 'Cmdr.', 'Col.', 'DR.', 'Mr.', 'Miss.'); create view Sentences as extract split using B.boundary retain right split point on B.text as sentence from ( extract D.text as text, regex /(([\\.\\?!]+\\s)|(\\n\\s*\\n))/ on D.text as boundary from Document D -- Filter the candidate boundaries. having Not(ContainsDict('AbbreviationsDict', CombineSpans(LeftContextTok(boundary, 1), boundary))) ) B;","title":"Examples"},{"location":"aql-ref-guide/#blocks","text":"{: #aql-blocks} Use the blocks extraction specification to identify blocks of contiguous spans across input text.","title":"Blocks"},{"location":"aql-ref-guide/#syntax_12","text":"{: #aql-blocks-syntax} blocks with count [between <min> and] <max> and separation [between 0 and] <max> (tokens| characters) on <name>.<column containing spans> as <output name>","title":"Syntax"},{"location":"aql-ref-guide/#description_12","text":"{: #aql-blocks-desc} with count [between<min> and] <max> Specifies how many spans can make up a block. The <min> and <max> values specify the minimum and maximum number of spans that can make up a block. [between 0 and] <max> Specifies the separation distance that is allowed between spans before they are no longer considered to be contiguous. (tokens| characters) Specifies whether the separation distance of the span represents the number of tokens or the number of characters. <name>.<column containing spans> The view name and column name on which the block operator is to be applied. <output name> Specifies a name for the output from the block operator.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_11","text":"{: #aql-blocks-usage} If the input scans contain multiple overlapping blocks in the input spans, a block extraction statement returns all possible blocks. Use consolidation to filter out redundant blocks. An extract statement with block extraction specification yields blocks that each consist of an aggregation of values of a certain field from across multiple input tuples. Therefore, its select list cannot include fields from its input view.","title":"Usage notes"},{"location":"aql-ref-guide/#examples_12","text":"{: #aql-reference-examples-13} Example 1: Extract blocks of words within a character range In the following code, the view TwoToThreeCapitalizedWords identifies blocks of two to three capitalized words within 100 characters of each other. create view CapitalizedWords as extract regex /[A-Z][a-z]*/ with flags 'CANON_EQ' on 1 token in D.text as word from Document D; create view TwoToThreeCapitalizedWords as extract blocks with count between 2 and 3 and separation between 0 and 100 characters on CW.word as capswords from CapitalizedWords CW; Example 2: Extract blocks of words within a token range The following code identifies blocks of exactly two capitalized words within five tokens of each other. create view TwoCapitalizedWords as extract blocks with count 2 and separation between 0 and 5 tokens on CW.word as capswords from CapitalizedWords CW;","title":"Examples"},{"location":"aql-ref-guide/#part-of-speech","text":"{: #aql-pos} Use the part-of-speech extraction specification to identify locations of different parts of speech across the input text.","title":"Part of speech"},{"location":"aql-ref-guide/#syntax_13","text":"{: #aql-pos-syntax} part_of_speech '<part of speech spec>' [and '<part of speech spec>']* [with language '<language code>'] [and mapping from <mapping table name>] on <input column> as <output column> from <input view>","title":"Syntax"},{"location":"aql-ref-guide/#description_13","text":"{: #aql-pos-desc} '<part of speech spec>' Identifies the parts of speech to extract from the input text. The '<part of speech spec>' is one of the following strings: A combination of an internal part-of-speech name and flags, as defined by a mapping table [and '<part of speech spec>']* Identifies the additional parts of speech tags for extraction. [with language '<language code>'] Specifies the language to be used in the extraction. The <language code> is a two-letter, lowercase language code, such as 'en' or 'ja'. If this argument is omitted, the language for part-of-speech extraction is assumed to be English [and mapping from <mapping table name>] Specifies the name of an AQL table that maps raw part-of-speech tags such as \"NOUN\" to combinations of high-level parts of speech and flags. While the optional mapping table can have variable names, a part-of-speech mapping table is required to have these column names: tag The column that holds a part-of-speech tag generated by your tokenizer. basetag The column that holds the corresponding internal tag. flagstr The column that holds a comma-delimited list of flags that are associated with the indicated part of speech. The mapping table must be defined by using the create table statement in the same module as the extract part_of_speech statement that uses it. It cannot be an imported table, and it cannot be an external table. bash create table POSMapping_EN(tag Text, basetag Text, flagstr Text) as values ('CCONJ','CONJ','coordinating'), ('SCONJ','CONJ','subordinating'); <input column> Specifies the column of the input view from which to extract part-of-speech information. <output column> Specifies the name of the column where the spans of the tokens with the indicated parts of speech are sent. <input view> Specifies the input view from which to extract part-of-speech information.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_12","text":"{: #aql-pos-usage} Part of speech extraction is not provided by the built-in tokenizer. If the system uses the Standard tokenizer, a part_of_speech extraction generates an error.","title":"Usage notes"},{"location":"aql-ref-guide/#examples_13","text":"{: #aql-reference-examples-14} Example 1: Using a part of speech tag directly in an extract statement The view EnglishNoun extracts English nouns (singular or mass) or proper nouns (singular). create view EnglishNoun as extract parts_of_speech 'NOUN' and 'PROPN' with language 'en' on D.text as noun from Document D;","title":"Examples"},{"location":"aql-ref-guide/#sequence-patterns","text":"{: #aql-seq-patterns} Use the pattern extraction specification to perform pattern matching across an input document and other spans extracted from the input document.","title":"Sequence patterns"},{"location":"aql-ref-guide/#syntax_14","text":"{: #aql-seq-ptn-syntax} The general syntax of a sequence pattern is to first specify the pattern to be matched in the text, and then to specify what is to be returned by the extractor. The final part of the sequence pattern specifies what is the input to the pattern; it might be a column from a previously defined view, or it might be the entire document text. pattern <pattern specification> [return clause] [with inline_match on <viewname.colname>]","title":"Syntax"},{"location":"aql-ref-guide/#description_14","text":"{: #aql-seq-ptn-desc} <pattern specification> A <pattern specification> is composed of multiple Atoms. An individual Atom can be a column from an already-defined view, a fixed string, or a regular expression. You can specify your Atoms to be optional and repeating, and specify token gaps between Atoms. The pattern specification is part of a larger AQL statement, which includes an extract clause. Here is a simple example of how to create a view that contains three adjacent matches from earlier defined views. In this example, the entire combination is returned, which is what group 0 refers to: bash create view Money as extract pattern <C.match> <N.match> <Q.match> return group 0 as match from Currency C, Number N, Quantifier Q; If your Atoms do not need to be exactly adjacent to each other, then you can use token gaps between the Atoms to allow for more matches. This example finds mentions of persons that follow within 0 to 2 tokens by a phone number. Notice the <Token>{0,2} construct, which indicates that a gap of 0 to 2 tokens between the person and phone annotations is allowed. bash create view Phone as extract regex /(\\d{3})-(\\d{3}-\\d{4})/ on between 4 and 5 tokens in D.text return group 1 as areaCode and group 2 as restOfNumber and group 0 as fullNumber from Document D; create view PersonPhone as extract pattern (<P.name>) <Token>{0,2} (<Ph.fullNumber>) return group 0 as match and group 1 as person and group 2 as phone from Person P, Phone Ph; Token gap constructs are restricted to occur within sequence expressions. Also, each token gap in a sequence must be preceded and followed by a \"non-token gap\" expression. As a result, extract pattern statements produce exceptions: bash -> pattern consisting only of a token gap is an error extract pattern <Token> as match from ... -> pattern beginning with a token gap is an error extract pattern <Token> {0,2} <Ph.phone> as match from ... -> pattern ending with a token gap is an error extract pattern <P.name> <Token> ? as match from ... -> group consisting only of a token gap is an error extract pattern <P.name> (<Token>) <Ph.phone> as match from ... Use the (min,max) syntax to indicate the number of times each Atom repeats. You can also use the ? syntax to indicate that an Atom or repeating Atom is optional. Atoms, along with their indications for repeating and optional, are combined to create sequences. Here is a more complex example that shows how to repeat elements. Find candidate hotel names by identifying occurrences of one to three capitalized words, followed by a 'Hotel' or 'hotel' token. ```bash create view CapsWord as extract regex /[A-Z][a-z]*/ on 1 token in D.text as word from Document D; create view HotelCandidate as extract pattern {1,3} /[Hh]otel/ as hotelname from CapsWord CW; ``` You can also use the | operator to indicate a choice between Atoms, as in extract pattern <A.match>| <B.match> <C.match> as match from Apple A, Bacon B, Chocolate C; . This pattern can be explained as \u201cmatch either one A.match OR a sequence of a B.match followed by a C.match . You can see a complete example that uses the | operator in Example 1. After you created your pattern, each match to your <pattern specification> constructs an output result according to the return clause of the pattern specification, as well as the optional <select list> at the beginning of the extract statement. The results are filtered and consolidated according to the having , consolidate , and limit clauses of the extract statement. For example, if multiple overlapping matches exist for the pattern specification, all of the possible matches are returned, and you can use a consolidation clause to filter out redundant outputs. Consider the previous example, but now the goal is to remove matches that contain the word 'Sheraton', and to consolidate resulting matches by removing the ones that are contained within a larger match. For example, we do not want to find \u201cBest Garden Hotel\u201d and also \u201cGarden Hotel\u201d across the same span of text. bash create view HotelCandidate as extract pattern <CW.word>{1,3} /[Hh]otel/ as hotelname from CapsWord CW having Not(ContainsRegex(/.*Sheraton.*/,hotelname)) consolidate on hotelname using 'ContainedWithin'; Now that you are familiar with the syntax and some examples, this diagram outlines the full syntax for the pattern specification. Refer to this full syntax when you start building patterns to see how to structure the pattern that you want to build. If you are familiar with POSIX character-based regular expressions, then you recognize that the syntax is similar. In this case, the syntax allows for white space between elements, and what an element can be in order to suit the AQL purposes is also defined.Note that the term Alternation in this case means choice. Using a vertical bar between elements indicates that a choice exists, which can be grouped by using ( ) . Pattern -> Alternation Alternation -> Sequence | Sequence | ... | Sequence Sequence -> Optional Optional ... Optional Optional -> Repeat | Repeat ? Repeat -> Atom | Atom { min, max } Atom -> <view_name.column_name> | 'string' | <'string' [match parameters]> | /regex/ | <Token> | Group Group -> ( Pattern ) Specifically, an Atom can have six formats: <view_name.column_name> Specifies a column from one of the views, table, or table function references that are named in the from list of the extract pattern statement. 'string' Specifies a match to the specified string by using the AQL default dictionary match semantics. <'string' [match parameters]> Specifies a match to the specified string by using the dictionary matching semantics that are specified by the [match parameters] . The format of [match parameters] is case (exact | insensitive) . This format specifies the type of case-folding that is used to determine the string matches. To specify an exact case-sensitive match, select exact. To specify a match that is not case-sensitive match, select the default value insensitive. /regex/ Specifies a character-based regular expression match with matching constrained to a single token in the document text. <token> A match for any token. In addition, the syntax allows a special token gap construct to be specified in a sequence expression to indicate a match between min and max number of tokens, using the repetition construct, for example <token>{3, 5} matches between 3 and 5 tokens. [return clause] Generates extracted values for each match of the pattern expression according to the return clause. The return clause has the same semantics as the return clause in an extract regex statement. [with inline_match on <viewname.colname>] For the Atoms such as string and regex Atoms, the with inline_match clause determines which text object the system uses for string or regex extraction. For example, if the clause is with inline_match on Email.subject , then all dictionaries and regular expressions defined inline in the pattern specification are applied to the subject field of the Email view. If the with inline_match is absent, string and regular expression extraction are run by default on the entire Document.text. In this case, viewname must be the name of a view or table that is defined in the current module, or imported from another module; references to table functions are not allowed in the with inline_match clause. [with language as <language code(s)>] Specifies a comma-delimited list of two-letter language codes, such as en (English) or zh (Chinese) for the languages on which to evaluate the string. There is no match on documents whose language code is not contained in this string. If the language parameter is omitted, the evaluation language defaults to one of the following language sets: If it is declared, the language sets that are specified through the set default language statement in the containing module. The language sets that contain German (de), Spanish (es), English (en), French (fr), Italian (it), and the unspecified language (x_unspecified)","title":"Description"},{"location":"aql-ref-guide/#usage-notes_13","text":"{: #aql-seq-ptn-usage} The semantics of an extract pattern statement is driven by the pattern specification. Each match constructs an output result according to the return clause of the pattern specification and the select list at the top of the extract statement. The results are filtered and consolidated according to the having , consolidate , and limit clauses of the extract statement. If multiple overlapping matches exist for the pattern specification, a pattern extraction outputs all possible matches. Use consolidation to filter redundant outputs. The semantics of the from clause of an extract pattern statement are different from other forms of extract statements that do not have a pattern specification. The general semantics of an extract statement require that the extraction specification is evaluated over each combination of the views that are defined in the <from list> . If at least one of the views in the <from list> does not contain any results on a particular document, then the output of the extract statement is empty because the set of all combinations of results in the input views is empty. In the special case of extract pattern statements, the from clause is a placeholder that declares the names of relations that are involved in the pattern specification. The semantics of the statement are driven only by the pattern specification. In particular, the output of the statement can be non-empty even when some of the input views are empty. An extract statement that uses sequence pattern extraction can carry forward the columns of any view in the from list, but only if the view name does not appear in a repeat element of the pattern specification. For example, the statement CapsWordOneToThree results in a compilation error. The error occurs because the carried forward column CW.type at the top of the extract statement belongs to the view name CW , which is in the repeat element <CW.word>{1,3} of the pattern specification. ```bash create view CapsWord as extract 'UpperCase' as type, regex /[A-Z].*/ on 1 token in D.text as word from Document D; ---> This results in and error due to the repeating element CW.word create view CapsWordOneToThree as extract CW.type as type, pattern {1,3} as match from CapsWord CW; output view CapsWordOneToThree; ``` For columns that are carried forward from view names that appear in alternation or optional elements of the pattern specification, the value of the output column is null when the corresponding alternative or the optional element is not present in the text. An example that illustrates this point is in the Person view of Example 1. {: note} Groups that occur under a repeat element cannot be output in the return clause of the statement. For example, the following statement causes an exception: bash create view CapsWordOneToThree as extract pattern (<CW.word>){1,3} return group 0 as fullmatch and group 1 as word -- not allowed due to repeat from CapsWord CW;","title":"Usage notes"},{"location":"aql-ref-guide/#examples_14","text":"{: #aql-reference-examples-15} Example 1: Sequence pattern with capturing groups The goal of this example is to find person names by identifying occurrences of given name, optionally followed by middle initial, followed by a surname, and the entire match that is optionally preceded by a common salutation. In addition, the extractor returns the entire match as reference, the first group as salutation, and the second group as name and carry forward the values of the given name, middle initial, and surname from the respective input views. create view MiddleInitial as extract regex /\\b([\\p{Lu}\\p{M}*]\\.\\s*){1,5}\\b/ on between 1 and 10 tokens in D.text as initial from Document D; create view Person as extract F.first as first, M.initial as middle, L.last as last, pattern ('Mr.'|'Ms.'|'Miss')? (<F.first> <M.initial>? <L.last>) return group 0 as reference and group 1 as salutation and group 2 as name from FirstName F, MiddleInitial M, LastName L; Since the subpattern expression ('Mr.'|'Ms.'|'Miss')? is optional, the value of the salutation output column is null when a salutation is not present in the text. Similarly, since the pattern subexpression <M.initial>? is optional, the value of the output column middle is null when a middle initial is not present. Example 2: Sequence pattern with string match and match parameters The goal of this example is to find occurrences of meeting notes for known projects by examining the title annotations of the document. Notice the with inline_match clause, which specifies that string matching is done over the match field of the view Title , as opposed to the entire document text. create view Project as extract regex /[Pp]roject\\s?\\w*/ on D.text as name from Document D; create view Title as extract regex /[A-z][a-z]+.*/ on between 1 and 20 tokens in D.text as match from Document D; create view MeetingNote as extract pattern <'Meeting Notes:'[with case exact]> (<P.name>) return group 0 as match and group 1 as projectname with inline_match on Title.match from Project P; Example 3: Sequence pattern that returns non-empty results even when an input view is empty The following statement generates results even when the input view LastName is empty. The second part of the pattern specification, <L.name>? contains an optional element. The semantics of the pattern specification is designed to output all spans that are composed of either a FirstName.name span or a FirstName.name span that is immediately followed by a LastName.name span. Therefore, on documents for which the view LastName is empty, the result of the statement consists of all spans that comprise a single FirstName.name span that is identified from that document. create dictionary FirstNamesDict as ( 'Aaron', 'Matthew', 'Peter' ); create dictionary LastNamesDict as ( 'Anthony', 'Lehman', 'Radcliff' ); create view LastName as extract dictionary 'LastNamesDict' on D.text as last from Document D having MatchesRegex(/((\\p{L}\\p{M}*)+\\s+)?\\p{Lu}\\p{M}*.{1,20}/, last); create view FirstName as extract dictionary 'FirstNamesDict' on D.text as first from Document D having MatchesRegex(/\\p{Lu}\\p{M}*.{1,20}/, first); create view PersonName as extract pattern <F.first> <L.last>? as fullName from FirstName F, LastName L;","title":"Examples"},{"location":"aql-ref-guide/#the-select-statement","text":"{: #aql-select-stmt} The select statement in AQL provides a powerful mechanism for using various specifications to construct and combine sets of tuples.","title":"The select statement"},{"location":"aql-ref-guide/#syntax_15","text":"{: #aql-select-stmt-syntax} The select statement is similar in structure to an SQL SELECT statement: select `<select list>` from `<from list>` [where `<where clause>`] [consolidate on `<column>` [using '`<policy>`' [with priority from `<column> ` [priority order]]]] [group by `<group by list>`] [order by `<order by list>`] [limit `<maximum number of output tuples for each document>`];","title":"Syntax"},{"location":"aql-ref-guide/#description_15","text":"{: #aql-select-stmt-desc} <select list> A comma-delimited list of output expressions. <from list> A comma-delimited list that is the source of the tuples to be selected. [where <where clause>] Defines a predicate to be applied on each tuple that is generated from the Cartesian product of all of the tuples in the relations in the from clause. This clause is optional. [consolidate on<column>[using '<policy>' [with priority from <column> priority order]]] Defines a consolidation policy to manage overlapping spans. This clause is optional. [group by<group by list>] Groups the tuples that are produced from the same document by common values of a specified field. This clause is optional. [order by<order by list>] Orders the output tuples that are produced by the select statement from each document. The order is based on the values of the order-by list, a comma-delimited list of expressions. This clause is optional. [limit <maximum number of output tuples for each document>] Limits the number of output tuples for each document to the specified maximum. This clause is optional.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_14","text":"{: #aql-select-stmt-usage} The semantics of the select statement are as follows: Determine the input data (in tuples) by taking the Cartesian product of relations in the from list. For each input tuple that is generated, filter it by applying the predicates in the (optional) where clause. If the optional group by clause is present, group tuples that are produced from the same document by the values that are specified in the group-by list and compute the result of the aggregate functions within the select list. Consolidate any overlapping tuples, according to the policy defined in the (optional) consolidation clause. If the optional order by clause is present, order these tuples by the values of the order-by list. Compute all expressions within the select list on each tuple, and rename the columns as specified by the as clauses. If the optional limit clause is present, limit the number of output tuples to the specified number of tuples for each document.","title":"Usage Notes"},{"location":"aql-ref-guide/#examples_15","text":"{: #aql-reference-examples-16} An example of how to use the select statement is to extract phone numbers that match a pattern. Assume that the PhoneNumbers view that extracts phone numbers of the pattern XXX-XXX-XXXX for United States is already defined. This select statement evaluates the regular expression for the pattern 444-888-XXXX across the input text. The view has the output columns documentText and phoneNumber . In addition, the output is limited to the first occurrence of this phone number pattern that is identified per document. create view PhoneNumbersPattern1 as select D.documentText, D.phoneNumber from PhoneNumbers D where MatchesRegex(/444-888-\\d{4}/,D.phoneNumber) limit 1; Another example of how you can use the select statement is to find approximate mappings of people and their corresponding phone numbers. Assume that the view Person is already defined, and that it has the columns person and the view PhoneNumbers . This select statement evaluates the where clause to find text spans that contain a person mention followed by a phone number within 1 to 3 words or tokens. The input to this statement is represented by a join of the Person and PhoneNumbers views in the from list. create view PersonPhone as select P1.documentText, P1.person, P2.phoneNumber, CombineSpans(P1.person,P2.phoneNumber) as personPhoneSpan from Person P1, PhoneNumbers P2 where FollowsTok(P1.person,P2.phoneNumber,1,3); The personPhoneSpan column will contain the matching spans that give the approximate person-phone mapping. personPhoneSpan John : 433-999-1000 Martha Mob 433-999-1001 The select list The select list in an AQL select or extract statement consists of a comma-delimited list of output expressions. The from list The second part of a select or an extract statement in AQL is the from list. The from list is a comma-separated list that is the source of the tuples to be selected or extracted. The where clause The optional where clause defines a predicate to be applied on each tuple generated from the Cartesian product of all tuples in the relations in the from clause. The consolidate on clause The optional consolidate on clause specifies how overlapping spans are resolved across tuples that are output by a select or extract statement. Tuples with non-overlapping spans are not affected when this clause is used. The group by clause The optional group by clause of a select statement directs the runtime component to group tuples that are produced from the same document by common values of a specified field. The order by clause The optional order by clause directs the runtime component to order the output tuples that are produced by the select statement from each document based on the values of the order by list, which is a comma-delimited set of expressions The limit clause The optional limit clause specifies a limit on the number of output tuples that are produced by the select statement for a document. The select... into statement The select ... into statement is useful for defining a view and specifying that it is an output view in a single statement.","title":"Examples"},{"location":"aql-ref-guide/#the-select-list","text":"{: #aql-select-list} The select list in an AQL select or extract statement consists of a comma-delimited list of output expressions.","title":"The select list"},{"location":"aql-ref-guide/#syntax_16","text":"{: #aql-sel-list-syntax} Each select expression must be in one of the following forms: select <viewname>.<colname> as <alias> | <viewname>.* | <expr> as <alias> | case when <predfunction1()> then <expr1> when <predfunction2()> then <expr2>... when <predfunctionn()> then <exprn> [else <expr_default>] as <name>","title":"Syntax"},{"location":"aql-ref-guide/#description_16","text":"{: #aql-sel-list-desc} <viewname>.<colname> as <alias> <viewname> Specifies the view from which to select columns. <colname> Specifies the column in that view. <alias> Specifies the name by which the selected field is known. This field is an optional field. It is selected to be a part of each output tuple. If <alias> is not specified, the name of the column is by default the <colname> . Can be a simple identifier or a double-quoted identifier. <viewname>.* Specifies the name of a view. This syntax indicates that all columns of the specified view must be carried forward to the enclosing select or extract statement. Like SQL, AQL allows the shorthand select * statement. The effect of this statement is to select all columns from all inputs that are specified in the from clause of the select statement. However, the shorthand extract * statement is not supported. <expr> as <alias> Represents the assignment of an expression to an attribute of the encompassing view. <expr> Specifies an expression that is composed of scalar function calls, aggregate function calls, or a constant. <name> Represents the name of the column that holds the result of the expression that is specified by <expr> as <alias> . If <alias> is not specified, the name of the column is by default the <name> . Can be a simple identifier or a double-quoted identifier. when<function1()> then <expr1> when <function2()> then <expr2> ... when <functionN()> then <exprn> [else ] <expr_default> as <name> <function1()>, <function2()>, <functionN()> Specify scalar functions that return type Boolean. <expr1>, <expr2>, <exprn>, <expr_default> Specify expressions that are composed of scalar function calls and must return the same type. If the result of <function1()> is true, then the result of the case expression is the result of <expr1> , and none of the subsequent when clauses are evaluated. Otherwise, subsequent when clauses (if any) are evaluated in the same manner. When none of the conditions of the when clauses are satisfied, the result of this case expression is the result of the default expression <expr_default> . This expression is specified in the optional else clause. If the [else] clause is absent, the result of this case expression is null .","title":"Description"},{"location":"aql-ref-guide/#usage-notes_15","text":"{: #aql-sel-list-usage} The following statement is not supported: bash select * from Document; The contents of the Document view might not be fully known in the current context or scope of the .aql file where this select statement is issued. The lack of information about the contents is because multiple require document with columns statements provided outside of the current .aql file might alter the eventual schema definition of this special Document view when it is used at the level of a module. The shorthand Document.* statement is not a valid AQL construct. You can explicitly select fields from the Document view. The following example shows a valid explicit selection of fields from a Document view: bash select D.label as label, D.text as text from Document D;","title":"Usage notes"},{"location":"aql-ref-guide/#examples_16","text":"{: #aql-reference-examples-17} The following examples illustrate various forms of the select list. Example 1: Explicit value assignment using a constant This example shows the assignment of a constant value to a view attribute within the select list. The field that is called polarity indicates whether the polarity of PS.match is positive or negative (Notice the explicit assignment of a constant value to this attribute). create view PositiveSentimentsWithPolarity as select 'positive' as polarity, PS.match as sentiment from PositiveSentiments PS; create view NegativeSentimentsWithPolarity as select 'negative' as polarity, NS.match as sentiment from NegativeSentiments NS; Example 2: Explicit value assignment using function call The following example illustrates how the result of a function call is explicitly assigned to a view attribute within the select list. create view Citizenship as select P.Name as name, MatchesDict('USCities.dict', P.birthPlace) as isUSCitizen from Person P; Example 3: Select list expression from a dictionary extraction The following example illustrates how the select list expression can pick values of type Span from a result of dictionary extraction. create view PersonNames as select N.match as name from (extract dictionary 'firstNames.dict' on D.text as match from Document D )N; Example 4: Examples of case expressions This first example shows you how to specify null handling on specific fields: create view School as select case when Not(NotNull(P.education)) then 'Unknown' else GetString(P.education) as name from Person P; This example explains how to classify data: create view Company as select PM.name as productname, case when ContainsRegex (/IBM/,PM.name) then 'IBM' when ContainsDict ('OpenSourceDict',PM.name) then 'OSS' else 'Unknown' as name from ProductMatches PM;","title":"Examples"},{"location":"aql-ref-guide/#the-from-list","text":"{: #aql-from-list} The second part of a select or an extract statement in AQL is the from list. The from list is a comma-separated list that is the source of the tuples to be selected or extracted.","title":"The from list"},{"location":"aql-ref-guide/#syntax_17","text":"{: #aql-from-list-syntax} from <from list item> <name> [, <from list item> <name>]","title":"Syntax"},{"location":"aql-ref-guide/#description_17","text":"{: #aql-from-list-desc} <from list item> A view, table, table function reference or nested AQL statement. All nested statements in AQL must be surrounded by parentheses. <name> Local name of the <from list item> , that is scoped within the select statement or the extract statement. A local name can be a simple identifier or a double-quoted identifier. Local names that contain spaces, punctuation characters, or AQL keywords must be enclosed in double quotation marks.","title":"Description"},{"location":"aql-ref-guide/#examples_17","text":"{: #aql-reference-examples-18} Example 1: A from list with a view and a nested statement This example shows a from list that references a view and a nested extract statement. The example assigns the result of the statement to the local name FN . The example also assigns the outputs of the LastName view to the local name Last Name . create dictionary LastNamesDict as ( 'Anthony', 'Lehman', 'Radcliff' ); create view LastName as extract dictionary 'LastNamesDict' on D.text as lastname from Document D; create view FromList as select * from (extract dictionary 'first.dict' on D.text as firstname from Document D) FN, LastName \"Last Name\" where Follows(FN.firstname, \"Last Name\".lastname, 0, 1); The following names are contained in the external dictionary first.dict: #Dictionary for given names Aaron Candra Freeman Mathew Matthew Zoraida","title":"Examples"},{"location":"aql-ref-guide/#the-where-clause","text":"{: #aql-where} The optional where clause defines a predicate to be applied on each tuple generated from the Cartesian product of all tuples in the relations in the from clause.","title":"The where clause"},{"location":"aql-ref-guide/#syntax_18","text":"{: #aql-where-syntax} select <select list> from <from list> [where <where clause>]","title":"Syntax"},{"location":"aql-ref-guide/#description_18","text":"{: #aql-where-desc} <where clause> Specifies one or more predicates. A join occurs when any predicate in a where clause involves fields from more than one view that belongs to the from list. This predicate must be a conjunction of a set of built-in predicate functions or other user-defined functions that return the data type Boolean: bash function1() and function2() and ... and functionn() The where clause is optional and can be omitted from a select statement if no predicates exist to apply.","title":"Description"},{"location":"aql-ref-guide/#examples_18","text":"{: #aql-reference-examples-19} Example 1: Filter out joined tuples by using a predicate in the WHERE clause This example shows a where clause that finds only phrases that consist of valid given names that are followed within 0-1 characters by valid surnames. -- a view containing words that are valid given names create view FirstName as extract dictionary 'first.dict' on D.text as firstname from Document D; -- a view containing words that are valid surnames create view LastName as extract dictionary 'last.dict' on D.text as lastname from Document D; -- a view containing phrases consisting of valid given names -- followed within 0-1 characters by valid surnames. create view FullName as select * from FirstName FN, LastName LN where Follows (FN.firstname, LN.lastname, 0, 1); The following names are contained in the external dictionary first.dict: #Dictionary for given names Aaron Candra Freeman Mathew Matthew Zoraida The following names are contained in the external dictionary last.dict: #Dictionary for surnames Anthony Lehman Radcliff","title":"Examples"},{"location":"aql-ref-guide/#the-consolidate-on-clause","text":"{: #aql-con-on} The optional consolidate on clause specifies how overlapping spans are resolved across tuples that are output by a select or extract statement. Tuples with non-overlapping spans are not affected when this clause is used.","title":"The consolidate on clause"},{"location":"aql-ref-guide/#syntax_19","text":"{: #aql-con-on-syntax} The following code is an example of the general structure of this clause: consolidate on <target> [using '<policy>'[ with priority from <priority_column> [ <priority_order> ]]]","title":"Syntax"},{"location":"aql-ref-guide/#description_19","text":"{: #aql-con-on-desc} <target> Specifies a column in a view in the from clause, or an expression that is composed of scalar function calls involving columns of views that are specified in the from clause as arguments. '<policy>' Specifies one of the following consolidation policies that is supported by Text Analytics: ContainedWithin This policy is the default. If spans A and B overlap, and A completely contains B, this policy then removes the tuple that contains span B from the output. If A and B are the same, then it removes one of them. The choice of which tuple to remove is arbitrary. NotContainedWithin If spans A and B overlap, and A completely contains B, this policy then removes the span A from the output. If A and B are the same, then it removes one of them. The choice of which tuple to remove is arbitrary. ContainsButNotEqual This policy is the same as ContainedWithin, except that the spans that are exactly equal are retained. ExactMatch If a set of spans covers the same region of text, this policy returns exactly one of them. All other spans are left untouched. LeftToRight This policy processes the spans in order from left to right. When overlap occurs, it retains the leftmost, longest, non-overlapping span. This policy emulates the overlap-handling policy of most regular expression engines. <priority_column> Specifies a column of type Text, String, Integer, or Float. Can be specified only with the LeftToRight consolidation policy. <priority_order> Specifies either ascending or descending order. Can be specified only with the LeftToRight consolidation policy. The ascending order ensures that if a tuple T1 has priority 1 and a tuple T2 has priority 2, T1 has a higher priority than T2. By contrast, if the priority order is descending, T2 has a higher priority. The default value of the priority order is ascending.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_16","text":"{: #aql-con-on-usage} When the priority clause is present, the semantics of consolidation follow this order: Process spans from left to right, and when spans overlap, retain the leftmost spans. If you have multiple overlapping spans that start at the same offset, retain the ones with the highest priority according to the priority order. Break remaining ties by retaining the longest spans among those spans with the same priority. Consolidate treats nulls as identical. All inputs with a null <consolidate target> result in a single output tuple, which is chosen randomly among those inputs. This behavior is similar to how tuples are consolidated with an identical span in the target column. The exception to resulting in a single output tuple is if the policy is ContainsButNotEqual. In that case, the null <consolidate target> outputs all inputs with null consolidate target.","title":"Usage notes"},{"location":"aql-ref-guide/#examples_19","text":"{: #aql-reference-examples-20} Example 1: Consolidate on single column This example directs the system to examine the field Person.name of all output tuples and to use the ContainedWithin consolidation policy to resolve overlap. consolidate on Person.name using 'ContainedWithin' Example 2: Consolidate on expression, involving multiple columns This example directs the system to examine the result of applying the CombineSpans scalar function to fields Person.firstname and Person.lastname in each output tuple. It resolves overlap by using the ContainedWithin consolidation policy. consolidate on CombineSpans(Person.firstname, Person.lastname) using 'ContainedWithin' Example 3: Consolidate by using LeftToRight policy and priority order Suppose that the following Term tuples are extracted from the input text John Doe: match: `John`, priority: `1` and match: `John Doe`, priority: `2` Both spans have the same begin offset. When you consolidate by using the LeftToRight policy for ascending priority order, the tuple ( match: John, priority: 1 ) is retained because it has the higher priority. When you consolidate by using the descending priority order, the tuple ( match: John Doe, priority: 2 ) is retained, as in the following example: create view ConsolidatePeopleWithPrioritiesAscending as select P.match as match, P.weight as weight from People P consolidate on P.match using 'LeftToRight' with priority from P.weight ascending;","title":"Examples"},{"location":"aql-ref-guide/#the-group-by-clause","text":"{: #aql-group-by} The optional group by clause of a select statement directs the runtime component to group tuples that are produced from the same document by common values of a specified field.","title":"The group by clause"},{"location":"aql-ref-guide/#syntax_20","text":"{: #aql-group-by-syntax} select <select list> from <from list> [where <where clause>] ... [group by <group by list>]","title":"Syntax"},{"location":"aql-ref-guide/#description_20","text":"{: #aql-group-by-desc} <group by list> Specifies a comma-delimited list of expressions that involves columns of the views in the from clause and scalar function calls. When you apply the group by clause, each group of tuples that shares common values for all group by expressions produces a single output tuple that is representative for the entire group. A field or expression that does not appear in the group by clause cannot appear in the select list, unless it is used in an aggregate function call. The order of expressions in the list does not matter. The group by clause treats all nulls as identical. Group by on a column with null values results in a single group.","title":"Description"},{"location":"aql-ref-guide/#examples_20","text":"{: #aql-reference-examples-21} Example 1: Computing aggregate values Use the group by clause to compute aggregate values. This example counts the number of occurrences of each given name in the document. In this example, Count is an aggregate function. create view SampleView as select GetText(P.firstname) as name, Count(GetText(P.firstname)) as occurrences from (extract dictionary 'first.dict' on D.text as firstname from Document D ) P group by GetText(P.firstname); In this case, first.dict is an external dictionary that contains the following entries: #Dictionary for given names Aaron Candra Freeman Matthew Zoraida The following steps describe semantics of this statement: Group tuples that are produced by the subquery in the from clause by the text content of their firstname field. For each group, count the number of tuples with a non-null firstname value. Produce a single output tuple for each such group, with two values, the name and the number of tuples in that group. Example 2: Issues with grouping dissimilar fields This example illustrates a statement that is not valid. select GetText(P.firstname) as first, GetText(P.lastname) as last, Count(P.firstname) as occurrences from Person P group by GetText(P.firstname); The inclusion of GetText(P.lastname) in the select list is not accepted, since tuples with the same firstname values might have different lastname values, which leads to ambiguity.","title":"Examples"},{"location":"aql-ref-guide/#the-order-by-clause","text":"{: #aql-order-by} The optional order by clause directs the runtime component to order the output tuples that are produced by the select statement from each document based on the values of the order by list, which is a comma-delimited set of expressions","title":"The order by clause"},{"location":"aql-ref-guide/#syntax_21","text":"{: #aql-order-by-syntax} select ... [order by <order by list>]","title":"Syntax"},{"location":"aql-ref-guide/#description_21","text":"{: #aql-order-by-desc} <order by list> Specifies a comma-delimited list of expressions. The order is based on the values of a comma-delimited list of expressions. The order by clause supports expressions that return numeric (Integer or Float), Text, or Span data types. If an expression in the order by clause returns a type Span, the result tuples are compared by comparing the relevant span values. In the following example, the span values of the person field are compared. ```bash order by P.person ``` The order by clause treats nulls as unordered (amongst themselves). Nulls are ordered lower than other objects.","title":"Description"},{"location":"aql-ref-guide/#examples_21","text":"{: #aql-reference-examples-22} Example 1: Order by multiple expressions Assume that person is a field of type Span. The following order by clause specifies that the statement returns tuples within each document. They are ordered lexicographically by the text of the person field and then by the beginning of the person field. order by GetText(P.person), GetBegin(P.person)","title":"Examples"},{"location":"aql-ref-guide/#the-limit-clause","text":"{: #aql-limit} The optional limit clause specifies a limit on the number of output tuples that are produced by the select statement for a document.","title":"The limit clause"},{"location":"aql-ref-guide/#syntax_22","text":"{: #aql-limit-syntax} select <select list> from <from list> ... [limit <maximum number of output tuples for each document>];","title":"Syntax"},{"location":"aql-ref-guide/#description_22","text":"{: #aql-limit-desc} <maximum number of output tuples for each document> Specifies the maximum number of output tuples for each document. If the limit value is greater than or equal to the total number of tuples that could be returned, all of the tuples are returned.","title":"Description"},{"location":"aql-ref-guide/#examples_22","text":"{: #aql-reference-examples-23} Example 1: Limiting the number of returns This example returns the first three person names in each document: create view SampleView as select * from Person P order by GetBegin(P.name) limit 3;","title":"Examples"},{"location":"aql-ref-guide/#the-select-into-statement","text":"{: #aql-select-into} The select ... into statement is useful for defining a view and specifying that it is an output view in a single statement.","title":"The select... into statement"},{"location":"aql-ref-guide/#syntax_23","text":"{: #aql-select-into-syntax} select <select list> into <output view name> from <from list> [where <where clause>] [consolidate on <column> [using '<policy>' [with priority from <column> [priority order]]]] [group by <group by list>] [order by <order by list>] [limit <maximum number of output tuples for each document>];","title":"Syntax"},{"location":"aql-ref-guide/#description_23","text":"{: #aql-select-into-desc} <output view name> Specifies the name of the output view that is defined by the statement. The select ... into statement is identical to the select statement, except for the additional into <output view name> clause.","title":"Description"},{"location":"aql-ref-guide/#examples_23","text":"{: #aql-reference-examples-24} Example 1: Defining a view This example defines a view that is called PersonPhone and also specifies this view as an output view. select P.name as name, Ph.number as phoneNumber into PersonPhone from Person P, Phone Ph; This example is equivalent to the following two statements: create view PersonPhone as select P.name as name, Ph.number as phoneNumber from Person P, Phone Ph; output view PersonPhone;","title":"Examples"},{"location":"aql-ref-guide/#the-detag-statement","text":"{: #aql-detag} The detag statement in AQL provides the function to detag or remove all the markup from HTML or XML documents before you run AQL extractors. The detag statement can also retain the original locations of tags and any values that are stored in these tags. When a detag statement removes tags from a document, the runtime component remembers the mapping between offsets from the detagged text and the original markup source. The Remap function, a special built-in function that maps spans from the detagged text back to their equivalent spans from the original source.","title":"The detag statement"},{"location":"aql-ref-guide/#syntax_24","text":"{: #aql-detag-syntax} detag <input view name>.<text column> as <output view name> [detect content_type (always|never)] [annotate element '<element name>' as <auxiliary view name> [with attribute '<attribute name>' as <column name>] [and attribute '<attribute name>' as <column name>] [, element ...]];","title":"Syntax"},{"location":"aql-ref-guide/#description_24","text":"{: #aql-detag-desc} <input view name>.<text column> <input view name> Specifies the name of the input view on which to do the detag process. The <input view name> can be a simple identifier or a double-quoted identifier. <text column> Specifies the text field of the input view on which to do the detag process. The <text column> can be a simple identifier or a double-quoted identifier. <output view name> Specifies the name of the output view that contains the detagged text. The output view contains a single column that is called text , which holds the detagged text. The <output view name> can be a simple identifier or a double-quoted identifier. always|never Specifies whether to verify that the contents are HTML or XML before the detag statement is processed. When you run non-HTML and non-XML text through a detagger, problems can occur if the text contains XML special characters such as < , > , or & . If the detect content_type clause is missing, the default value is always and the system always detects content. always Specifies that verification always occurs before the operation is attempted to avoid problems with parsing documents that are not HTML or XML. If the value of the <text column> does not appear to contain a markup, the system skips detagging for the current document. never Specifies that verification never occurs before the detag operation is attempted. The system attempts to detag the target text, even if the text does not contain any HTML or XML content. <element name> Specifies the name of the HTML or XML element to be annotated. The optional annotate clause can direct the runtime component to remember information about the removed tags by creating one or more views. <auxiliary view name> Specifies the name of the view that is created to hold the original tags and their attributes. Can be a simple identifier or a double-quoted identifier. <attribute name> Name of an attribute of the HTML or XML element. <column name> The name of the column in the <auxiliary view name> that is used to store the values of <attribute name> . Can be a simple identifier or a double-quoted identifier.","title":"Description"},{"location":"aql-ref-guide/#examples_24","text":"{: #aql-reference-examples-25} Example 1: Specifying the detag output view and an auxiliary view In this example, the DetaggedDoc view is created to hold the detagged version of the original text in the text attribute of the view Document . In addition to creating a DetaggedDoc view, the annotate clause creates an auxiliary view called Anchor . This auxiliary view has two columns. One column, called match , contains the anchor text. The other column, called linkTarget , contains the actual target of the link as text. The spans in the match column are over the text value of the DetaggedDoc view. detag Document.text as DetaggedDoc annotate 'a' as Anchor with attribute 'href' as linkTarget; Example 2: Using the Remap function The following example illustrates how the Remap function is used to map spans from the detagged text back to their equivalents in the original source. -- Strip out tags from each document, provided that the document -- is in HTML or XML format. -- Remember the locations and content of all <A> and <META> tags -- in the original source document. detag Document.text as DetaggedDoc detect content_type always annotate element 'a' as Anchor with attribute 'href' as target, element 'meta' as Meta with attribute 'name' as name and attribute 'content' as content; output view DetaggedDoc; -- Create a view containing all lists of keywords in the -- document META tags. create view MetaKeywordsLists as select M.content as list from Meta M where MatchesRegex(/keywords/, 'CASE_INSENSITIVE', M.name) and NotNull(M.content); -- Create a dictionary of \"interesting\" web sites create dictionary InterestingSitesDict as ( 'ibm.com', 'slashdot.org' ); -- Create a view containing all anchor tags whose targets contain -- a match of the \"interesting sites\" dictionary. create view InterestingLinks as select A.match as anchortext, A.target as href from Anchor A where ContainsDict('InterestingSitesDict', A.target); -- Find all capitalized words in the anchor text of links to -- \"interesting\" web sites. create view InterestingWords as extract I.href as href, regex /[A-Z][a-z]+/ on 1 token in I.anchortext as word from InterestingLinks I; -- Map spans in the InterestingWords view back to the original -- HTML or XML source of the document. create view InterestingWordsHTML as select I.href as href, Remap(I.word) as word from InterestingWords I;","title":"Examples"},{"location":"aql-ref-guide/#documenting-the-detag-statement-with-aql-doc","text":"The AQL Doc comment for a detag statement contains the following information: General description about the function of the statement. @field for each text field of the input view on which to do the detag process. @auxView specifies the view name. @auxViewField specifies the fully qualified column name of the view. /** * Detags the input document * @field text the detagged text of the document * @auxView Anchor stores the anchor points from tagged doc * @auxViewField Anchor.linkTarget stores the href attribute of anchor tag */ detag Document.text as DetaggedDoc annotate element 'a' as Anchor with attribute 'href' as linkTarget;","title":"Documenting the detag statement with AQL Doc"},{"location":"aql-ref-guide/#the-create-dictionary-and-create-external-dictionary-statements","text":"{: #aql-create-dict} The create dictionary and create external dictionary statements are used to define dictionaries of words or phrases to identify matching terms across input text through extract statements or predicate functions. The create dictionary statement allows the specification of dictionary content in source AQL code, and the dictionary content is serialized inside the compiled representation of the module (the .tam file). The create external dictionary statement allows you to specify the dictionary content when the extractor is instantiated, instead of in source AQL code, and you do not have to recompile the module. Therefore, external dictionaries are powerful constructs that allow the AQL developer to expose customization points in a compiled module. Dictionaries can be created from three sources: Dictionary files Inline dictionary declarations Tables that are created with the create table statement and the create external table statement.","title":"The create dictionary and create external dictionary statements"},{"location":"aql-ref-guide/#syntax_25","text":"{: #aql-create-dict-syntax} The internal create dictionary statement has three syntactical forms, from file , from table , and an inline format. Internal dictionary From file: ```bash create dictionary from file ' ' [with language as ' '] [and case (exact | insensitive)] [and lemma_match]; ``` From table: ```bash create dictionary from table with entries from [and language as ' '] [and case (exact | insensitive)] [and lemma_match]; ``` Inline format bash create dictionary <dictionary name> [with language as '<language code(s)>'] [and case (exact | insensitive)] [and lemma_match] as ( '<entry 1>', '<entry 2>', ... , '<entry n>' ) ; External dictionary bash create external dictionary <dictionary-name> required [true|false] [with language as '<language codes>'] [and case (exact | insensitive )] [and lemma_match];","title":"Syntax"},{"location":"aql-ref-guide/#description_25","text":"{: #aql-create-dict-desc} <dictionary name> Specifies a name for the new internal or external dictionary. Can be a simple identifier or double-quoted identifier. '<file name>' Specifies the name of the file that contains dictionary entries. Dictionary files are carriage-return-delimited text files with one dictionary entry per line. Entries in a dictionary file can consist of multiple tokens. <table name> Specifies the name of the table from which to add dictionary entries. Dictionaries cannot be created from a table that is imported from another module. <column name> Specifies the name of the column in the table from which to add dictionary entries. required [true|false] Specifies whether external content for the external dictionary is required to run the module. true If the clause is required true , you must supply a URI to the location of the file that contains the external content. The specified file must contain content. If the URI is not supplied, or if the file does not contain content, the runtime component throws an exception. false If the clause is required false , the module can be run successfully even if a URI to the external content is not supplied for this dictionary. If a URI is not supplied, the runtime component treats it as an empty dictionary. The use of create external dictionary <dictionary-name> allow_empty is now deprecated and results in a compiler warning. {: deprecated} '<language code(s)>' Specifies a comma-delimited list of two-letter language codes, such as en (English) or zh (Chinese) for the languages, or for the document languages for external dictionaries, on which to evaluate the dictionary. The dictionary produces no results on documents whose language code is not contained in this string. If the language parameter is omitted, the dictionary language defaults to one of the following language sets: The language sets that are specified through the set default language statement, if it is declared, in the containing module. The language sets that contain German (de), Spanish (es), English (en), French (fr), Italian (it), and the unspecified language (x_unspecified). lemma_match Use lemmatization to find matches for similar words to a dictionary term in your documents. Lemmatization is the process of determining the lemma for a given word. A lemma is a word that can be used as a match for a single given term. For example, the term \u201cgo\u201d can be matched to the terms \u201cgoes\u201d, \u201cgoing\u201d, \u201cgone\u201d, or \u201cwent\u201d. This process involves complex tasks such as understanding context and determining the part of speech of a word in a sentence. Lemmatization is not provided by the built-in tokenizer. Lemma match is performed only for dictionaries declared with the lemma match clause. The semantics for dictionary extraction with the lemma_match clause are as follows: The lemmatized form for each token of the input document is computed. The dictionary is evaluated against the lemmatized document. You cannot use the lemma_match option with the case exact option. If both are used, a compiler error is returned. {: important} case (exact | insensitive) Specifies the type of case-folding that the dictionary performs when it determines whether a specific region of the document matches. exact Specifies an exact case-sensitive match. insensitive Specifies a match that is not case-sensitive match. This option is the default value. '<entry 1>', '<entry 2>', ... , '<entry n>' Specifies the strings that you want to include in the inline dictionary. Entries in an inline dictionary can consist of one or more tokens.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_17","text":"{: #aql-create-dict-usage} The from file and from table formats are recommended, especially when you anticipate modifying the entries, or when you have many entries. By using these formats, you can modify the contents of the dictionary without modifying the code. When the create dictionary statement is processed by the modular AQL compiler, references to dictionary file locations that are specified in the create dictionary ... from file syntax must be relative to the root of the module in which this create dictionary statement is issued. You can specify comments in a dictionary file by preceding the comment with the character # . Comments can start anywhere in a line. If you want to specify comments that span multiple lines, you must precede each line with the comment character. If the comment character is part of a dictionary entry, you must escape it using the backslash character (\\), as in \\# . If the backslash character is part of the dictionary entry, it must be escaped with itself, as in \\\\ . For external dictionaries, when modules are being loaded, you must specify a list of URIs to external dictionaries, as required by the modules that are being loaded. Lemmatization of dictionaries : The primary difference between the existing dictionary matching semantics and the lemmatized semantics is that the match is performed against the lemmatized form of the document, instead of the original form of the document. Dictionary entries that belong to a dictionary with lemma match enabled have these prerequisites: A dictionary entry can contain one or more tokens, where each entry token is a lemma. To create a dictionary of lemmas, you can use the [GetLemma scalar function. The tokens of a dictionary entry should be separated by a white space. If the token consists of white spaces, then the white spaces should be escaped by using the backslash (\\) character. The following table shows the differences between the create external dictionary statement and the create dictionary statement: create external dictionary create dictionary Defines a placeholder for a dictionary whose content is supplied at initialization time. Requires that the content of the dictionary is available at compile time. Serialized in the compiled representation (.tam) of a module.","title":"Usage notes"},{"location":"aql-ref-guide/#examples_25","text":"{: #aql-reference-examples-26} Example 1: Creating an external dictionary The external dictionary, PersonPositiveClues, expects to be populated with values from an external file at load-time. It also expects to be matched against some Western languages as specified by its flags. module PersonModuleEnglish; create external dictionary PersonPositiveClues allow_empty false with case exact; export dictionary PersonPositiveClues; Example 2: Lemmatization Consider a dictionary that has lemma match enabled and contains two entries: go shop and went shopping. The document contains the text Anna went shopping . The lemmatized form of the input document is Anna go shop . Lemma match returns went shopping as a match for the entry go shop. The original document text is not compared to the dictionary entries, only the lemmatized document text. Therefore, no matches exist in the document for the entry went shopping .","title":"Examples"},{"location":"aql-ref-guide/#documenting-the-create-dictionary-and-create-external-dictionary-statements-with-aql-doc","text":"The AQL Doc comment for a create dictionary statement contains the following information: General description about the dictionary. /** * A dictionary of terms used to greet people. * */ create dictionary GreetingDict as ( 'regards', 'regds', 'hello', 'hi', 'thanks', 'best', 'subj', 'to', 'from' ); The AQL Doc comment for a create external dictionary statement contains the general description about the dictionary that is created. The following string illustrates the format: /** * Customizable dictionary of given names. * Evaluated on English, French, Italian, German, Portuguese, Spanish text. */ create external dictionary CustomFirstNames_WesternEurope allow_empty true;","title":"Documenting the create dictionary and create external dictionary statements with AQL Doc"},{"location":"aql-ref-guide/#the-create-table-statement","text":"{: #aql-create-table} The create table statement creates an AQL table. The create table statement in AQL is used to define static lookup tables to augment annotations with more information.","title":"The create table statement"},{"location":"aql-ref-guide/#syntax_26","text":"{: #aql-cr-tab-syntax} create table <table name> ( <colname> <type> [, <colname> <type>]* ) as values ( <value> [, <value>]*), ... ( <value> [, <value>]*);","title":"Syntax"},{"location":"aql-ref-guide/#description_26","text":"{: #aql-cr-tab-desc} <table name> Specifies the name of the table to create. The <table name> can be a simple identifier or a double-quoted identifier. <colname> Specifies the name of the column to create. <type> Specifies the AQL data type for the associated column. All columns must be of type Text , Integer, Float or Boolean. <value> Specifies the tuples to be populated in the created table.","title":"Description"},{"location":"aql-ref-guide/#examples_26","text":"{: #aql-reference-examples-27} Example 1: Creating a table of company names In this example, the create table statement adds more location metadata to company name annotations: -- Create a dictionary of company names create dictionary CompanyNames as ('IBM', 'BigCorp', 'Initech'); -- Find all matches of the company names dictionary. create view Company as extract dictionary 'CompanyNames' on D.text as company from Document D; -- Create a table that maps company names to locations of -- corporate headquarters. create table NameToLocation (name Text, location Text) as values ('IBM', 'USA'), ('BigCorp', 'Apex'), ('Initech', 'Dallas'), ('Acme Fake Company Names', 'Somewhere'); -- Use the table to augment the Company view with location -- information. create view CompanyLoc as select N2C.location as loc, C.company as company from Company C, NameToLocation N2C where Equals(GetText(C.company), GetText(N2C.name)); output view CompanyLoc;","title":"Examples"},{"location":"aql-ref-guide/#documenting-the-create-table-statement-with-aql-doc","text":"The AQL Doc comment for a create table statement contains the following information: General description about the table. @field for every column name in the schema of this table. /** Create a table that maps company names to locations /** of corporate headquarters. * @field name name of the company * @field location location of corporate headquarters */ create table NameToLocation (name Text, location Text) as values ('IBM', 'USA'), ('Enron', 'UK'), ('Initech', 'Dallas'), ('Acme Fake Company Names', 'Somewhere');","title":"Documenting the create table statement with AQL Doc"},{"location":"aql-ref-guide/#the-create-external-table-statement","text":"{: #aql-create-ext-table} You can use the create external table statement to specify a table with established content when a compiled module is run across all input documents. You supply the table content during load time instead of in the source AQL code, and you do not have to recompile the module. External tables are powerful constructs that allow the AQL developer to expose customization points in a compiled module.","title":"The create external table statement"},{"location":"aql-ref-guide/#syntax_27","text":"{: #aql-cr-ext-tab-syntax} create external table <table-name> (<colname> <type> [, <colname> <type>]* ) allow_empty <true|false>;","title":"Syntax"},{"location":"aql-ref-guide/#description_27","text":"{: #aql-cr-ext-tab-desc} <table-name> Specifies name of the external table to create. The <table-name> can be a simple identifier or a double-quoted identifier. <colname> Specifies the name of the column to create. <type> Specifies the AQL data type for the associated column. All columns must be of type Text , Integer, Float or Boolean. [, <colname> <type>]* Specifies additional columns and AQL objects to be used in the external table. allow_empty [true|false] Specifies the value for the allow_empty clause. true If the clause is allow_empty true , the module can be run successfully even if a URI to the external content is not supplied for this table. If the URI is not supplied, the runtime component treats it as an empty table. false If the clause is allow_empty false , then you must supply a URI to the location of the file that contains the external content. The specified file must contain content. If the URI is not supplied, or if the file does not contain content, the runtime component throws an exception.","title":"Description"},{"location":"aql-ref-guide/#usage-notes_18","text":"{: #aql-cr-ext-tab-usage} The compiled representation of the module contains metadata about the external objects (views, dictionaries, and tables) that the module defines. When modules are being loaded, you must specify a list of URIs to external tables, as required by the modules that are being loaded. The supported format for the content of an external table is one CSV (.csv) file with header. The following table shows the differences between the create external table statement and the create table statement: create external table create table Defines a placeholder for a table whose content is supplied at initialization time. Requires that the content of the table is available at compile time. Serialized in the compiled representation (.tam) of a module.","title":"Usage notes"},{"location":"aql-ref-guide/#examples_27","text":"{: #aql-reference-examples-28} Example 1: Creating an external table that is populated at load time The external table, PersonNegativeClues, expects to be populated at load-time because of the flag, allow_empty false . module PersonModuleFrench; create external table PersonNegativeClues (name Text) allow_empty false; export table PersonNegativeClues; Example 2: Creating a dictionary with an external table Dictionaries can also be created from external tables, similar to being created from inline tables that are declared with the create table statement. create external table Product (nickName Text, formalName Text) allow_empty false; /** * Dictionary of product nicknames, from the nickName field * of the customizable external table Product. */ create dictionary ProductDict from table Product with entries from nickName;","title":"Examples"},{"location":"aql-ref-guide/#documenting-the-create-external-table-statement-with-aql-doc","text":"The AQL Doc comment for a create external table statement contains the following information: General description about the table. @field for every column name in the schema of this table. /** Create a table that maps company names to locations of corporate headquarters. * @field name name of the company * @field location location of corporate headquarters */ create external table Company2Location (name Text, location Text) allow_empty false;","title":"Documenting the create external table statement with AQL Doc"},{"location":"aql-ref-guide/#the-create-external-view-statement","text":"{: #aql-create-ext-view} The create external view statement in AQL allows specification of more metadata about a document as a new view, in addition to the predefined Document view that holds the textual and label content.","title":"The create external view statement"},{"location":"aql-ref-guide/#syntax_28","text":"{: #aql-cr-x-view-syntax} create external view <view_name> ( <colname> <type> [, <colname> <type>]* ) external_name '<view_external_name>';","title":"Syntax"},{"location":"aql-ref-guide/#description_28","text":"{: #aql-cr-x-view-desc} <view_name> Specifies the internal name of the external view. The external view is referred by this name in the AQL rules. A <view_name> can be a simple identifier or a double-quoted identifier. A <view_name> cannot contain the period character. <colname> Specifies the name of the column to define in the external view. <type> Specifies the data type for the associated column. The data types that are supported for external view columns are Text, Span, Integer, and Float. '<view_external_name>' Specifies the external name of the external view. External systems that populate tuples into the external view refer to the external view by the external name. The '<view_external_name>' must be a String constant that is encased in single quotation marks ('ExternalName').","title":"Description"},{"location":"aql-ref-guide/#examples_28","text":"{: #aql-reference-examples-29} To illustrate external views, consider an example application that requires that you identify the names of persons in email messages. Example 1: Identifying the names of persons in email messages Assume that the text of an email message is \"Ena, please send me the document ASAP\". While a human might be able to understand that Ena is a name of a person based on the text of the email, AQL rules that are written to identify names of persons with high precision from general text might be too conservative and not able to draw the same conclusion with high confidence, based on the fact that Ena is a capitalized word. One way to boost the coverage of the rules is to use words from the From , To , and CC fields of the email as more evidence. If the email is addressed to \"Ena Smith,\" and the application makes this information available to the extractor, then the extractor developer can write more AQL rules to boost the coverage of the extractor that is based on the domain knowledge that emails are usually addressed to people. For example, you can write AQL rules to identify person tokens from the email metadata fields. You then use that information as strong clues when you decide whether a capitalized token in the email text is a person name. In general, the email metadata is not part of the actual email message, but the application can make this metadata available to the extractor by using an external view. For each email that must be processed, the application can pass the email text as document text (to populate the view Document ) at run time. It can also pass the extra metadata by using an appropriately defined external view. The following statement defines an external view that is named EmailMetadata . The external view has a schema that contains three fields of type Text. At run time, the view EmailMetadata is automatically populated from an external type named EmailMetadataSrc . You can then reference the EmailMetadata view in your AQL rules, similarly to how you would reference any other view. create external view EmailMetadata (fromAddress Text, toAddress Text, ccAddress Text) external_name 'EmailMetadataSrc';","title":"Examples"},{"location":"aql-ref-guide/#documenting-the-create-external-view-statement-with-aql-doc","text":"The AQL Doc comment for a create external view statement contains the following information: General description about the view. @field for every column name in the view. /** * The external view named EmailMetadata, containing three fields * of type Text. At run time, the view EmailMetadata is * automatically populated from an external type named * EmailMetadataSrc. * * @field from the fromAddress field of the email * @field to the toAddress field of the email * @field cc the ccAddress field of the email */ create external view EmailMetadata(fromAddress Text, toAddress Text, ccAddress Text) external_name 'EmailMetadataSrc';","title":"Documenting the create external view statement with AQL Doc"},{"location":"aql-ref-guide/#file-formats-for-external-artifacts","text":"{: #aql-format-ext} Three types of external artifacts are supported: external views, external dictionaries, and external tables.","title":"File formats for external artifacts"},{"location":"aql-ref-guide/#external-dictionary","text":"The format for the file that contains entries for an external dictionary is defined here: Carriage-return-delimited text file. One dictionary entry per line. Recommended file extension is .dict, but other file extensions can be supported. Entries in the dictionary can consist of multiple tokens. Comments can be specified by preceding the content of the comment with the character, # . Comments can start anywhere in a line. Multi-line comments must contain the # character at the start of each line. Dictionary entries can contain comment characters, if each comment character is escaped with a backslash character. For example, \\# .","title":"External dictionary"},{"location":"aql-ref-guide/#external-table","text":"The supported file format for the content of an external table is a .csv file with header. The following example shows a create external table statement, and the .csv file that specifies the content of this external table. create external table Company2Location (name Text, location Text) allow_empty false; The first line of the .csv file contains the header. The remaining lines contain the data. name,location IBM,USA Infosys,India LG,Korea Vodafone,UK","title":"External table"},{"location":"aql-ref-guide/#external-view","text":"The content of external views can be specified in the following ways: When you run an extractor, you can specify external view content for a data collection only when you are using the JSON input format.","title":"External view"},{"location":"aql-ref-guide/#built-in-functions","text":"{: #aql-inc-func} AQL has a collection of built-in functions for use in extraction rules. Aggregate functions Aggregate functions are used to implement operations (such as counting, mathematical operations, and other operations) across a set of input values. These functions return only one result. Predicate functions Predicate functions test a certain predicate over its input arguments and return a corresponding Boolean value. Scalar functions Scalar functions perform an operation over the values of a field across a set of input tuples and return a non-Boolean value, such as a Span, Text, or Integer. These functions can be used within the select list of a select statement or an extract statement. They can also be used as inputs to predicate functions.","title":"Built-in functions"},{"location":"aql-ref-guide/#aggregate-functions","text":"{: #aql-aggr-func} Aggregate functions are used to implement operations (such as counting, mathematical operations, and other operations) across a set of input values. These functions return only one result. These functions can be used within the select list of a select statement but not within an extract statement. The following example is the general form of an aggregate function call: Aggregate_Function_Name(argument) The argument can be: An expression that consists of a column of a view in the from clause, or a combination of scalar functions that involve columns of the views in the from clause. In most cases, except as noted, null values for argument are ignored. The character * in the special case of the Count(*) aggregate function. In this case, all rows of the output are counted, including null values. Aggregate function Argument type Return type Return value Avg(expression) Integer, Float Float The average of all input values or null if no rows are selected Count(*) Integer The number of all input rows Count(expression) Any Integer The number of all non-null input values List(expression) Integer, Float, Text, Span List of scalar values of the same type as the input argument An unordered list of non-null input values: a bag, not a set, hence might contain duplicates. An empty list if only null values are selected Max(expression) Integer, Float, Text, Span Same as the argument type The maximum element across all input values or null if no rows are selected Min(expression) Integer, Float, Text, Span Same as the argument type The minimum element across all input values or null if no rows are selected Sum(expression) Integer, Float Same as the argument type The sum of all input values or null if no rows are selected Limitations of the current version: The current version of AQL supports the creation of scalar values through the aggregate function List.","title":"Aggregate functions"},{"location":"aql-ref-guide/#examples_29","text":"{: #aql-reference-examples-30} The following example illustrates how aggregate functions can count the number of person name annotations, or compute sets of given names that are associated with each distinct surname that is identified in a document: -- identify occurrences of given names in the document create view FirstName as extract dictionary 'firstNames.dict' on D.text as name from Document D; -- identify occurrences of surnames in the document create view LastName as extract dictionary 'lastNames.dict' on D.text as name from Document D; -- identify complete person names in the document create view Person as select F.name as firstname, L.name as lastname from FirstName F, LastName L where FollowsTok(F.name, L.name, 0, 0); -- count the number of person annotations in the document create view CountPerson as select Count(*) from Person; -- for each distinct surname, output a list of given names associated with it in the document create view FamilyMembers as select GetText(P.lastname) as lastname, List(GetText(P.firstname)) as firstnames from Person P group by GetText(P.lastname); The following example illustrates the use of the Min and Max functions: -- Extract stop words from input text create view StopWords as extract regex /\\s(the|in|a|an|as|to|from)\\s/ on D.text as match from Document D; -- Count the number of times each stop word matched above, was used in the text create view StopWordsCount as select GetText(S.match) as stopword, Count(S.match) as stopwordcount from StopWords S group by GetText(S.match); -- Retrieve the most used and least used stop word count create view StopWordUsageCount as select Min(S.stopwordcount) as least, Max(S.stopwordcount) as most from StopWordsCount S;","title":"Examples"},{"location":"aql-ref-guide/#predicate-functions","text":"{: #aql-pred-func} Predicate functions test a certain predicate over its input arguments and return a corresponding Boolean value. Input arguments to predicate functions include return values of other scalar or aggregate functions in addition to dictionaries, regular expressions, and more. These functions can be employed within the where clause of a select statement, and the having clause of an extract statement.","title":"Predicate functions"},{"location":"aql-ref-guide/#and","text":"The And function accepts a variable number of Boolean arguments and returns the results of a logical AND operation across all the input arguments. The AQL optimizer does not attempt to optimize the order of evaluation of the arguments to this function as part of the logical AND operation. If any input is null , the result is null . Consider this query format: select ... from ... where And(predicate1, predicate2); As a result, a query format that uses the AND operation often runs considerably slower than the same query in the form of: select ... from ... where predicate1 and predicate2; When possible, use the SQL-style and keyword instead of this function.","title":"And"},{"location":"aql-ref-guide/#contains","text":"The Contains function takes two spans as arguments: Contains(<span1>, <span2>) This function returns TRUE if span1 completely contains span2 . If span2 starts at or after the beginning of span1 , and ends at or before the end of span1 , span2 is completely contained. If either argument is null , the function returns null .","title":"Contains"},{"location":"aql-ref-guide/#containsdict","text":"The ContainsDict function checks if the text of a span contains any entry from a given dictionary. This function accepts as input arguments a dictionary, an optional flag specification, and a span to evaluate. ContainsDict('<dictionary>', ['<flags>', ]<span>) The ContainsDict function returns TRUE if the span contains one or more matches of the dictionary. The flags can be Exact or IgnoreCase . If Exact is used, a case-sensitive match is performed against each term in the dictionary. If IgnoreCase is used, the match that is performed against each term in the dictionary is not case-sensitive. If no flag is specified, the dictionary matches based on any flag that was specified when it was created. If no flag was specified during creation, it matches by using the IgnoreCase flag. If the span is null , the function returns null . The following example illustrates the use of the ContainsDict function: create dictionary EmployeePhoneDict as ( '121-222-2346', '121-234-1198', '121-235-8891' ); create view PhoneNum as extract regex /(\\d{3})-(\\d{3}-\\d{4})/ on between 4 and 5 tokens in D.text return group 1 as areaCode and group 2 as restOfNumber and group 0 as number from Document D; create view PhoneNumbers as select P.number as number from PhoneNum P where ContainsDict('EmployeePhoneDict',P.number); Dictionaries are always evaluated on token boundaries. For example, if a dictionary consists of the term fish, no match exists in the text Let's go fishing!.","title":"ContainsDict"},{"location":"aql-ref-guide/#containsdicts","text":"The ContainsDicts function checks if the text of a span contains any entry from any given dictionaries. This function accepts as input arguments two or more dictionaries, an optional flag specification, and a span to evaluate. ContainsDicts('<dictionary>','<dictionary>','<dictionary>', ['<flags>', ]<span>) The ContainsDicts function returns TRUE if the span contains one or more matches from at least one of the specified dictionaries. The flags can be Exact or IgnoreCase . If Exact is used, a case-sensitive match is performed against each of the terms in the dictionaries. If IgnoreCase is used, the match that is performed against each of the terms in the dictionaries is not case-sensitive. If no flag is specified, the dictionary matches based on any flag that was specified when it was created. If no flag was specified during creation, it matches by using the IgnoreCase flag. If either or both arguments are null, then the function returns null . The following example illustrates the use of the ContainsDicts function with the Exact flag: create view PersonWithFirstName as select P.reference as reference from Person P where ContainsDicts( 'FirstNamesUsedGlobally', 'FirstNamesUsedInGermanyLong', 'NickNamesUsedGlobally', 'FirstNamesUsedInGermanyShort', 'FirstNamesUsedInItaly', 'FirstNamesUsedInFrance', 'Exact', P.reference);","title":"ContainsDicts"},{"location":"aql-ref-guide/#containsregex","text":"The ContainsRegex function checks whether the text of a span matches a given regular expression. This function accepts a regular expression with which to match, an optional flag specification, and the input span against which to match. ContainsRegex(/<regular expression>/, ['<flags>', ]<span>) The function returns TRUE if the text of the span, which is taken as a separate Java\u2122 string, contains one or more matches of the regular expression. The function returns null if the span is null . The optional flags affect the matching behavior, similarly to flags used in Java regular expressions. The flags string is formed by combining one or more of the following flags by using | as the separator: CANON_EQ CASE_INSENSITIVE DOTALL LITERAL MULTILINE UNICODE (meaningless without CASE_INSENSITIVE) UNIX_LINES An example of a flags string is 'UNICODE | CASE_INSENSITIVE' Consider this example where ContainsRegex identifies product names along with their version number mentions on either sides. Unlike the example for MatchesRegex , a version number match is not strictly identified by using the regex , but by the context around a product name mention containing a token that matches against the regex . -- dictionary of product names create dictionary ProductNamesDict as ( 'IBM WebSphere Application Server', 'Microsoft Windows', 'Apple Mac OS', 'IBM Rational Application Developer', 'Apache HTTP Server', 'Eclipse', 'Google Android' ); -- extract product names from input text create view ProductNames as extract dictionary 'ProductNamesDict' on D.text as name from Document D; -- gather context around product name mention create view ProductNamesWithContext as select P.name as name, LeftContext(P.name, 5) as leftctxt, RightContext(P.name, 5) as rightctxt from ProductNames P; -- use a regex to identify products with version number mentions on either sides of the product mention create view ProductsWithVersionNumbers as ( select P.name as productname, P.leftctxt as productversion from ProductNamesWithContext P where ContainsRegex (/v\\d((\\.\\d)+)?/, P.leftctxt) ) union all ( select P.name as productname, P.rightctxt as productversion from ProductNamesWithContext P where ContainsRegex (/v\\d((\\.\\d)+)?/, P.rightctxt) );","title":"ContainsRegex"},{"location":"aql-ref-guide/#equals","text":"The Equals function takes two arguments of arbitrary type: Equals(<arg1>, <arg2>) Two spans are considered equal if they both begin and end at the same offsets and contain the same text. If either or both arguments are null, the function returns null . The following example illustrates the use of the Equals function. -- Select phone number spans whose text is equal to 001-543-2217 create view PhoneNumber as select P.number as number from PhoneNum P where Equals('001-543-2217',GetText(P.number));","title":"Equals"},{"location":"aql-ref-guide/#follows","text":"The Follows predicate function takes two span arguments and two integer arguments: Follows(<span1>, <span2>, <minchar>, <maxchar>) The function returns TRUE if the number of characters between the end of span1 and the beginning of span2 is between minchar and maxchar , inclusive. If any argument is null , the function returns null .","title":"Follows"},{"location":"aql-ref-guide/#followstok","text":"The FollowsTok predicate function is a version of Follows ; however, the FollowsTok distance arguments are in terms of tokens instead of characters: FollowsTok(<span1>, <span2>, <mintok>, <maxtok>) The FollowsTok function returns TRUE if the number of tokens between the end of span1 and the beginning of span2 is between mintok and maxtok , inclusive. If any argument is null , the function returns null .","title":"FollowsTok"},{"location":"aql-ref-guide/#greaterthan","text":"The GreaterThan predicate function takes two arguments of arbitrary type: GreaterThan(<arg1>, <arg2>) The function returns TRUE if <arg1> is greater than <arg2> . The function returns FALSE if either argument is null .","title":"GreaterThan"},{"location":"aql-ref-guide/#isnull","text":"The IsNull function tests whether data is, or is not, null. It takes a single argument of any type and returns TRUE if the single argument is null , and FALSE otherwise. The behavior of this predicate and the already defined NotNull predicate is different from all other predicates that return null on null input.","title":"IsNull"},{"location":"aql-ref-guide/#matchesdict","text":"The MatchesDict function takes a dictionary (as in a dictionary extraction), an optional flag specification, and a span as arguments: MatchesDict('<dictionary>', ['<flags>', ]<span>) The MatchesDict function returns TRUE if the span exactly matches one or more of the terms in the dictionary. The flags can be Exact or IgnoreCase . If Exact is used, a case-sensitive match is performed against each term in the dictionary. If IgnoreCase is used, the match that is performed against each term in the dictionary is not case-sensitive. If no flag is specified, the dictionary matches based on any flag that was specified when it was created. If no flag was specified during creation, it matches by using the IgnoreCase flag. If any argument is null , the function returns null . Dictionaries are always evaluated on token boundaries. For example, if a dictionary consists of the term fish, no match exists in the text Let's go fishing!.","title":"MatchesDict"},{"location":"aql-ref-guide/#matchesregex","text":"The MatchesRegex function has a similar syntax to ContainsRegex . Unlike ContainsRegex function, the MatchesRegex function returns TRUE only if the entire text of the span, which is taken as a separate Java string, matches the regular expression. If any argument is null , the function returns null . The optional flags affect the matching behavior similar to flags used in Java regular expressions. MatchesRegex(/<regular expression>/, ['<flags>', ]<span>) The flags string is formed by combining a subset of these flags by using | as the separator: CANON_EQ CASE_INSENSITIVE DOTALL LITERAL MULTILINE UNICODE (meaningless without CASE_INSENSITIVE) UNIX_LINES An example of a flags string is 'UNICODE | CASE_INSENSITIVE' Consider this example where MatchesRegex is used to identify product names along with their version number mentions to the right. Unlike the example in ContainsRegex section, the exact version number is identified as the token immediately following the product name mention. -- gather right context around product name mention create view ProductNamesWithContext as select P.name as name, RightContext(P.name, 5) as rightctxt from ProductNames P; -- use a regex to identify products with version number mentions to the right create view ProductsWithVersionNumbers as select P.name as productname, P.rightctxt as productversion from ProductNamesWithContext P where MatchesRegex (/v\\d((\\.\\d)+)?/, P.rightctxt);","title":"MatchesRegex"},{"location":"aql-ref-guide/#not","text":"The Not function takes a single Boolean argument and returns its complement. If the argument is null , the function returns null .","title":"Not"},{"location":"aql-ref-guide/#notnull","text":"The NotNull function takes a single argument of any type. As its name suggests, the NotNull function returns TRUE if the value of the argument is not null, and FALSE if the argument is null .","title":"NotNull"},{"location":"aql-ref-guide/#or","text":"The Or function takes a variable number of non-null Boolean arguments. If any argument is null , the function returns null . The Or function returns TRUE if any of them evaluates to TRUE .","title":"Or"},{"location":"aql-ref-guide/#overlaps","text":"The Overlaps function takes two span arguments: Overlaps(<span1>, <span2>) The function returns TRUE if the two input spans overlap in the document text. The function returns null if either argument is null .","title":"Overlaps"},{"location":"aql-ref-guide/#scalar-functions","text":"{: #aql-scalar-func} Scalar functions perform an operation over the values of a field across a set of input tuples and return a non-Boolean value, such as a Span, Text, or Integer. These functions can be used within the select list of a select statement or an extract statement. They can also be used as inputs to predicate functions. If a Text object is provided where a Span object is required, a converted Span object is automatically generated, which is based on this Text object, with begin and end offsets covering the whole length of the Text object. If a Span object is provided where a Text object is required, a converted Text object is automatically generated from the text value of the Span object.","title":"Scalar functions"},{"location":"aql-ref-guide/#chomp","text":"The Chomp function is similar to the Chomp operator in Perl, except that Chomp operates over spans instead of strings: Chomp(<span1>) The following example illustrates the use of the Chomp function. detag Document.text as DetaggedDoc annotate element 'a' as Anchor with attribute 'href' as linkTarget; create view Links as select Chomp(A.linkTarget) as link from Anchor A; If the input span contains any white space at the beginning or end, the Chomp function shrinks the span enough to remove the white space. The function then returns a new span with no leading or trailing white space. If the input span has no leading or trailing white space, then the Chomp function returns the same span. If the input span is null , then Chomp returns null .","title":"Chomp"},{"location":"aql-ref-guide/#combinespans","text":"The CombineSpans function takes two spans as input and returns the shortest span that completely covers both input spans if the spans are based on the same text object. CombineSpans(['IgnoreOrder',] <span1>, <span2>) The CombineSpans function is sensitive to the order of its input spans, unless you use the IgnoreOrder parameter. When the optional IgnoreOrder parameter is used, the order of the two spans is ignored. The following example illustrates the use of the CombineSpans function. create view FullName as select CombineSpans('IgnoreOrder',F.name, L.name) as fullName from FirstName F, LastName L where FollowsTok(F.name, L.name, 0,0); The semantics of the function are as follows: If either span1 or span2 is null , or the two spans are over different Text objects, the function returns null . Otherwise, if span1 is smaller than span2 or the IgnoreOrder parameter is used, the function returns the shortest span that covers both span1 and span2 . Otherwise ( span1 is larger than span2 and the IgnoreOrder is not used), the function returns a runtime error. Based on the definition of Span, the different scenarios of arguments to the CombineSpans function are as follows: Span 2 is always after span 1. In other words, left-to-right order is maintained: bash CombineSpans([0,7], [3,7]) returns the span [0,7] CombineSpans([0,7], [8,10]) returns the span [0,10] CombineSpans([0,7], [3,6]) returns the span [0,7] CombineSpans([0,7], [0,7]) returns the span [0,7] Span 2 is not after span 1. In other words, left-to-right order is not maintained: bash CombineSpans(\u2018IgnoreOrder\u2019, [0,10], [0,7]) returns the span [0,10] CombineSpans(\u2018IgnoreOrder\u2019, [3,6], [0,7]) returns the span [0,7] CombineSpans(\u2018IgnoreOrder\u2019, [3,7], [0,7]) returns the span [0,7] CombineSpans(\u2018IgnoreOrder\u2019, [8,10], [0,7]) returns the span [0,10] CombineSpans([3,6], [0,7]) will result in Runtime error as the IgnoreOrder flag has not been specified.","title":"CombineSpans"},{"location":"aql-ref-guide/#getbegin-and-getend","text":"The GetBegin function takes a single span argument and returns the begin offset of the input span. For example, GetBegin([5, 10]) returns the value 5 . Likewise, the GetEnd function returns the end offset of its input span. The following example illustrates the use of the GetBegin and GetEnd function. create view PersonOffsets as select GetBegin(P.name) as offsetBegin, GetEnd(P.name) as offsetEnd from Person P; For both of these functions, if the argument is null , the function returns null .","title":"GetBegin and GetEnd"},{"location":"aql-ref-guide/#getlanguage","text":"The GetLanguage function takes a single span argument and returns the two-letter language code of the source text of the span. If the argument is null , the function returns null . This statement produces meaningful results only if the data source is tagging text fields with the appropriate languages.","title":"GetLanguage"},{"location":"aql-ref-guide/#getlemma","text":"The GetLemma function takes a single Span or Text object as an argument and returns a string that contains the lemmatized form of the input span. If the argument is null , the function returns null . With dictionary entries for lemma match, this function can determine the lemmatized form of various tokens as returned by the tokenizer. For example, for the span went shopping GetLemma returns the lemma string go shop . The results of this function follow these rules: If the input span starts at the beginning of a token and ends at the end of a token, the result contains the sequence of lemmas that begins with the lemma of the first token, followed by a white space, followed by the lemma of the second token, followed by a white space, and so on (for example, dog cat fish bird ...). If the lemma for a token consists of white spaces, escape the white space by using the backslash character ( \\ ). If the input span starts or ends with white space (for example, it starts between two tokens or ends between two tokens), the function ignores the beginning and trailing white space. If the input span starts in the middle of a token or ends in the middle of a token, then the output consists of the following content, in this order, and separated by a white space: The surface form of the first partial token if it exists. The sequence of lemmas that correspond to the first to last complete tokens. If the lemma for any of the complete tokens consists of white spaces, escape the white spaces by using the backslash character (\\). The surface form of the last partial token if it exists. This function returns an error if the tokenizer that is being used is not capable of producing lemmas. You can use the GetLemma() function to create dictionaries of lemmatized forms. Call GetLemma() on an input that contains the terms whose lemmatized form you want to include in the dictionary.","title":"GetLemma"},{"location":"aql-ref-guide/#getlength","text":"The GetLength function takes a single span argument and returns the length of the input span. If the argument is null , the function returns null . For example, GetLength([5, 12]) returns a value of 7.","title":"GetLength"},{"location":"aql-ref-guide/#getlengthtok","text":"The GetLengthTok function takes a single span argument and returns the length of the input span in tokens. If the input argument is null , the function returns null .","title":"GetLengthTok"},{"location":"aql-ref-guide/#getstring","text":"The GetString function takes a single AQL object as its argument and returns a Text object that is formed from the string representation of the object. For span and text arguments, the values that are returned are different from those returned by GetText() . For Text objects, the returned value includes single quotation marks surrounding the text string. For span objects, the returned value includes in addition offsets in brackets. For scalar lists, this function returns the GetString() values of the elements of the list, concatenated with semicolons. For Integer, Float, Boolean, and String arguments, this function returns the value of the argument as a string. For null arguments, this function returns null .","title":"GetString"},{"location":"aql-ref-guide/#gettext","text":"The GetText function takes a single span or text as an argument. For span input, it returns the text object based on the actual text string that the span marks. For text input, it returns the input text object. If the input is null , then this function returns null . For example: GetText([5, 12]) The span returns the substring of the document from character position 5 - 12. The GetText function has two primary uses. Testing for string equality between the text marked by two spans. -- Create a dictionary of company names create dictionary CompanyNames as ('IBM', 'BigCorp', 'Initech'); -- Find all matches of the company names dictionary. create view Company as extract dictionary 'CompanyNames' on D.text as company from Document D; -- Create a table that maps company names to locations of -- corporate headquarters. create table NameToLocation (name Text, location Text) as values ('IBM', 'USA'), ('BigCorp', 'Apex'), ('Initech', 'Dallas'), ('Acme Fake Company Names', 'Somewhere'); -- Use the table to augment the Company view with location -- information. create view CompanyLoc as select N2C.location as loc, C.company as company from Company C, NameToLocation N2C where Equals(GetText(C.company), GetText(N2C.name)); output view CompanyLoc; Splitting a document into smaller subdocuments. For example, if the main document is a blog that consists of multiple blog entries, you can use GetText to create a subdocument for each blog entry. detag Document.text as DetaggedBlog annotate element 'blog' as Blog with attribute 'name' as title; create view BlogEntry as select B.match as entry, B.title as title from Blog B; -- Turn each tuple in the BlogEntry view into a sub-document create view BlogEntryDoc as select GetText(B.title) as title, GetText(B.entry) as body from BlogEntry B; output view BlogEntryDoc; --Dictionary for Companies create dictionary CompanyNameDict as ( 'A Corporation', 'B Corporation' ); -- Run an extraction over the sub-documents. -- The spans that this \"extract\" statement creates will have -- offsets relative to the blog entries themselves, as opposed -- to the original multi-entry document. create view CompanyName as extract dictionary 'CompanyNameDict' on B.body as name from BlogEntryDoc B; output view CompanyName;","title":"GetText"},{"location":"aql-ref-guide/#leftcontext-and-rightcontext","text":"The LeftContext function takes a Span and an Integer as input: LeftContext(<input span>, <nchars>) The LeftContext(<input span>, <nchars>) function returns a new span that contains the nchars characters of the document immediately to the left of <input span> . If the input span starts less than <nchars> characters from the beginning of the document, then LeftContext() returns a span that starts at the beginning of the document and continues until the beginning of the input span. For example, LeftContext([20, 30], 10) returns the span [10, 20]. The span LeftContext([5, 10], 10) returns [0, 5]. If the input starts on the first character of the document, LeftContext() returns a zero-length span. Similarly, the RightContext function returns the text to the right of its input span. For both functions, if either argument is null , the function returns null .","title":"LeftContext and RightContext"},{"location":"aql-ref-guide/#leftcontexttok-and-rightcontexttok","text":"The LeftContextTok and RightContextTok functions are versions of LeftContext and RightContext that take distances in terms of tokens: LeftContextTok(<input span>, <num tokens>) RightContextTok(<input span>, <num tokens>) The following example illustrates the use of the RightContextTok function. create view Salutation as extract regex /Mr\\.|Ms\\.|Miss/ on D.text as salutation from Document D; --Select the token immediately following a Salutation span create view NameCandidate as select RightContextTok(S.salutation, 1) as name from Salutation S; For both functions, if either argument is null , the function returns null .","title":"LeftContextTok and RightContextTok"},{"location":"aql-ref-guide/#remap","text":"The Remap function takes a single span argument: Remap(<span>) If the input span is over a text object that was produced by transforming another text object , the Remap function converts the span into a span over the original \"source\" text. For example, if the span N.name is over a detagged document that is produced by running HTML through the AQL detag statement, then Remap(<N.name>) returns an equivalent span over the original HTML. If the span argument was produced by running the detag statement over an empty document, the function remaps the spans to the beginning of the document (in other words, Document.text[0-0]). Also, if the detag statement produces an empty string, the function remaps the spans to the beginning of the document. The only part of AQL that produces such derived text object is the detag statement. The following example illustrates the use of the Remap function: -- Detag the HTML document and annotate the anchor tags detag Document.text as DetagedDoc annotate element 'a' as Anchor; -- Remap the Anchor Tags create view AnchorTag as select Remap(A.match) as anchor from Anchor A; If the argument to Remap is not a derived text object or a span over a derived text object , the function generates an error. If the argument is null , the function returns null .","title":"Remap"},{"location":"aql-ref-guide/#spanbetween","text":"The SpanBetween function takes two spans as input and returns the span that exactly covers the text between the two spans if the spans are based on the same text object, and returns null if they are based on different text objects: SpanBetween(['IgnoreOrder',] <span1>, <span2>) When the optional IgnoreOrder parameter is used, the order of the two spans is ignored by the AQL compiler. If no text exists between the two spans, then SpanBetween returns an empty span that starts at the end of <span1> . Like CombineSpans ,\u001d SpanBetween is sensitive to the order of its inputs, unless you use the IgnoreOrder parameter. So SpanBetween([5, 10], [50, 60]) returns the span [10, 50] , while SpanBetween([50, 60], [5, 10]) returns the span [60, 60] . If the argument to SpanBetween is null , then the function returns null .","title":"SpanBetween"},{"location":"aql-ref-guide/#spanintersection","text":"The SpanIntersection function takes two spans as input and returns a span that covers the text that both inputs cover if the spans are based on the same text object, and returns null if they are based on different text objects: SpanIntersection(<span1>, <span2>) For example, SpanIntersection([5, 20], [10, 60]) returns the span [10, 20] , while SpanIntersection([5, 20], [7, 10]) returns the span [7, 10] . If the two spans do not overlap, then SpanIntersection returns null . If either span input is null , the function returns null .","title":"SpanIntersection"},{"location":"aql-ref-guide/#subspantok","text":"The SubSpanTok function takes as input a span and a pair of offsets into the span: SubSpanTok(<span>, <first_tok>, <last_tok>) As the name of the function suggests, the <first_tok> and <last_tok> arguments are distances in tokens, according to whatever tokenizer the system is configured to use. The SubSpanTok function returns a new span that covers the indicated range of tokens, inclusive, within the input span. If the specified range starts inside the span and goes beyond the end of the span, then the portion of the range that is contained is returned. If <first_tok> represents a distance beyond the target span, then SubSpanTok returns a zero-length span that starts at the beginning of the input span. If any input is null , the function returns null .","title":"SubSpanTok"},{"location":"aql-ref-guide/#tolowercase","text":"The ToLowerCase function takes a single object as its argument and returns a lowercase string representation of the object . The conversion to a string occurs in the same way that the GetString() function performs the conversion. The primary use of this function is to perform equality joins that are not case-sensitive: where Equals(ToLowerCase(C.company), ToLowerCase(N2C.name)) If the input object is null , the function returns null .","title":"ToLowerCase"},{"location":"aql-ref-guide/#the-create-function-statement","text":"{: #aql-create-func} To perform operations on extracted values that are not supported by AQL, you can define custom functions to use in extraction rules called user-defined functions (UDFs). AQL supports user-defined scalar functions and user-defined table functions. Java\u2122 and PMML are the only supported implementation language for UDFs. A scalar function returns a single scalar value, and a table function returns one or more tuples, in other words, a table. You implement user-defined functions (UDFs) by following four steps: Implementing the function AQL supports user-defined functions (UDFs) that are implemented in Java or PMML. 2. Declaring it in AQL. You can make the user-defined scalar functions and machine learning models from PMML files available to AQL by using the create function statement. 3. Using it in AQL. User-defined functions work with AQL statements and clauses. 4. Debugging the UDF. Because Java based user-defined functions (UDFs) are implemented as public methods in Java classes, you debug your UDFs in the same way that you debug your Java programs.","title":"The create function statement"},{"location":"aql-ref-guide/#implementing-user-defined-functions","text":"{: #aql-user-def-imp} AQL supports user-defined functions (UDFs) that are implemented in Java\u2122 or PMML. This section specifically focuses on UDFs that are implemented in Java. For UDFs implemented in PMML, the machine learning model is stored inside the PMML XML file. Refer to the documentation of PMML for how to create these models: [http://dmg.org/pmml/v4-1/GeneralStructure.html] You can implement a Scalar UDF as a public method in a Java class. You can implement a table UDF as a public method in a Java class that extends the API com.ibm.avatar.api.udf.TableUDFBase. In addition, a table UDF can optionally override the method initState() of the superclass com.ibm.avatar.api.udf.TableUDFBase. If the UDF has an input parameter of type Span, Text, or ScalarList, or the return type of the UDF is Span, Text, or ScalarList, the Java class must import the classes com.ibm.avatar.algebra.datamodel.Span, com.ibm.avatar.algebra.datamodel.Text, or com.ibm.avatar.algebra.datamodel.ScalarList to compile. Table functions require extra APIs to provide output schema information. These APIs belong to the base class com.ibm.systemt.api.udf.TableUDFbase. If a subclass contains more than one table function, a separate Java object is created for each instance. For the UDF code to retrieve non-class resources from its JAR file, the only supported method is getResourceAsStream() . Other methods for accessing resources (such as getResource() , getResources() , getSystemResource() ) are not supported. For example, a UDF JAR file that contains the properties file my.properties in the package com.ibm.myproject.udfs, can access it with the following Java statement: InputStream in = this.getClass().getClassLoader(). getResourceAsStream(\u201ccom/ibm/myproject/udfs/my.properties\u201d);","title":"Implementing user-defined functions"},{"location":"aql-ref-guide/#lifecycle-of-udfs-implemented-in-java","text":"The following operations are performed when the extractor is compiled (the CompileAQL.compile() API), instantiated ( OperatorGraph.createOG() API) and validated ( OperatorGraph.validateOG() API), exactly once for each create function statement in AQL: Load the Java class that contains the UDF method, using a fresh new class loader. The class is searched inside the UDF JAR specified in the corresponding create function statement. Any other classes that are required when loading this class are also searched inside the same UDF JAR, and if not found, the search is delegated to the classloader that instantiated the Runtime. Create an instance of that class. For table UDFs, invoke the method initState() . Since those steps are performed for each create function statement, if a Java class contains multiple UDF methods (and all methods are used in AQL in different create function statements), then the class is loaded multiple times, each time in a separate class loader (Step 1). Furthermore, multiple instances of the class are created (Step 2) and the method initState() is called once for each instance (Step 3). The reason every UDF results in a separate class loader is to allow different UDFs to use different versions of the same class (or library). At runtime ( OperatorGraph.execute() API), the UDF class is not loaded again because the class is loaded during extractor instantiation. The Java method that implements the UDF is invoked when necessary to compute pieces of the output. When the extractor is used within a single thread, that means anywhere from zero to possibly many invocations for each input document exist (and most likely multiple invocations throughout the life of the Operator Graph object). When the extractor is shared across multiple threads, different threads can hit the method around the same time (with different inputs). If you require a specific part of the UDF code to be evaluated exactly once, for example, to initialize data structures needed by the UDF method across invocations, that code should not be placed in the UDF evaluation method, since that method is most likely executed multiple times throughout the life of the extractor (like the OperatorGraph object), and can be hit almost simultaneously when the extractor is shared across multiple threads. Instead: For scalar UDFs, follow standard Java programming principles. For example, place the code in a static block. For table UDFs, place the code in the initState() method, or follow standard Java programming principles. For example, place the code in a static block. When doing so, remember that the class might be loaded multiple times during extractor compilation, instantiation and validation, as explained in Steps 1-3 above. If the code that initializes the UDF is placed in a static block, that code is executed each time that the class is loaded (potentially multiple times), therefore, introducing an overhead during compilation and operator graph instantiation. If the overhead is large, follow these best practices: To minimize overhead during compilation time, the best practice is to place compile-time heavy resources like large dictionaries or UDFs with large initialization time in a separate AQL module and export them. Try compiling only the AQL modules that changed and other modules that depending on them. To minimize overhead during operator graph instantiation and validation: If the initialization code is required by a single UDF method, then place that UDF method in a separate Java class (and place the heavy instantiation code in a static initializer as before, or use some other mechanism that ensures the code is executed once). If the initialization code is shared by multiple UDF methods, then the Runtime and AQL do not provide an explicit mechanism to ensure that initialization code is executed exactly once. In such a situation, if the initialization time is prohibitive, the only solution is to place the shared resources and initialization code on the system class path. In other words, place the initialization code in a new class MySeparateClass.java, and store the initialization result in a static variable of this class, such as MySeparateClass.myVar. Package MySeparateClass.java along with any resources that are needed during initialization in a JAR file and place that JAR on the system class path. The UDF methods can refer to the initialized model using the MySeparateClass.myVar static variable. The initialization code is now executed exactly once - when the class MySeparateClass.java is loaded by the system classloader. Tip: The compiled representation of a module (the .tam file) contains the serialized binary code of the UDF. Therefore, if the same UDF code is referenced by create function statements in two different modules, the UDF code is serialized in the compiled representation of both modules. In other words, the UDF code is serialized twice. In such cases, you can avoid the redundancy by creating a separate module that acts as a library of UDFs, and reuse that library in other modules. To create a library of UDFs, follow these steps: Create one module. Place all UDF JAR files inside that module. Define all necessary functions with the create function statements. Export them all with export function statements so that they can be imported and used in other modules.","title":"Lifecycle of UDFs implemented in Java"},{"location":"aql-ref-guide/#examples_30","text":"{: #aql-reference-examples-31} Example 1: Implementing a scalar UDF This example shows the implementation of a scalar UDF named toUpperCase. This UDF takes as input a value of type Span and outputs a string value that consists of the text content of the input Span, converted to uppercase. package com.ibm.biginsights.textanalytics.udf; import com.ibm.avatar.algebra.datamodel.Span; /** * @param s input Span * @return all upper-case version of the text of the input Span */ public String toUpperCase(Span s) { return s.getText().toUpperCase(); } Example 2: Implementing a scalar UDF that uses a table as input This example shows the implementation of a scalar UDF named TableConsumingScalarFunc. This UDF takes as input two lists of tuples and outputs a string value that concatenates the content of the two input tuple lists. import com.ibm.avatar.algebra.datamodel.TupleList; /** * Example implementation of a user-defined scalar function using a table as input */ public class TableConsumingScalarFunc { /** * Main entry point to the `scalar` function. This function takes two lists of tuples and concatenates them into a single string. * * @param arg1 first set of tuples to merge * @param arg2 second set of tuples to merge * @return the two sets of tuples, concatenated */ public String eval (TupleList arg1, TupleList arg2) { StringBuilder sb = new StringBuilder (); sb.append(\"Input1: \"); sb.append(arg1.toPrettyString ()); sb.append(\"\\nInput2: \"); sb.append(arg2.toPrettyString ()); return sb.toString (); } } Example 3: Implementing a table UDF that uses a table as input This example shows the implementation of a table UDF named TableConsumingTableFunc. This UDF takes as input two lists of tuples and outputs a single list of tuples that contains tuples from the first input interspersed with tuples from the second input. Notice that the implementation extends from the base class com.ibm.avatar.api.udf.TableUDFBase, which provides APIs for obtaining the input and output schemas. package com.ibm.test.udfs; import java.lang.reflect.Method; import com.ibm.avatar.algebra.datamodel.AbstractTupleSchema; import com.ibm.avatar.algebra.datamodel.FieldCopier; import com.ibm.avatar.algebra.datamodel.Tuple; import com.ibm.avatar.algebra.datamodel.TupleList; import com.ibm.avatar.algebra.datamodel.TupleSchema; import com.ibm.avatar.api.exceptions.TableUDFException; import com.ibm.avatar.api.udf.TableUDFBase; /** * Example implementation of a user-defined table function */ public class TableConsumingTableFunc extends TableUDFBase { /** Accessors for copying fields from input tuples to output tuples. */ private FieldCopier arg1Copier, arg2Copier; /** * Main entry point to the `table` function. This function takes two lists of tuples and generates a new list of wide * tuples, where element i of the returned list is created by joining element i of the first input with element i of * the second input. * * @param arg1 first set of tuples to merge * @param arg2 second set of tuples to merge * @return the two sets of tuples, interleaved */ public TupleList eval (TupleList arg1, TupleList arg2) { TupleSchema retSchema = getReturnTupleSchema (); TupleList ret = new TupleList (retSchema); // We skip any records that go off the end int numRecs = Math.min (arg1.size (), arg2.size ()); for (int i = 0; i < numRecs; i++) { Tuple retTuple = retSchema.createTup (); Tuple inTup1 = arg1.getElemAtIndex (i); Tuple inTup2 = arg2.getElemAtIndex (i); arg1Copier.copyVals (inTup1, retTuple); arg2Copier.copyVals (inTup2, retTuple); // System.err.printf (\"%s + %s = %s\\n\", inTup1, inTup2, retTuple); ret.add (retTuple); } return ret; } /** * Initialize the internal state of the `table` function. In this case, we create accessors to copy fields from input * tuples to output tuples. * * @see com.ibm.avatar.api.udf.TableUDFBase#initState() for detailed description */ @Override public void initState () throws TableUDFException { // Create accessors to do the work of copying fields from input tuples to output tuples AbstractTupleSchema arg1Schema = getRuntimeArgSchema ().getFieldTypeByIx (0).getRecordSchema (); AbstractTupleSchema arg2Schema = getRuntimeArgSchema ().getFieldTypeByIx (1).getRecordSchema (); TupleSchema retSchema = getReturnTupleSchema (); // Create offsets tables for a straightforward copy. String[] srcs1 = new String[arg1Schema.size ()]; String[] dests1 = new String[arg1Schema.size ()]; String[] srcs2 = new String[arg2Schema.size ()]; String[] dests2 = new String[arg2Schema.size ()]; for (int i = 0; i < srcs1.length; i++) { srcs1[i] = arg1Schema.getFieldNameByIx (i); dests1[i] = retSchema.getFieldNameByIx (i); } for (int i = 0; i < srcs2.length; i++) { srcs2[i] = arg2Schema.getFieldNameByIx (i); dests2[i] = retSchema.getFieldNameByIx (i + srcs1.length); } arg1Copier = retSchema.fieldCopier (arg1Schema, srcs1, dests1); arg2Copier = retSchema.fieldCopier (arg2Schema, srcs2, dests2); } /** * Check the validity of tuple schemas given in the AQL \u201ccreate function\u201d. * * @see com.ibm.avatar.api.udf.TableUDFBase#validateSchema(TupleSchema, TupleSchema, TupleSchema, Method, Boolean) for * description of arguments */ @Override public void validateSchema (TupleSchema declaredInputSchema, TupleSchema runtimeInputSchema, TupleSchema returnSchema, Method methodInfo, boolean compileTime) throws TableUDFException { // The output schema should contain the columns of the two input schemas in order. AbstractTupleSchema arg1Schema = declaredInputSchema.getFieldTypeByIx (0).getRecordSchema (); AbstractTupleSchema arg2Schema = declaredInputSchema.getFieldTypeByIx (1).getRecordSchema (); System.err.printf (\"TableConsumingTableFunc: Input schemas are %s and %s\\n\", arg1Schema, arg2Schema); // First check sizes if (returnSchema.size () != arg1Schema.size () + arg2Schema.size ()) { throw new TableUDFException ( \"Schema sizes don't match (%d + %d != %d)\", arg1Schema.size (), arg2Schema.size (), returnSchema.size ()); } // Then check types for (int i = 0; i < arg1Schema.size (); i++) { if (false == (arg1Schema.getFieldTypeByIx (i).equals (returnSchema.getFieldTypeByIx (i)))) { throw new TableUDFException ( \"Field type %d of output doesn't match corresponding field of first input arg (%s != %s)\", i, returnSchema.getFieldTypeByIx (i), arg1Schema.getFieldTypeByIx (i)); } } for (int i = 0; i < arg2Schema.size (); i++) { if (false == (arg2Schema.getFieldTypeByIx (i).equals (returnSchema.getFieldTypeByIx (i + arg1Schema.size ())))) { throw new TableUDFException ( \"Field type %d of output doesn't match corresponding field of first input arg (%s != %s)\", i + arg1Schema.size (), returnSchema.getFieldTypeByIx (i + arg1Schema.size ()), arg2Schema.getFieldTypeByIx (i)); } } } }","title":"Examples"},{"location":"aql-ref-guide/#declaring-user-defined-functions","text":"{: #aql-user-def-dec} You can make the user-defined scalar functions and machine learning models from PMML files available to AQL by using the create function statement.","title":"Declaring user-defined functions"},{"location":"aql-ref-guide/#syntax_29","text":"{: #aql-declare-syntax} The general syntax of the create function statement is as follows: create function <function-name>(<input-schema-definition>) return <return-type> [like <column-name>] | table ( <output-schema-definition) external_name <ext-name> language [java | pmml] [deterministic | not deterministic] [return null on null input | called on null input]; <input-schema-definition> <column-name> <data-type> | table (<output-schema-definition>) as locator [,<column-name> <data-type> | table (<output-schema-definition>) as locator ]* <output-schema-definition> <column-name> <data-type> [,<column-name> <data-type>]*","title":"Syntax"},{"location":"aql-ref-guide/#description_29","text":"{: #aql-declare-desc} <function-name> The <function-name> declares the AQL name of the UDF. The UDF is referred to in the AQL code with this name <input-schema-definition> Specifies the input parameters of the UDF. An input parameter has a name, which is specified as <column-name> , and can be either a scalar type or a table locator. When the language is PMML, the function must take a single table that is called params as the argument. <column-name> Specifies the name of a column in the input or the output of the UDF. <data-type> The type of an input scalar parameter to the UDF, the type of a column in the schema of an input table to the UDF or in the schema of the output table of the UDF. Possible values for <data-type> are Integer, Float, String, Text, Span, Boolean, or ScalarList. table (<output-schema-definition>) as locator This input type allows a function to take as input the entire contents of a given AQL view as computed on the current document. The locator parameter references views or tables as arguments. <return-type> For scalar UDFs, the <return-type> specifies the type of the scalar value that is returned by the UDF. Possible values for the return type are: Integer, Float, String, Text, Span, Boolean, or ScalarList. If the return type is Integer, the Java\u2122 function that implements the UDF returns objects of type Integer. The UDF implementation cannot return the primitive int type. If the return type is Text or Span, specify the input parameter from which the return type is derived. You can specify the input parameter by using the optional like specification, since spans are always from an underlying column. If the return type is ScalarList, specify the input parameter from which the scalar type inside the ScalarList is to be inferred. When the language is PMML, the function must return a table. <output-schema-definition> For table UDFs, the <output-schema-definition> specifies the output schema of the table that is returned by the UDF, including the column names and their types. <ext-name> The external_name specifies where to find the implementation of the UDF. When the language is Java, it is a string of the form '<jar-file-name>:<fully-qualified-class-name>!<method-name>' , which consists of three parts: JAR file name: When you compile modular AQL, the location references of UDF JAR files must be relative to the root of the module in which the reference is made. Class name: Fully qualified class name that contains the UDF implementation Method name: The method must be a public method of the class. The method signature must match the parameter types that are specified in the create function statement. Automatic type conversion is not done by the runtime component. When the language is PMML, the external_name clause specifies a string that is the location of the PMML file relative to the module\u2019s root directory. The current implementation supports models that are expressed by using the PMML standard version 4.1 and evaluates them using the version 1.0.22 org.jpmml library. language The language specification points to the language of implementation of the UDF. The runtime component supports only UDFs that are implemented in Java\u2122. deterministic/not deterministic The optional deterministic/not deterministic specifies whether the function is stateless. A deterministic function always returns the same value for the same set of input values. return null on null input The optional return null on null input specifies the function behavior when one or more of the input values are null. If return null on null input is specified, the system returns null on null input without invoking the UDF. If called on null input is specified, the UDF is invoked even for null input values.","title":"Description"},{"location":"aql-ref-guide/#usage-notes-for-udfs-implemented-in-pmml","text":"{: #aql-declare-usage} Functions that are created from PMML files take a single table that is called params as their argument and output a table. The implementation of the function maps input fields between the input and output schema declared in the create function statement and the schema that is specified in the PMML file. In other words, the DataDictionary section that describes the names and types of fields that can appear in the input and output records that the model produces and consumes, the MiningSchema section that tells which named files defined in the DataDictionary section are in each tuple that represents a feature vector ,and the Output section that tells which named fields defined in the DataDictionary section are present in each tuple of the external representation of the output of the model. These functions must be table functions; each row in the table is passed to the PMML model and produce an output row. This schema information is necessary because the PMML and AQL type systems do not match perfectly. For example, PMML has several types to represent timestamps, while AQL currently requires users to encode timestamps as string values. The schema information also allows developers who know AQL but not PMML to understand the AQL rule set. The AQL compiler checks the input schema against the input schema of the PMML file to ensure that the two schemas are compatible. If the two schemas contain fields with the same name but incompatible types, compilation fails with an appropriate error message. If the function\u2019s input or output schemas contain extra or missing columns, the resulting function ignores these columns and does not generate an error. The order of column names can be different between the AQL definition and the PMML file. If a column name appears in both input and output schemas but not in the PMML schema, then the values of that column are passed to the output of the function.","title":"Usage notes for UDFs implemented in PMML"},{"location":"aql-ref-guide/#examples_31","text":"{: #aql-reference-examples-32} Example 1: Declaring scalar UDFs with scalar values as input using Java The following example shows a create function statement that declares a function that is named udfCombineSpans . The user-defined function takes in two spans as input and returns a merged span similar to the first input span. The actual UDF Java\u2122 function is packaged in a JAR file that is named udfs.jar , under the udfjars directory. The method that implements the function is combineSpans in class com.ibm.test.udfs.MiscScalarFunc . The method also declares a function udfSpanGreaterThan that returns true if the span is greater than the specified size. /** * A function to combine two spans and return the resultant span from beginOffset of first span, to endOffset of second span * @param p1 first input span * @param p2 second input span * @return a span from beginOffset of p1 till endOffset of p2 */ create function udfCombineSpans(p1 Span, p2 Span) return Span like p1 external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!combineSpans' language java deterministic return null on null input; /** * A function to compare an input Span's size against an input size * @param span input span * @param size input size to be compared against * @return Boolean true if input span's size is greater than input argument size, else false */ create function udfSpanGreaterThan(span Span, size Integer) return Boolean external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!spanGreaterThan' language java deterministic; The next example shows how to declare a function that is named udfCompareNames that is given a list of names nameList and a single name myName. The output is a list similar to the input list, which contains only the entries from nameList similar to myName. /** * A function to compare an input string against a list of names * and returns a list of entries from nameList similar to myName. * @param nameList a list of names * @param myName a name to compare against the list * @return a list of entries from nameList similar to myName */ create function udfCompareNames(nameList ScalarList, myName String) return ScalarList like nameList external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!compareNames' language java deterministic; Example 2: Declaring scalar UDFs with tables as input using Java The following example illustrates the use of the table as locator input type. It shows a create function statement that declares a function that is called MyScalarFunc. The Java\u2122 implementation of the UDF is packaged inside the class com.ibm.test.udfs.TableConsumingScalarFunc. This UDF takes as input two lists of tuples and outputs a string value that concatenates the content of the two input tuple lists. The Java\u2122 implementation of the UDF is included in Implementing user-defined functions . -- Declare a simple scalar function that turns two tables into a big string create function MyScalarFunc( firstArg table( spanVal Span ) as locator, secondArg table( spanVal Span, strVal Text ) as locator ) return String external_name 'udfjars/udfs.jar:com.ibm.test.udfs.TableConsumingScalarFunc!eval' language java deterministic called on null input; Example 3: Declaring table UDFs using Java The following example illustrates the use of the return table clause. The example shows a create function statement that declares a table function called MyScalarFunc. The Java\u2122 implementation of the UDF is packaged inside the class com.ibm.test.udfs.TableConsumingTableFunc. This UDF takes as input two lists of tuples and outputs a list of tuples that merges together the input lists. The Java\u2122 implementation of the UDF is included in Implementing user-defined functions . -- Declare a simple table function that \"zips together\" two input tables create function MyTableFunc( firstArg table( spanVal Span ) as locator, secondArg table( spanVal Span, strVal Text ) as locator ) return table( outSpan1 Span, outSpan2 Span, outStr Text) external_name 'udfjars/udfs.jar:com.ibm.test.udfs.TableConsumingTableFunc!eval' language java deterministic called on null input; Example 4: Declaring a table UDF implemented in PMML The following example shows a create function statement that declares a table function named IrisDecisionTree using PMML language. The PMML implementation of the UDF is specified in the IrisTree.xml file, which is shown in the following example. The model that is stored in the IrisTree.xml is a decision tree model. -- Scoring function based on a decision tree model stored in IrisTree.xml create function IrisDecisionTree( params table( sepal_length Float, sepal_width Float, petal_length Float, petal_width Float ) as locator ) return table( class Text, actual_class Text, -- This PMML file also defines additional output fields \"Probability_Iris-setosa\" Float, \"Probability_Iris-versicolor\" Float, \"Probability_Iris-virginica\" Float) external_name 'IrisTree.xml' language pmml; IrisTree.xml <?xml version=\"1.0\"?> <PMML version=\"3.2\" xmlns=\"http://www.dmg.org/PMML-3_2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.dmg.org/PMML-3_2 http://www.dmg.org/v3-2/pmml-3-2.xsd\"> <Header copyright=\"Copyright (c) 2012 DMG\" description=\"RPart Decision Tree Model\"> <Extension name=\"user\" value=\"DMG\" extender=\"Rattle/PMML\"/> <Application name=\"Rattle/PMML\" version=\"1.2.29\"/> <Timestamp>2012-09-27 12:46:08</Timestamp> </Header> <DataDictionary numberOfFields=\"5\"> <DataField name=\"class\" optype=\"categorical\" dataType=\"string\"> <Value value=\"Iris-setosa\"/> <Value value=\"Iris-versicolor\"/> <Value value=\"Iris-virginica\"/> </DataField> <DataField name=\"sepal_length\" optype=\"continuous\" dataType=\"double\"> <Interval closure=\"closedClosed\" leftMargin=\"4.3\" rightMargin=\"7.9\"/> </DataField> <DataField name=\"sepal_width\" optype=\"continuous\" dataType=\"double\"> <Interval closure=\"closedClosed\" leftMargin=\"2\" rightMargin=\"4.4\"/> </DataField> <DataField name=\"petal_length\" optype=\"continuous\" dataType=\"double\"> <Interval closure=\"closedClosed\" leftMargin=\"1\" rightMargin=\"6.91\"/> </DataField> <DataField name=\"petal_width\" optype=\"continuous\" dataType=\"double\"> <Interval closure=\"closedClosed\" leftMargin=\"0.1\" rightMargin=\"2.5\"/> </DataField> </DataDictionary> <TreeModel modelName=\"RPart_Model\" functionName=\"classification\" algorithmName=\"rpart\" splitCharacteristic=\"binarySplit\" missingValueStrategy=\"defaultChild\"> <MiningSchema> <MiningField name=\"class\" usageType=\"predicted\"/> <MiningField name=\"sepal_length\" usageType=\"supplementary\"/> <MiningField name=\"sepal_width\" usageType=\"supplementary\"/> <MiningField name=\"petal_length\" usageType=\"active\"/> <MiningField name=\"petal_width\" usageType=\"supplementary\"/> </MiningSchema> <Output> <OutputField name=\"class\" optype=\"categorical\" dataType=\"string\" feature=\"predictedValue\"/> <OutputField name=\"Probability_Iris-setosa\" optype=\"continuous\" dataType=\"double\" feature=\"probability\" value=\"Iris-setosa\"/> <OutputField name=\"Probability_Iris-versicolor\" optype=\"continuous\" dataType=\"double\" feature=\"probability\" value=\"Iris-versicolor\"/> <OutputField name=\"Probability_Iris-virginica\" optype=\"continuous\" dataType=\"double\" feature=\"probability\" value=\"Iris-virginica\"/> </Output> <Node id=\"1\" score=\"Iris-virginica\" recordCount=\"105\" defaultChild=\"3\"> <True/> <ScoreDistribution value=\"Iris-setosa\" recordCount=\"33\" confidence=\"0.314285714285714\"/> <ScoreDistribution value=\"Iris-versicolor\" recordCount=\"35\" confidence=\"0.333333333333333\"/> <ScoreDistribution value=\"Iris-virginica\" recordCount=\"37\" confidence=\"0.352380952380952\"/> <Node id=\"2\" score=\"Iris-setosa\" recordCount=\"33\"> <SimplePredicate field=\"petal_length\" operator=\"lessThan\" value=\"2.6\"/> <ScoreDistribution value=\"Iris-setosa\" recordCount=\"33\" confidence=\"1\"/> <ScoreDistribution value=\"Iris-versicolor\" recordCount=\"0\" confidence=\"0\"/> <ScoreDistribution value=\"Iris-virginica\" recordCount=\"0\" confidence=\"0\"/> </Node> <Node id=\"3\" score=\"Iris-virginica\" recordCount=\"72\" defaultChild=\"7\"> <SimplePredicate field=\"petal_length\" operator=\"greaterOrEqual\" value=\"2.6\"/> <ScoreDistribution value=\"Iris-setosa\" recordCount=\"0\" confidence=\"0\"/> <ScoreDistribution value=\"Iris-versicolor\" recordCount=\"35\" confidence=\"0.486111111111111\"/> <ScoreDistribution value=\"Iris-virginica\" recordCount=\"37\" confidence=\"0.513888888888889\"/> <Node id=\"6\" score=\"Iris-versicolor\" recordCount=\"37\"> <SimplePredicate field=\"petal_length\" operator=\"lessThan\" value=\"4.85\"/> <ScoreDistribution value=\"Iris-setosa\" recordCount=\"0\" confidence=\"0\"/> <ScoreDistribution value=\"Iris-versicolor\" recordCount=\"34\" confidence=\"0.918918918918919\"/> <ScoreDistribution value=\"Iris-virginica\" recordCount=\"3\" confidence=\"0.0810810810810811\"/> </Node> <Node id=\"7\" score=\"Iris-virginica\" recordCount=\"35\"> <SimplePredicate field=\"petal_length\" operator=\"greaterOrEqual\" value=\"4.85\"/> <ScoreDistribution value=\"Iris-setosa\" recordCount=\"0\" confidence=\"0\"/> <ScoreDistribution value=\"Iris-versicolor\" recordCount=\"1\" confidence=\"0.0285714285714286\"/> <ScoreDistribution value=\"Iris-virginica\" recordCount=\"34\" confidence=\"0.971428571428571\"/> </Node> </Node> </Node> </TreeModel> </PMML>","title":"Examples"},{"location":"aql-ref-guide/#documenting-the-create-function-statement-with-comments","text":"The AQL Doc comment for a create function statement contains the following information: A general description about the function. The @param description that specifies each parameter name that is used in the function. Indicate the type of the parameter, whether it is a scalar type or a table. If the parameter is a table, describe the schema of the table, including column names and types in the order in which they appear in the schema of the table expected as input. The @return description that specifies the information that is returned by the function. If the function returns a table, specify the output schema of the table, including column names and types in the order in which they appear in the schema of the output table. /** * A function to compare an input string against a list of names * and returns a list of entries from nameList similar to myName. * @param nameList a list of names * @param myName a name to compare against the list * @return a list of entries from nameList similar to myName */ create function udfCompareNames(nameList ScalarList, myName String) return ScalarList like nameList external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!compareNames' language java deterministic;","title":"Documenting the create function statement with comments"},{"location":"aql-ref-guide/#using-user-defined-functions","text":"{: #aql-user-def-use} User-defined functions work with AQL statements and clauses. User-defined scalar functions work with AQL statements and clauses, similar to built-in functions. Specifically, scalar UDFs can be used in the select , where , having , group by , and order by clauses in the same way as built-in scalar functions, such as GetBegin and LeftContext . User-defined scalar functions that return a Boolean type can be used as predicates in where and having clauses. User-defined table functions (table UDFs) can be used within the from clause of a select statement or an extract statement.","title":"Using user-defined functions"},{"location":"aql-ref-guide/#examples_32","text":"{: #aql-reference-examples-33} Example 1: Using scalar UDFs implemented in Java with scalar values as input This example demonstrates how to use the udfCombineSpans and udfSpanGreaterThan functions that are declared in Declaring user-defined functions . create function udfCombineSpans(p1 Span, p2 Span) return Span like p1 external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!combineSpans' language java deterministic return null on null input; create function udfSpanGreaterThan(span Span, size Integer) return Boolean external_name 'udfjars/udfs.jar:com.ibm.test.udfs.MiscScalarFunc!spanGreaterThan' language java deterministic; -- identify occurrences of given names in the document create dictionary FirstNamesDict from file 'firstNames.dict'; create view FirstName as extract dictionary 'FirstNamesDict' on D.text as name from Document D; -- Use a UDF to merge the name that is longer than 7 characters with the text to its right in a -- way that is appropriate to the application. create view NameAndContext as select udfCombineSpans(F.name, RightContext(F.name, 50)) as name from FirstName F where udfSpanGreaterThan(F.name, 7); Example 2: Using Scalar UDFs implemented in Java with tables as input The following example shows how to use a scalar function that uses tables as input. This example illustrates the usage of the MyScalarFunc table UDF function, whose Java\u2122 implementation is included in Implementing user-defined functions . They are the same column types, but have different column names. -- Declare a simple scalar function that turns two tables into a big string create function MyScalarFunc( firstArg table( spanVal Span ) as locator, secondArg table( spanVal Span, strVal Text ) as locator ) return String external_name 'udfjars/udfs.jar:com.ibm.test.udfs.TableConsumingScalarFunc!eval' language java deterministic called on null input; -- Create two views to serve as inputs to the `table` function. -- Note that column names don't match, but types do. create view FirstInputView as extract regex /\\d+/ on 1 token in D.text as match from Document D; create view SecondInputView as select S.match as spanCol, 'Dummy string' as textCol from (extract regex /[A-Z][a-z]+/ on 1 token in D.text as match from Document D) S; -- Call the `scalar` function defined above from the select list. create view ScalarFuncOutput as select MyScalarFunc(FirstInputView, SecondInputView) as func from Document D; Example 3: Using Table UDFs implemented in Java This example illustrates the usage of the MyTableFunc table UDF function, whose Java\u2122 implementation is included in Implementing user-defined functions . -- Declare a simple table function that \"zips together\" two input tables create function MyTableFunc( firstArg table( spanVal Span ) as locator, secondArg table( spanVal Span, strVal Text ) as locator ) return table( outSpan1 Span, outSpan2 Span, outStr Text) external_name 'udfjars/udfs.jar:com.ibm.test.udfs.TableConsumingTableFunc!eval' language java deterministic called on null input; -- Create two views to serve as inputs to the `table` function. -- Note that column names don't match, but types do. create view FirstInputView as extract regex /\\d+/ on 1 token in D.text as match from Document D; create view SecondInputView as select S.match as spanCol, 'Dummy string' as textCol from (extract regex /[A-Z][a-z]+/ on 1 token in D.text as match from Document D) S; -- Use the `table` function create view TabFuncOutput as select T.outSpan1 as span1, T.outSpan2 as span2 from MyTableFunc(FirstInputView, SecondInputView) T; As in Example 2, The example defines two views: FirstInputView with schema (match Span) and SecondInputView with schema (spanCol Span, textCol Text) . The last view in the example, TabFuncOutput invokes the table function MyTableFunc in the from clause, with the two input views. As explained in Example 2, a view is compatible with a table locator argument of a function, as long as the number of columns and the column types in the schema of the input view and table locator match. However, it is not necessary that the column names match. Finally, the select clause discards column outStr from the output table of MyTableFunc, keeping only the first two columns outSpan1 and outSpan2 . Example 4: Using a table UDF implemented in PMML This example illustrates the usage of the IrisDecisionTree table UDF function that is declared in example 4 of Declaring user-defined functions . -- External view to hold the input records create external view IrisData( sepal_length Float, sepal_width Float, petal_length Float, petal_width Float ) external_name 'IrisData'; --Invoke the function on the input data records create view IrisDecisionTreeOutput as select * from IrisDecisionTree(IrisData); output view IrisDecisionTreeOutput;","title":"Examples"},{"location":"reference/","text":"In-Depth This section is a collection of further references and implementation notes.","title":"Reference"},{"location":"reference/#in-depth","text":"This section is a collection of further references and implementation notes.","title":"In-Depth"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/","text":"10 Minutes to SystemT (Java-API) Table of Contents {:toc} Add SystemT to Your Project Prerequisites Java Development Kit 8, 11, 17 Installation Add Dependency to SystemT Add dependency settings to rbr-annotation-service-core in your project's pom.xml file. Analyze Text Create AQL Code We show a very simple AQL code to extract mentions of programming languages and their versions, such as Python version 3.7 and Java 8 . This example is very simplistic. AQL can express much more than we can cover in a simple Hello World example. To learn AQL in depth, see Learning AQL . This is the directory structure and files we will create: demoAqlModule \\ ---main.aql AQL code is organized in modules. AQL modules are similar to Java packages, or Python libraries. An AQL module can import concepts from other AQL modules, and can export concepts to be consumed in other AQL modules. We first create an AQL module called demoAqlModule . An AQL module is a directory. The name of the directory is the name of the AQL module. The AQL code lives in .aql files inside the module directory. For more details, see AQL modules reference and AQL Reference . Create a new file, called main.aql in the module directory. Declare the module's name (same as the module directory name). module demoAqlModule; Declare a dictionary containing common programming language names. Also, define the matching semantics for the dictionary (in this case, the matching is case insensitive). Dictionaries can be declared inline, of from files or tables, and can have various matching semantics. create dictionary ProgrammingLanguages_Dict with language as 'en' and case insensitive as ('AQL', 'C++', 'Java', 'JavaScript', 'Objective C', 'Python', 'PhP'); Extract mentions of those names in the input text. The extraction happens on an object Document.text , which by default, represents the input text in AQL. create view ProgrammingLanguageName as extract dictionary ProgrammingLanguage_Dict on D.text as name from Document D; Use a regular expression to find spans that look like version numbers. create view VersionNumber as extract regex /[Vv]?\\d{1,3}(\\.\\d{1,2}){0,3}/ on D.text as version from Document D; We can now use an AQL pattern to combine our building blocks, ProgrammingLanguageName and VersionNumber , into a larger concept. We declare a pattern to extract mentions of programming languages, optionally followed within 0 to 1 tokens by a version number. Patterns are powerful AQL constructs to match regular expressions over tokens and annotations. create view ProgrammingLanguageWithVersion as extract pattern (<P.name>) ( ('version'|'v'|'v.')? (<V.version>) )? return group 0 as fullMatch and group 1 as name and group 4 as version with inline_match on Document.text from ProgrammingLanguageName P, VersionNumber V consolidate on fullMatch using 'LeftToRight'; Since the pattern has optional components, it can match overlapping portions of the text. For example, in the text I use Python 3.7 , the pattern matches both Python 3.7 and Python (with and without the version number). Therefore, we use the consolidate clause to resolve these overlapping matches. There are many consolidation policies available in AQL, see AQL Reference for details. Finally, we must declare which of the AQL concepts to output. The SystemT runtime engine saves CPU time by not executing any AQL code that does not contribute to an output. output view ProgrammingLanguageWithVersion as 'ProgrammingLanguageWithVersion'; You're done creating your first AQL code! But, this is just the beginning. If you want to harness the entire expressive power of AQL, check out Learning AQL . Java API: Low-level vs. High-level SystemT has two Java APIs: The low-level Java API allows compiling and executing AQL code. The input and output are low-level Java objects specific to SystemT. In addition, the API provides access to functionality such as querying compiled AQL modules for their input and output schema, and profiling AQL code at runtime. Use this API if you are building an application that requires access to all these functionalitites. For example, the Watson Knowledge Studio (WKS) uses the low-level Java API to generate AQL code, and compile it on the fly in the rule editor. The high-level Java API is a simple interface around the most common functionality: compiling and executing compiled AQL code. The API also hides low-level SystemT data objects as Jackson JSON objects for ease of consumption. This API is further wrapped and exposed in SystemT's Python binding. Using the Low-level Java API Use this API to compile AQL code programmatically, instantiate a set of compiled AQL modules, and execute them on input documents. In addition, this API also provides access to other methods such as querying a set of compiled modules for their input and output schema, and a sample-based AQL Runtime Profiler. See API Reference for details of each method. Compile AQL Code into Text Analytics Modules (TAM) files The following shows how to use the SystemT low-level Java API to compile AQL source code into Text Analytics Modules (.tam) files. // Set up compilation parameters CompileAQLParams params = new CompileAQLParams (); // The names of source AQL modules to compile String[] sourceModuleNames = new String[] {\"demoAqlModule\"}; String sourceModuleUri = new File(\"./src/main/aql/demoAqlModule\").toURI().toString(); String[] sourceModuleUris = new String[] { sourceModuleUri}; params.setInputModules (sourceModuleUris); // The module path, only if your source AQL uses an AQL library (other compiled AQL modules) params.setModulePath (null); // Set up the output directory where source AQL modules get compiled into .tam files String compiledTamUri = new File (\"./target\").toURI ().toString () ; params.setOutputURI (compiledTamUri); // Compile AQL modules to .tam files CompileAQL.compile (params); Instantiate Extractor from TAM Files The following shows how to use the SystemT low-level Java API to instantiate an extractor, known as OperatorGraph object, from Text Analytics Modules (.tam) files. OperatorGraph og = OperatorGraph.createOG (sourceModuleNames, compiledTamUri, null, null); Execute Extractor The following code takes English text and writes analysis result to console. See API Reference for details of each method. // Assume the document schema expected by all extractors is (label text, text Text); TupleSchema docSchema = og.getDocumentSchema (); TextSetter textSetter = docSchema.textSetter (Constants.DOCTEXT_COL); // Make a document tuple out of the input text Tuple docTup = docSchema.createTup (); String inputStr = \"I like implementing NLP models in AQL. I can execute AQL from Java 8 and Python 3.7.\"; textSetter.setVal (docTup, new Text (inputStr, LangCode.en)); // Run the extractor Map<String, TupleList> result = og.execute (docTup, null, null); Get Result The analysis result is stored in TupleList objects. The schema of the tuples in each tuple list, including the names and types of each attribute in the tuple list, along with all tuples, and attribute values can be accessed using the low-level Java API. For details see API Reference . // Print the extractor results for (String viewName : result.keySet ()) { // All tuples of the output view TupleList tups = result.get (viewName); System.out.printf (\"Output View %s:\\n\", viewName); // The schema of the output view AbstractTupleSchema schema = tups.getSchema (); // Iterate through the tuples of the output view TLIter itr = tups.iterator (); while (itr.hasNext ()) { Tuple tup = itr.next (); System.out.printf (\"\\n %s\\n\", tup); // Create and use accessor objects -- do this ONCE, ahead of time. // The accessor objects should be created ONCE, ahead of time. The accessors can be // reused subsequently to access values of all tuples of this output view, across all input documents for (int fieldIx = 0; fieldIx < schema.size (); fieldIx++) { // Obtain the field name from the view schema String fieldName = schema.getFieldNameByIx (fieldIx); // Use a Span accessor to access fields of type Span. if (schema.getFieldTypeByIx (fieldIx).getIsSpan ()) { FieldGetter<Span> accessor = schema.spanAcc (fieldName); // Using the accessor to get the field value Span span = accessor.getVal (tup); if (null == span) System.out.printf (\" %s: %s\\n\", fieldName, null); else System.out.printf (\" %s: %s, beginOffset: %d, endOffset: %d\\n\", fieldName, span.getText (), span.getBegin(), span.getEnd()); } // Use an Integer accessor to access fields of type Integer else if (schema.getFieldTypeByIx (fieldIx).getIsIntegerType ()) { FieldGetter<Integer> accessor = schema.intAcc (fieldName); // Using the accessor to get the field value int intVal = accessor.getVal (tup); System.out.printf (\" %s: %d\\n\", fieldName, intVal); } // Similarly, we have accessors for other scalar data types in AQL, such as // Text, Float, Boolean and ScalarList } } } For the following input text: I like implementing NLP models in AQL. I can execute AQL from Java 8 and Python 3.7. The above code will print to console: Output View ProgrammingLanguageWithVersion: [[34-37]: 'AQL', [34-37]: 'AQL', NULL(3 fields)] fullMatch: AQL, beginOffset: 34, endOffset: 37 name: AQL, beginOffset: 34, endOffset: 37 version: null [[53-56]: 'AQL', [53-56]: 'AQL', NULL(3 fields)] fullMatch: AQL, beginOffset: 53, endOffset: 56 name: AQL, beginOffset: 53, endOffset: 56 version: null [[62-68]: 'Java 8', [62-66]: 'Java', [67-68]: '8'(3 fields)] fullMatch: Java 8, beginOffset: 62, endOffset: 68 name: Java, beginOffset: 62, endOffset: 66 version: 8, beginOffset: 67, endOffset: 68 [[73-83]: 'Python 3.7', [73-79]: 'Python', [80-83]: '3.7'(3 fields)] fullMatch: Python 3.7, beginOffset: 73, endOffset: 83 name: Python, beginOffset: 73, endOffset: 79 version: 3.7, beginOffset: 80, endOffset: 83 Using external dictionaries and tables When you are creating the operator graph, you can also pass on the content of the external dictionaries and the external tables that are required by the loaded compiled modules by using the com.ibm.avatar.api.ExternalTypeInfo API. The following example shows how to load modules and pass the content of external dictionaries by using the OperatorGraph.createOG() API: // URI to the location where the compiled modules should be stored String COMPILED_MODULES_PATH = new File (\"textAnalytics/bin\").toURI ().toString (); // Name of the AQL modules to be loaded String[] TEXTANALYTICS_MODULES = new String[] { \"main\", \"metricsIndicator_dictionaries\", \"metricsIndicator_externalTypes\", \"metricsIndicator_features\", \"metricsIndicator_udfs\" }; // Create an instance of tokenizer TokenizerConfig TOKENIZER = new TokenizerConfig.Standard (); // Create an empty instance of the container used to pass in actual content of external dictionaries and // external tables to the loader ExternalTypeInfo externalTypeInfo = ExternalTypeInfoFactory.createInstance (); // Qualified name of the external dictionary 'abbreviations' declared in the module 'metricsIndicator_dictionaries' // through the 'create external dictionary...' statement String EXTERNAL_DICT_NAME = \"metricsIndicator_dictionaries.abbreviations\"; // URI pointing to the file abbreviations.dict containing entries for 'abbreviations' external dictionary String EXTERNAL_DICT_URI = new File (\"resource/dictionaries\", \"abbreviations.dict\").toURI ().toString (); // Populate the empty ExternalTypeInfo object with entries for 'abbreviations' dictionary externalTypeInfo.addDictionary (EXTERNAL_DICT_NAME, EXTERNAL_DICT_URI); // Similarly, populate the content of external tables into ExternalTypeInfo object // using the ExternalTypeInfo.addTable() API // Instantiate the OperatorGraph object OperatorGraph extractor = OperatorGraph.createOG (TEXTANALYTICS_MODULES, COMPILED_MODULES_PATH, externalTypeInfo, TOKENIZER); Using external views If your extractor contains external views, you must first prepare the content in a format as specified in file formats for external artifacts and the create external view statement. Then, pass the extractor as one of the input arguments to the execute API. The execute() method then annotates the document and returns the extraction results. Note: The content for external views that are defined inside an extractor is optional. The execute() method will not return an error if the content for one or more external views is not provided for annotating input documents. The following example shows how to populate external views while annotating documents: // Load Operator Graph OperatorGraph extractor = OperatorGraph.createOG (TEXTANALYTICS_MODULES, COMPILED_MODULES_PATH, externalTypeInfo, TOKENIZER); // Directory containing the collection of documents to extract information from, in one of the supported input // collection formats. File INPUT_DATA_COLLECTION = new File (\"data/ibmQuarterlyReports\"); // Open a reader over input document set. DocReader inputDataReader = new DocReader (INPUT_DATA_COLLECTION); // Qualified name of the external view as it is defined in a \"create external view\" statement of your extractor. // In this code snippet we assume the external view has been declared using the following AQL statement: // create external view MyExternalView(stringField Text, integerField Integer) // external_name MyExternalView_ExternalName; String EXTERNAL_VIEW_NAME = \"ModuleName.MyExternalView\"; // Obtain the schema of the external view from the loaded OperatorGraph TupleSchema externalViewSchema = extractor.getExternalViewSchema (EXTERNAL_VIEW_NAME); // Prepare accessor objects to get and set values for different fields from and in a tuple. // In this example, the first column of the view is of type text, // and the second column is of type integer. // Setter for the text field TextSetter textSetter = externalViewSchema.textSetter (externalViewSchema.getFieldNameByIx (0)); // Setter for the integer field FieldSetter<Integer> intSetter = externalViewSchema.intSetter (externalViewSchema.getFieldNameByIx (1)); // Similarly you can create setter for fields with other data types // like float,span ..etc // Prepare a Tuplelist with two tuples { {\"text1\",1}, {\"test2\",2} } TupleList externalViewTups = new TupleList (externalViewSchema); Tuple externalViewTup; // Prepare two external view tuples externalViewTup = externalViewSchema.createTup (); textSetter.setVal (externalViewTup, \"text1\"); intSetter.setVal (externalViewTup, 1); externalViewTups.add (externalViewTup); externalViewTup = externalViewSchema.createTup (); textSetter.setVal (externalViewTup, \"text2\"); intSetter.setVal (externalViewTup, 2); externalViewTups.add (externalViewTup); // Process the documents one at a time. System.err.println (\"Executing SystemT ...\"); int ndoc = 0; while (inputDataReader.hasNext ()) { Tuple doc = inputDataReader.next (); // Prepare the content of the external view in a map where the // key represents an external view name, and the value is the list of tuples of that external view Map<String, TupleList> extViewTupMap = new HashMap<String, TupleList> (); extViewTupMap.put (EXTERNAL_VIEW_NAME, externalViewTups); // Annotate the current document, generating every single output // type that the extractor produces. // The second argument is an optional list of what output types to generate; null means \"return all types\" // The third argument is the content of the external views Map<String, TupleList> results = extractor.execute (doc, null, extViewTupMap); } Query the extractor schema and other metadata Class OperatorGraph provides APIs to query a loaded operator graph about its input document schema, and to list all the output types and their schema. The following example illustrates the use of these APIs: // Load Operator Graph OperatorGraph extractor = OperatorGraph.createOG (TEXTANALYTICS_MODULES, COMPILED_MODULES_PATH, externalTypeInfo, TOKENIZER); // Schema of the document expected by the loaded Operator Graph TupleSchema inputDocumentSchema = extractor.getDocumentSchema (); System.out.println (\"\\n Displaying input document schema of the constructed extractor : \"); System.out.println (inputDocumentSchema.toString ()); // Schema for every output view type in this extractor Map<String, TupleSchema> outputTypesSchema = extractor.getOutputTypeNamesAndSchema (); for (String outputType : outputTypesSchema.keySet ()) { System.out.println (\"Output schema for output view type : \" + outputType); System.out.println (outputTypesSchema.get (outputType)); } ModuleMetadata API Class ModuleMetadata provides APIs to query the Text Analytics module (TAM) about the: - schema of the view `Document` - list of elements \\(views/tables/functions/dictionaries\\) exported by the module - list of views output by the module - schema of the exported or output views - external dictionaries, tables and views declared in the module - list of other modules that this module depends upon The following example illustrates how to load the module metadata from a compiled Text Analytics module (TAM), and later query the loaded module about its metadata: // URI to the location where the compiled modules should be stored String COMPILED_MODULES_PATH = new File (\"textAnalytics/bin\").toURI ().toString (); // Name of the compiled AQL modules String[] TEXTANALYTICS_MODULES = new String[] { \"main\", \"metricsIndicator_dictionaries\", \"metricsIndicator_externalTypes\", \"metricsIndicator_features\", \"metricsIndicator_udfs\" }; // Read metadata for all the modules compiled in Step#1 ModuleMetadata[] modulesMetadata = ModuleMetadataFactory.readMetaData (TEXTANALYTICS_MODULES, COMPILED_MODULES_PATH); // Query metadata of each module to obtain the following: // 1) Exported views and their schemas // 2) External dictionaries and external tables for (int metadataIndex = 0; metadataIndex < modulesMetadata.length; ++metadataIndex) { ModuleMetadata metadata = modulesMetadata[metadataIndex]; System.out.printf (\"\\n Displaying metadata for module named '%s': \", metadata.getModuleName ()); // Obtain list of views exported by this module and their schema String[] exportedViews = metadata.getExportedViews (); for (int exportedviewIndex = 0; exportedviewIndex < exportedViews.length; exportedviewIndex++) { // Fetch ViewMetadata for exported view ViewMetadata exportedViewMetadata = metadata.getViewMetadata (exportedViews[exportedviewIndex]); System.out.printf (\"\\n Exported view name is '%s' and its schema is '%s'.\", exportedViews[exportedviewIndex], exportedViewMetadata.getViewSchema ()); } // Obtain list of external dictionaries and inquire if it is required to pass in entries for the dictionary while // loading String[] externalDictionaries = metadata.getExternalDictionaries (); for (int externalDictIndex = 0; externalDictIndex < externalDictionaries.length; externalDictIndex++) { // Fetch metadata for the external dictionary DictionaryMetadata dictionaryMetadata = metadata.getDictionaryMetadata (externalDictionaries[externalDictIndex]); System.out.printf (\"\\n External dictionary name is '%s' and it is an '%s' dictionary.\", externalDictionaries[externalDictIndex], dictionaryMetadata.isAllowEmpty () ? \"Optional\" : \"Required\"); } // Similarly, you can obtain list of external tables and inquire if it is required to pass in entries for the table // while loading // Obtain list of modules this module depends on List<String> dependentModules = metadata.getDependentModules (); System.out.printf (\"\\n Module named '%s' depends on following modules %s.\", metadata.getModuleName (), dependentModules); } MultiModuleMetadata API While individual modules contain metadata to describe their own schema and artifacts, applications might need metadata about the attributes of combined modules, numerous times, without having to create and load the extractor in memory. The MultiModuleMetadata API provides metadata for extractors formed from a group of modules. The following example illustrates the usage of the MultiModuleMetadata API: /** * Method to illustrate the ability to understand what an extractor represents before having to create it * * Useful when an application consuming text-analytics needs to know extractor specifics ahead of creating extractor * This can also be used by say, a tool that displays extractor specifics for users before they choose within the tool * * @throws TextAnalyticsException * @throws Exception */ public void illustrateMultiModuleMetadata() throws TextAnalyticsException, Exception { MultiModuleMetadata modulesMetadata = ModuleMetadataFactory.readAllMetaData(TEXTANALYTICS_MODULES, COMPILED_MODULES_PATH); System.out.println(\"Input document schema across these modules : \"+ modulesMetadata.getDocSchema()); System.out.println(\"Tokenizer used in creating this extractor : \"+modulesMetadata.getTokenizerType()); String[] outputViews = modulesMetadata.getOutputViews(); // Schema of each output view across these modules for (String outputView : outputViews) { ViewMetadata viewMetadata = modulesMetadata.getViewMetadata(outputView); System.out.println(\"Schema for output view : \"+ outputView + \" is \"+ viewMetadata.getViewSchema()); } // Schema of each exported view across these modules String[] exportedViews = modulesMetadata.getExportedViews(); for(String exportedView : exportedViews) { ViewMetadata viewMetadata = modulesMetadata.getViewMetadata(exportedView); System.out.println(\"Schema for exported view : \"+ exportedView + \" is \" + viewMetadata.getViewSchema()); } // Particulars of any function being exported from within any of the modules String[] exportedFunctions = modulesMetadata.getExportedFunctions(); for (String exportedFunction : exportedFunctions) { // Fetch function metadata FunctionMetadata functionMetadata = modulesMetadata.getFunctionMetadata(exportedFunction); System.out.println(\"Particulars of exported function : \"+exportedFunction); System.out.println(\"Function external name : \"+functionMetadata.getExternalName()); // Function input parameters Param[] functionParams = functionMetadata.getParameters(); int ix = 0; for (Param functionParam : functionParams) System.out.println(\"Function parameter #\"+(++ix)+\"--> Name : \"+functionParam.getName()+\" ; Type : \"+functionParam.getType()); // Function return type System.out.println(\"Function return type : \"+functionMetadata.getReturnType()); } /** * Like above, one could use similar public methods exposed by the MultiModuleMetadata API to obtain information * about dictionaries, tables, comments relevant to the modules being considered. */ } The DocReader API The SystemT low-level Java API also includes the DocReader API , a convenient Java API for reading input document collections that are stored on the local file system in one of the supported input collection formats. The DocReader API supports the input document collections that use the data collection formats. Annotating a document collection (non-JSON, non-CSV) The following example demonstrates how to annotate document collections in an input format that allows only the default document schema (text Text, [label Text])). Create a DocReader object, and pass in the location of the input document collection as its parameter. Then, use the DocReader.next() function to obtain the next document in a collection. // Directory containing the collection of documents from which to extract information, in one of the supported input collection formats. File INPUT_DOCS_FILE = new File (\"/path/to/input/collection/on/disk\"); // Create the operator graph String modulePath = \"/path/to/modules/on/disk\"; TokenizerConfig tokenizer = new TokenizerConfig.Standard (); OperatorGraph og = OperatorGraph.createOG (new String[] { \"module1\", \"module2\" }, modulePath, null, tokenizer); // Open a reader over input document set DocReader docs = new DocReader (INPUT_DOCS_FILE); while (docs.hasNext ()) { Tuple docTuple = docs.next (); // Execute the operator graph on the current document, generating every single output type that the extractor produces. Map<String, TupleList> results = og.execute (docTuple, null, null); // Process the results as required } Annotating a CSV document collection Annotating a CSV document collection is similar to annotating a regular document collection, except that a CSV file can support a non-default document schema. When you create the DocReader object for a CSV document collection, use the constructor that specifies a custom document schema, as seen in the following example. Then, use DocReader.next() to traverse the collection. // Directory containing the collection of documents from which to extract information in CSV format File INPUT_DOCS_FILE = new File (\"/path/to/csv/input/collection/on/disk/\"); // Create the operator graph String modulePath = \"/path/to/modules/on/disk\"; TokenizerConfig tokenizer = new TokenizerConfig.Standard (); OperatorGraph og = OperatorGraph.createOG (new String[] { \"module1\", \"module2\" }, modulePath, null, tokenizer); // Use the operator graph to determine the document schema. TupleSchema docSchema = og.getDocumentSchema (); // Open a reader over input document set DocReader docs = new DocReader (INPUT_DOCS_FILE, docSchema, null); while (docs.hasNext ()) { Tuple docTuple = docs.next (); // Execute the operator graph on the current document, generating every single output type that the extractor produces. Map<String, TupleList> results = og.execute (docTuple, null, null); // Process the results as required } Annotating a JSON document collection For a JSON document collection, use the static method DocReader.makeDocAndExternalPairsItr() to retrieve an Iterator that returns the document content and any corresponding external view tuples. To use this method, you must build an external view map object that represents all the external views, with corresponding external names and document schemas. // Directory containing the collection of documents from which to extract information in a JSON format. File INPUT_DOCS_FILE = new File (\"/path/to/json/input/collection/on/disk\"); // Create the operator graph String modulePath = \"/path/to/modules/on/disk\"; TokenizerConfig tokenizer = new TokenizerConfig.Standard (); OperatorGraph og = OperatorGraph.createOG (new String[] { \"module1\", \"module2\" }, modulePath, null, tokenizer); Map<Pair<String, String>, TupleSchema> extViewsMap = new HashMap<Pair<String, String>, TupleSchema> (); // Prepare external view map for (String extViewName : og.getExternalViewNames ()) { // Get external name of the external view String extViewExternalName = og.getExternalViewExternalName (extViewName); // Get schema of external view Pair<String, String> extViewNamePair = new Pair<String, String> (extViewName, extViewExternalName); TupleSchema schema = og.getExternalViewSchema (extViewName); extViewsMap.put (extViewNamePair, schema); // iterate over doc tuples with associated external views Iterator<Pair<Tuple, Map<String, TupleList>>> itr = DocReader.makeDocandExternalPairsItr ( INPUT_DOCS_FILE.toURI ().toString (), og.getDocumentSchema (), extViewsMap); while (itr.hasNext ()) { Pair<Tuple, Map<String, TupleList>> docExtViewTup = itr.next (); Tuple docTuple = docExtViewTup.first; Map<String, TupleList> extViewData = docExtViewTup.second; Map<String, TupleList> results = og.execute (docTuple, null, extViewData); // Process the results as required } Text Analytics URI formats You can specify the input and output location of your files with Uniform Resource Identifiers (URIs). The following operations are examples of API calls that use URIs: Reading an AQL file for compilation Writing a TAM file Loading modules from a TAM file Loading module metadata Loading external artifact data Compared to other applications, SystemT has a flexible definition of what constitutes a valid URI. You can use a URI that does not contain a file system scheme (schemeless URI), as well as file system schemes of type HDFS. The full HDFS URI (hdfs://namenode.ibm.com:9080/directory/file.tam) has an HDFS scheme and specified authority. The HDFS URI (hdfs:///directory/file.tam) has an HDFS scheme and no specified authority. The local URI (file:///directory/file.tam) has a local file system scheme and no specified authority. The schemeless absolute URI (/directory/file.tam) has a POSIX-compliant absolute file path. The schemeless relative URI (directory/file.tam) has a POSIX-compliant relative file path. For more information about the specifics of the following APIs, see the Javadoc classes and APIs. Java API Supported URI formats CompileAQLParams(...) [constructor] - Local - Schemeless absolute - Schemeless relative CompileAQLParams.setInputModules(...) - Local - Schemeless absolute - Schemeless relative CompileAQLParams.setModulePath(...) - Local - Schemeless absolute - Schemeless relative CompileAQLParams.setOutputURI(...) - Local - Schemeless absolute - Schemeless relative OperatorGraph.createOG(...) - Full HDFS - HDFS - Local - Schemeless absolute - Schemeless relative OperatorGraph.validateOG(...) - Full HDFS - HDFS - Local - Schemeless absolute - Schemeless relative ExternalTypeInfo.addDictionary(...) - Full HDFS - HDFS - Local - Schemeless absolute - Schemeless relative ExternalTypeInfo.addTable(...) - Full HDFS - HDFS - Local - Schemeless absolute - Schemeless relative ModuleMetadataFactory.readMetaData(...) - Full HDFS - HDFS - Local - Schemeless absolute - Schemeless relative ModuleMetadataFactory.readAllMetaData(...) - Local - Schemeless absolute - Schemeless relative Schemeless URIs (formats 5 and 6) are resolved to a schemed URI in a process that is called scheme auto-discovery so that you can reuse a URI in applications without committing to a specific underlying file system scheme: If you are using APIs that do not support a distributed file system (for example, compile APIs), the auto-resolved scheme is always local file system (file://). If the APIs support a distributed file system (for example OperatorGraph.createOG()), the auto-resolved scheme is one of HDFS if installed ( hdfs://), otherwise it is the local file system (file://). To determine whether a DFS is installed, SystemT attempts to read the fs.default.name property from the file core-site.xml in the directory that is specified by the environment variable HADOOP_CONF_DIR. It then sets the scheme to be identical to the scheme of the URI contained in that property. If the read fails for any reason, the scheme is set to local file system. When the scheme is auto-discovered in a relative file path URI (format 6), the URI is resolved to an absolute URI: If the scheme is a distributed file system ( hdfs://), you can assume that the URI is relative to the root of the DFS that is specified. If the scheme is local file system (file://), then you can assume that the URI is relative to the current working directory (usually the directory from which the application was started). Example 1 In this first example, you can create an operator graph with the module path of application/modules (format 6), using the OperatorGraph.createOG() API. The system checks $HADOOP_CONF_DIR/core-site.xml for the property file fs.default.name, which is set to hdfs://bigserver.widgets.org. The module path is then set to hdfs://bigserver.widgets.org/application/modules. Example 2 In the same environment, the HADOOP_CONF_DIR environment variable is not set and the directory where SystemT is started is your home directory /home/username. With no distributed file system found, the module path is set to file:///home/username/application/modules. Using the High-level Java API Prepare Compiled Text Analytics Modules (TAM) files SystemT's high-level API is designed to compile AQL code and to execute compiled AQL code (TAM modules). For reference, AQL compilation can be completed by using either the high-level API or the low-level API , tams \\ ---demoAqlModule.tam Create Configuration File SystemT high-level API is designed to integrate application easily, so the input object and output annotation result are defined as Jackson Json node. To construct annotation result, we need to setup annotation core module by using a configuration json file named manifest.json Here is the manifest.json used in this tutorial: { \"annotator\": { \"version\": \"1.0\", \"key\": \"Demo\" }, \"annotatorRuntime\": \"SystemT\", \"version\": \"1.0\", \"acceptedContentTypes\": [ \"text/html\", \"text/plain\" ], \"serializeAnnotatorInfo\": false, \"location\": \"./model\", \"serializeSpan\": \"locationAndText\", \"tokenizer\": \"standard\", \"modulePath\": [ \"tams\" ], \"moduleNames\": [ \"demoAqlModule\" ], \"inputTypes\": null, \"outputTypes\": [ \"ProgrammingLanguageName\", \"ProgrammingLanguageWithVersion\" ], \"externalDictionaries\": null, \"externalTables\": {} } This looks hard to understand at a glance, but not many fields are mandatory to modify for trial. For a complete reference, see Specification of manifest.json . Here's some key fields to use your own custom annotator. Model file path: Set location to the model folder. sh \"location\": \"./model\", Module path: Path to .tam folder path relative to the value in location . sh \"modulePath\": [ \"tams\" ] Module name: Names of AQL modules that you want to execute (coincide with the names of TAM files without .tam extension). sh \"moduleNames\": [ \"demoAqlModule\" ] Output types: Output views that you want to execute (other output views may exist in your TAM files, but they will not be executed unless explicitly included here). sh \"outputTypes\": [ \"ProgrammingLanguageName\", \"ProgrammingLanguageWithVersion\" ] For further details, please refer to Specification of manifest.json Now we have . \u251c\u2500\u2500 model \u2502 \u251c\u2500\u2500 manifest.json \u2502 \u251c\u2500\u2500 tams \u2502 \u2502 \u2514\u2500\u2500 demoAqlModule.tam \u2502 \u2514\u2500\u2500 aql \u2502 \u2514\u2500\u2500 demoAqlModule \u2502 \u2514\u2500\u2500 main.aql Configuration for AQL Compilation Add the following field to specify the location of the AQL files, if you'd like to compile and execute a model including AQL files. Source modules: Path to .aql folder path relative to the value in location . sh \"sourceModules\": [ \"aql/demoAqlModule\" ] Instantiate and Execute Extractor The following code takes English text and writes analysis result to console. See API Reference for details of each method. // Annotation service core object (an embeddable library, not a service) AnnotationService as = new AnnotationService(); // Use jackson objectmapper to read manifest.json and dump result ObjectMapper mapper = new ObjectMapper().enable(SerializationFeature.INDENT_OUTPUT); // STEP 1. Load configuration (manifest.json) Path manifestPath = Paths.get(\"model/manifest.json\"); AnnotatorBundleConfig cfg = mapper.readValue (Files.readAllBytes (manifestPath), AnnotatorBundleConfig.class); // STEP 2. Prepare execution parameter // LanguageCode for text String language = \"en\"; ExecuteParams execParams = new ExecuteParams (-1L, language, null, AnnotatorBundleConfig.ContentType.PLAIN.getValue (), null); // Text to process String text = \"I like implementing NLP models in AQL. I can execute AQL from Java 8 and Python 3.7.\"; ObjectNode objNode = mapper.createObjectNode (); objNode.put (\"label\", \"docLabel\"); objNode.put (\"text\", text); // STEP 3. Invoke annotation service core and get result JsonNode outputJson = as.invoke (cfg, objNode, execParams); Get Result The analysis result is stored in Jackson JsonNode objects. For the following input text: I like implementing NLP models in AQL. I can execute AQL from Java 8 and Python 3.7. The above code will print to console: { \"annotations\" : { \"ProgrammingLanguageName\" : [ { \"name\" : { \"location\" : { \"begin\" : 34, \"end\" : 37 }, \"text\" : \"AQL\" } }, { \"name\" : { \"location\" : { \"begin\" : 53, \"end\" : 56 }, \"text\" : \"AQL\" } }, { \"name\" : { \"location\" : { \"begin\" : 62, \"end\" : 66 }, \"text\" : \"Java\" } }, { \"name\" : { \"location\" : { \"begin\" : 73, \"end\" : 79 }, \"text\" : \"Python\" } } ], \"ProgrammingLanguageWithVersion\" : [ { \"fullMatch\" : { \"location\" : { \"begin\" : 34, \"end\" : 37 }, \"text\" : \"AQL\" }, \"name\" : { \"location\" : { \"begin\" : 34, \"end\" : 37 }, \"text\" : \"AQL\" }, \"version\" : null }, { \"fullMatch\" : { \"location\" : { \"begin\" : 53, \"end\" : 56 }, \"text\" : \"AQL\" }, \"name\" : { \"location\" : { \"begin\" : 53, \"end\" : 56 }, \"text\" : \"AQL\" }, \"version\" : null }, { \"fullMatch\" : { \"location\" : { \"begin\" : 62, \"end\" : 68 }, \"text\" : \"Java 8\" }, \"name\" : { \"location\" : { \"begin\" : 62, \"end\" : 66 }, \"text\" : \"Java\" }, \"version\" : { \"location\" : { \"begin\" : 67, \"end\" : 68 }, \"text\" : \"8\" } }, { \"fullMatch\" : { \"location\" : { \"begin\" : 73, \"end\" : 83 }, \"text\" : \"Python 3.7\" }, \"name\" : { \"location\" : { \"begin\" : 73, \"end\" : 79 }, \"text\" : \"Python\" }, \"version\" : { \"location\" : { \"begin\" : 80, \"end\" : 83 }, \"text\" : \"3.7\" } } ] }, \"instrumentationInfo\" : { \"annotator\" : { \"version\" : \"1.0\", \"key\" : \"Demo\" }, \"runningTimeMS\" : 4, \"documentSizeChars\" : 92, \"numAnnotationsTotal\" : 8, \"numAnnotationsPerType\" : [ { \"annotationType\" : \"ProgrammingLanguageName\", \"numAnnotations\" : 4 }, { \"annotationType\" : \"ProgrammingLanguageWithVersion\", \"numAnnotations\" : 4 } ], \"interrupted\" : false, \"success\" : true } }","title":"10 Minutes to SystemT (Java-API)"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#10-minutes-to-systemt-java-api","text":"Table of Contents {:toc}","title":"10 Minutes to SystemT (Java-API)"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#add-systemt-to-your-project","text":"","title":"Add SystemT to Your Project"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#prerequisites","text":"Java Development Kit 8, 11, 17","title":"Prerequisites"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#installation","text":"","title":"Installation"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#add-dependency-to-systemt","text":"Add dependency settings to rbr-annotation-service-core in your project's pom.xml file.","title":"Add Dependency to SystemT"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#analyze-text","text":"","title":"Analyze Text"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#create-aql-code","text":"We show a very simple AQL code to extract mentions of programming languages and their versions, such as Python version 3.7 and Java 8 . This example is very simplistic. AQL can express much more than we can cover in a simple Hello World example. To learn AQL in depth, see Learning AQL . This is the directory structure and files we will create: demoAqlModule \\ ---main.aql AQL code is organized in modules. AQL modules are similar to Java packages, or Python libraries. An AQL module can import concepts from other AQL modules, and can export concepts to be consumed in other AQL modules. We first create an AQL module called demoAqlModule . An AQL module is a directory. The name of the directory is the name of the AQL module. The AQL code lives in .aql files inside the module directory. For more details, see AQL modules reference and AQL Reference . Create a new file, called main.aql in the module directory. Declare the module's name (same as the module directory name). module demoAqlModule; Declare a dictionary containing common programming language names. Also, define the matching semantics for the dictionary (in this case, the matching is case insensitive). Dictionaries can be declared inline, of from files or tables, and can have various matching semantics. create dictionary ProgrammingLanguages_Dict with language as 'en' and case insensitive as ('AQL', 'C++', 'Java', 'JavaScript', 'Objective C', 'Python', 'PhP'); Extract mentions of those names in the input text. The extraction happens on an object Document.text , which by default, represents the input text in AQL. create view ProgrammingLanguageName as extract dictionary ProgrammingLanguage_Dict on D.text as name from Document D; Use a regular expression to find spans that look like version numbers. create view VersionNumber as extract regex /[Vv]?\\d{1,3}(\\.\\d{1,2}){0,3}/ on D.text as version from Document D; We can now use an AQL pattern to combine our building blocks, ProgrammingLanguageName and VersionNumber , into a larger concept. We declare a pattern to extract mentions of programming languages, optionally followed within 0 to 1 tokens by a version number. Patterns are powerful AQL constructs to match regular expressions over tokens and annotations. create view ProgrammingLanguageWithVersion as extract pattern (<P.name>) ( ('version'|'v'|'v.')? (<V.version>) )? return group 0 as fullMatch and group 1 as name and group 4 as version with inline_match on Document.text from ProgrammingLanguageName P, VersionNumber V consolidate on fullMatch using 'LeftToRight'; Since the pattern has optional components, it can match overlapping portions of the text. For example, in the text I use Python 3.7 , the pattern matches both Python 3.7 and Python (with and without the version number). Therefore, we use the consolidate clause to resolve these overlapping matches. There are many consolidation policies available in AQL, see AQL Reference for details. Finally, we must declare which of the AQL concepts to output. The SystemT runtime engine saves CPU time by not executing any AQL code that does not contribute to an output. output view ProgrammingLanguageWithVersion as 'ProgrammingLanguageWithVersion'; You're done creating your first AQL code! But, this is just the beginning. If you want to harness the entire expressive power of AQL, check out Learning AQL .","title":"Create AQL Code"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#java-api-low-level-vs-high-level","text":"SystemT has two Java APIs: The low-level Java API allows compiling and executing AQL code. The input and output are low-level Java objects specific to SystemT. In addition, the API provides access to functionality such as querying compiled AQL modules for their input and output schema, and profiling AQL code at runtime. Use this API if you are building an application that requires access to all these functionalitites. For example, the Watson Knowledge Studio (WKS) uses the low-level Java API to generate AQL code, and compile it on the fly in the rule editor. The high-level Java API is a simple interface around the most common functionality: compiling and executing compiled AQL code. The API also hides low-level SystemT data objects as Jackson JSON objects for ease of consumption. This API is further wrapped and exposed in SystemT's Python binding.","title":"Java API: Low-level vs. High-level"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#using-the-low-level-java-api","text":"Use this API to compile AQL code programmatically, instantiate a set of compiled AQL modules, and execute them on input documents. In addition, this API also provides access to other methods such as querying a set of compiled modules for their input and output schema, and a sample-based AQL Runtime Profiler. See API Reference for details of each method.","title":"Using the Low-level Java API"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#compile-aql-code-into-text-analytics-modules-tam-files","text":"The following shows how to use the SystemT low-level Java API to compile AQL source code into Text Analytics Modules (.tam) files. // Set up compilation parameters CompileAQLParams params = new CompileAQLParams (); // The names of source AQL modules to compile String[] sourceModuleNames = new String[] {\"demoAqlModule\"}; String sourceModuleUri = new File(\"./src/main/aql/demoAqlModule\").toURI().toString(); String[] sourceModuleUris = new String[] { sourceModuleUri}; params.setInputModules (sourceModuleUris); // The module path, only if your source AQL uses an AQL library (other compiled AQL modules) params.setModulePath (null); // Set up the output directory where source AQL modules get compiled into .tam files String compiledTamUri = new File (\"./target\").toURI ().toString () ; params.setOutputURI (compiledTamUri); // Compile AQL modules to .tam files CompileAQL.compile (params);","title":"Compile AQL Code into Text Analytics Modules (TAM) files"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#instantiate-extractor-from-tam-files","text":"The following shows how to use the SystemT low-level Java API to instantiate an extractor, known as OperatorGraph object, from Text Analytics Modules (.tam) files. OperatorGraph og = OperatorGraph.createOG (sourceModuleNames, compiledTamUri, null, null);","title":"Instantiate Extractor from TAM Files"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#execute-extractor","text":"The following code takes English text and writes analysis result to console. See API Reference for details of each method. // Assume the document schema expected by all extractors is (label text, text Text); TupleSchema docSchema = og.getDocumentSchema (); TextSetter textSetter = docSchema.textSetter (Constants.DOCTEXT_COL); // Make a document tuple out of the input text Tuple docTup = docSchema.createTup (); String inputStr = \"I like implementing NLP models in AQL. I can execute AQL from Java 8 and Python 3.7.\"; textSetter.setVal (docTup, new Text (inputStr, LangCode.en)); // Run the extractor Map<String, TupleList> result = og.execute (docTup, null, null);","title":"Execute Extractor"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#get-result","text":"The analysis result is stored in TupleList objects. The schema of the tuples in each tuple list, including the names and types of each attribute in the tuple list, along with all tuples, and attribute values can be accessed using the low-level Java API. For details see API Reference . // Print the extractor results for (String viewName : result.keySet ()) { // All tuples of the output view TupleList tups = result.get (viewName); System.out.printf (\"Output View %s:\\n\", viewName); // The schema of the output view AbstractTupleSchema schema = tups.getSchema (); // Iterate through the tuples of the output view TLIter itr = tups.iterator (); while (itr.hasNext ()) { Tuple tup = itr.next (); System.out.printf (\"\\n %s\\n\", tup); // Create and use accessor objects -- do this ONCE, ahead of time. // The accessor objects should be created ONCE, ahead of time. The accessors can be // reused subsequently to access values of all tuples of this output view, across all input documents for (int fieldIx = 0; fieldIx < schema.size (); fieldIx++) { // Obtain the field name from the view schema String fieldName = schema.getFieldNameByIx (fieldIx); // Use a Span accessor to access fields of type Span. if (schema.getFieldTypeByIx (fieldIx).getIsSpan ()) { FieldGetter<Span> accessor = schema.spanAcc (fieldName); // Using the accessor to get the field value Span span = accessor.getVal (tup); if (null == span) System.out.printf (\" %s: %s\\n\", fieldName, null); else System.out.printf (\" %s: %s, beginOffset: %d, endOffset: %d\\n\", fieldName, span.getText (), span.getBegin(), span.getEnd()); } // Use an Integer accessor to access fields of type Integer else if (schema.getFieldTypeByIx (fieldIx).getIsIntegerType ()) { FieldGetter<Integer> accessor = schema.intAcc (fieldName); // Using the accessor to get the field value int intVal = accessor.getVal (tup); System.out.printf (\" %s: %d\\n\", fieldName, intVal); } // Similarly, we have accessors for other scalar data types in AQL, such as // Text, Float, Boolean and ScalarList } } } For the following input text: I like implementing NLP models in AQL. I can execute AQL from Java 8 and Python 3.7. The above code will print to console: Output View ProgrammingLanguageWithVersion: [[34-37]: 'AQL', [34-37]: 'AQL', NULL(3 fields)] fullMatch: AQL, beginOffset: 34, endOffset: 37 name: AQL, beginOffset: 34, endOffset: 37 version: null [[53-56]: 'AQL', [53-56]: 'AQL', NULL(3 fields)] fullMatch: AQL, beginOffset: 53, endOffset: 56 name: AQL, beginOffset: 53, endOffset: 56 version: null [[62-68]: 'Java 8', [62-66]: 'Java', [67-68]: '8'(3 fields)] fullMatch: Java 8, beginOffset: 62, endOffset: 68 name: Java, beginOffset: 62, endOffset: 66 version: 8, beginOffset: 67, endOffset: 68 [[73-83]: 'Python 3.7', [73-79]: 'Python', [80-83]: '3.7'(3 fields)] fullMatch: Python 3.7, beginOffset: 73, endOffset: 83 name: Python, beginOffset: 73, endOffset: 79 version: 3.7, beginOffset: 80, endOffset: 83","title":"Get Result"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#using-external-dictionaries-and-tables","text":"When you are creating the operator graph, you can also pass on the content of the external dictionaries and the external tables that are required by the loaded compiled modules by using the com.ibm.avatar.api.ExternalTypeInfo API. The following example shows how to load modules and pass the content of external dictionaries by using the OperatorGraph.createOG() API: // URI to the location where the compiled modules should be stored String COMPILED_MODULES_PATH = new File (\"textAnalytics/bin\").toURI ().toString (); // Name of the AQL modules to be loaded String[] TEXTANALYTICS_MODULES = new String[] { \"main\", \"metricsIndicator_dictionaries\", \"metricsIndicator_externalTypes\", \"metricsIndicator_features\", \"metricsIndicator_udfs\" }; // Create an instance of tokenizer TokenizerConfig TOKENIZER = new TokenizerConfig.Standard (); // Create an empty instance of the container used to pass in actual content of external dictionaries and // external tables to the loader ExternalTypeInfo externalTypeInfo = ExternalTypeInfoFactory.createInstance (); // Qualified name of the external dictionary 'abbreviations' declared in the module 'metricsIndicator_dictionaries' // through the 'create external dictionary...' statement String EXTERNAL_DICT_NAME = \"metricsIndicator_dictionaries.abbreviations\"; // URI pointing to the file abbreviations.dict containing entries for 'abbreviations' external dictionary String EXTERNAL_DICT_URI = new File (\"resource/dictionaries\", \"abbreviations.dict\").toURI ().toString (); // Populate the empty ExternalTypeInfo object with entries for 'abbreviations' dictionary externalTypeInfo.addDictionary (EXTERNAL_DICT_NAME, EXTERNAL_DICT_URI); // Similarly, populate the content of external tables into ExternalTypeInfo object // using the ExternalTypeInfo.addTable() API // Instantiate the OperatorGraph object OperatorGraph extractor = OperatorGraph.createOG (TEXTANALYTICS_MODULES, COMPILED_MODULES_PATH, externalTypeInfo, TOKENIZER);","title":"Using external dictionaries and tables"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#using-external-views","text":"If your extractor contains external views, you must first prepare the content in a format as specified in file formats for external artifacts and the create external view statement. Then, pass the extractor as one of the input arguments to the execute API. The execute() method then annotates the document and returns the extraction results. Note: The content for external views that are defined inside an extractor is optional. The execute() method will not return an error if the content for one or more external views is not provided for annotating input documents. The following example shows how to populate external views while annotating documents: // Load Operator Graph OperatorGraph extractor = OperatorGraph.createOG (TEXTANALYTICS_MODULES, COMPILED_MODULES_PATH, externalTypeInfo, TOKENIZER); // Directory containing the collection of documents to extract information from, in one of the supported input // collection formats. File INPUT_DATA_COLLECTION = new File (\"data/ibmQuarterlyReports\"); // Open a reader over input document set. DocReader inputDataReader = new DocReader (INPUT_DATA_COLLECTION); // Qualified name of the external view as it is defined in a \"create external view\" statement of your extractor. // In this code snippet we assume the external view has been declared using the following AQL statement: // create external view MyExternalView(stringField Text, integerField Integer) // external_name MyExternalView_ExternalName; String EXTERNAL_VIEW_NAME = \"ModuleName.MyExternalView\"; // Obtain the schema of the external view from the loaded OperatorGraph TupleSchema externalViewSchema = extractor.getExternalViewSchema (EXTERNAL_VIEW_NAME); // Prepare accessor objects to get and set values for different fields from and in a tuple. // In this example, the first column of the view is of type text, // and the second column is of type integer. // Setter for the text field TextSetter textSetter = externalViewSchema.textSetter (externalViewSchema.getFieldNameByIx (0)); // Setter for the integer field FieldSetter<Integer> intSetter = externalViewSchema.intSetter (externalViewSchema.getFieldNameByIx (1)); // Similarly you can create setter for fields with other data types // like float,span ..etc // Prepare a Tuplelist with two tuples { {\"text1\",1}, {\"test2\",2} } TupleList externalViewTups = new TupleList (externalViewSchema); Tuple externalViewTup; // Prepare two external view tuples externalViewTup = externalViewSchema.createTup (); textSetter.setVal (externalViewTup, \"text1\"); intSetter.setVal (externalViewTup, 1); externalViewTups.add (externalViewTup); externalViewTup = externalViewSchema.createTup (); textSetter.setVal (externalViewTup, \"text2\"); intSetter.setVal (externalViewTup, 2); externalViewTups.add (externalViewTup); // Process the documents one at a time. System.err.println (\"Executing SystemT ...\"); int ndoc = 0; while (inputDataReader.hasNext ()) { Tuple doc = inputDataReader.next (); // Prepare the content of the external view in a map where the // key represents an external view name, and the value is the list of tuples of that external view Map<String, TupleList> extViewTupMap = new HashMap<String, TupleList> (); extViewTupMap.put (EXTERNAL_VIEW_NAME, externalViewTups); // Annotate the current document, generating every single output // type that the extractor produces. // The second argument is an optional list of what output types to generate; null means \"return all types\" // The third argument is the content of the external views Map<String, TupleList> results = extractor.execute (doc, null, extViewTupMap); }","title":"Using external views"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#query-the-extractor-schema-and-other-metadata","text":"Class OperatorGraph provides APIs to query a loaded operator graph about its input document schema, and to list all the output types and their schema. The following example illustrates the use of these APIs: // Load Operator Graph OperatorGraph extractor = OperatorGraph.createOG (TEXTANALYTICS_MODULES, COMPILED_MODULES_PATH, externalTypeInfo, TOKENIZER); // Schema of the document expected by the loaded Operator Graph TupleSchema inputDocumentSchema = extractor.getDocumentSchema (); System.out.println (\"\\n Displaying input document schema of the constructed extractor : \"); System.out.println (inputDocumentSchema.toString ()); // Schema for every output view type in this extractor Map<String, TupleSchema> outputTypesSchema = extractor.getOutputTypeNamesAndSchema (); for (String outputType : outputTypesSchema.keySet ()) { System.out.println (\"Output schema for output view type : \" + outputType); System.out.println (outputTypesSchema.get (outputType)); } ModuleMetadata API Class ModuleMetadata provides APIs to query the Text Analytics module (TAM) about the: - schema of the view `Document` - list of elements \\(views/tables/functions/dictionaries\\) exported by the module - list of views output by the module - schema of the exported or output views - external dictionaries, tables and views declared in the module - list of other modules that this module depends upon The following example illustrates how to load the module metadata from a compiled Text Analytics module (TAM), and later query the loaded module about its metadata: // URI to the location where the compiled modules should be stored String COMPILED_MODULES_PATH = new File (\"textAnalytics/bin\").toURI ().toString (); // Name of the compiled AQL modules String[] TEXTANALYTICS_MODULES = new String[] { \"main\", \"metricsIndicator_dictionaries\", \"metricsIndicator_externalTypes\", \"metricsIndicator_features\", \"metricsIndicator_udfs\" }; // Read metadata for all the modules compiled in Step#1 ModuleMetadata[] modulesMetadata = ModuleMetadataFactory.readMetaData (TEXTANALYTICS_MODULES, COMPILED_MODULES_PATH); // Query metadata of each module to obtain the following: // 1) Exported views and their schemas // 2) External dictionaries and external tables for (int metadataIndex = 0; metadataIndex < modulesMetadata.length; ++metadataIndex) { ModuleMetadata metadata = modulesMetadata[metadataIndex]; System.out.printf (\"\\n Displaying metadata for module named '%s': \", metadata.getModuleName ()); // Obtain list of views exported by this module and their schema String[] exportedViews = metadata.getExportedViews (); for (int exportedviewIndex = 0; exportedviewIndex < exportedViews.length; exportedviewIndex++) { // Fetch ViewMetadata for exported view ViewMetadata exportedViewMetadata = metadata.getViewMetadata (exportedViews[exportedviewIndex]); System.out.printf (\"\\n Exported view name is '%s' and its schema is '%s'.\", exportedViews[exportedviewIndex], exportedViewMetadata.getViewSchema ()); } // Obtain list of external dictionaries and inquire if it is required to pass in entries for the dictionary while // loading String[] externalDictionaries = metadata.getExternalDictionaries (); for (int externalDictIndex = 0; externalDictIndex < externalDictionaries.length; externalDictIndex++) { // Fetch metadata for the external dictionary DictionaryMetadata dictionaryMetadata = metadata.getDictionaryMetadata (externalDictionaries[externalDictIndex]); System.out.printf (\"\\n External dictionary name is '%s' and it is an '%s' dictionary.\", externalDictionaries[externalDictIndex], dictionaryMetadata.isAllowEmpty () ? \"Optional\" : \"Required\"); } // Similarly, you can obtain list of external tables and inquire if it is required to pass in entries for the table // while loading // Obtain list of modules this module depends on List<String> dependentModules = metadata.getDependentModules (); System.out.printf (\"\\n Module named '%s' depends on following modules %s.\", metadata.getModuleName (), dependentModules); } MultiModuleMetadata API While individual modules contain metadata to describe their own schema and artifacts, applications might need metadata about the attributes of combined modules, numerous times, without having to create and load the extractor in memory. The MultiModuleMetadata API provides metadata for extractors formed from a group of modules. The following example illustrates the usage of the MultiModuleMetadata API: /** * Method to illustrate the ability to understand what an extractor represents before having to create it * * Useful when an application consuming text-analytics needs to know extractor specifics ahead of creating extractor * This can also be used by say, a tool that displays extractor specifics for users before they choose within the tool * * @throws TextAnalyticsException * @throws Exception */ public void illustrateMultiModuleMetadata() throws TextAnalyticsException, Exception { MultiModuleMetadata modulesMetadata = ModuleMetadataFactory.readAllMetaData(TEXTANALYTICS_MODULES, COMPILED_MODULES_PATH); System.out.println(\"Input document schema across these modules : \"+ modulesMetadata.getDocSchema()); System.out.println(\"Tokenizer used in creating this extractor : \"+modulesMetadata.getTokenizerType()); String[] outputViews = modulesMetadata.getOutputViews(); // Schema of each output view across these modules for (String outputView : outputViews) { ViewMetadata viewMetadata = modulesMetadata.getViewMetadata(outputView); System.out.println(\"Schema for output view : \"+ outputView + \" is \"+ viewMetadata.getViewSchema()); } // Schema of each exported view across these modules String[] exportedViews = modulesMetadata.getExportedViews(); for(String exportedView : exportedViews) { ViewMetadata viewMetadata = modulesMetadata.getViewMetadata(exportedView); System.out.println(\"Schema for exported view : \"+ exportedView + \" is \" + viewMetadata.getViewSchema()); } // Particulars of any function being exported from within any of the modules String[] exportedFunctions = modulesMetadata.getExportedFunctions(); for (String exportedFunction : exportedFunctions) { // Fetch function metadata FunctionMetadata functionMetadata = modulesMetadata.getFunctionMetadata(exportedFunction); System.out.println(\"Particulars of exported function : \"+exportedFunction); System.out.println(\"Function external name : \"+functionMetadata.getExternalName()); // Function input parameters Param[] functionParams = functionMetadata.getParameters(); int ix = 0; for (Param functionParam : functionParams) System.out.println(\"Function parameter #\"+(++ix)+\"--> Name : \"+functionParam.getName()+\" ; Type : \"+functionParam.getType()); // Function return type System.out.println(\"Function return type : \"+functionMetadata.getReturnType()); } /** * Like above, one could use similar public methods exposed by the MultiModuleMetadata API to obtain information * about dictionaries, tables, comments relevant to the modules being considered. */ }","title":"Query the extractor schema and other metadata"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#the-docreader-api","text":"The SystemT low-level Java API also includes the DocReader API , a convenient Java API for reading input document collections that are stored on the local file system in one of the supported input collection formats. The DocReader API supports the input document collections that use the data collection formats.","title":"The DocReader API"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#annotating-a-document-collection-non-json-non-csv","text":"The following example demonstrates how to annotate document collections in an input format that allows only the default document schema (text Text, [label Text])). Create a DocReader object, and pass in the location of the input document collection as its parameter. Then, use the DocReader.next() function to obtain the next document in a collection. // Directory containing the collection of documents from which to extract information, in one of the supported input collection formats. File INPUT_DOCS_FILE = new File (\"/path/to/input/collection/on/disk\"); // Create the operator graph String modulePath = \"/path/to/modules/on/disk\"; TokenizerConfig tokenizer = new TokenizerConfig.Standard (); OperatorGraph og = OperatorGraph.createOG (new String[] { \"module1\", \"module2\" }, modulePath, null, tokenizer); // Open a reader over input document set DocReader docs = new DocReader (INPUT_DOCS_FILE); while (docs.hasNext ()) { Tuple docTuple = docs.next (); // Execute the operator graph on the current document, generating every single output type that the extractor produces. Map<String, TupleList> results = og.execute (docTuple, null, null); // Process the results as required }","title":"Annotating a document collection (non-JSON, non-CSV)"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#annotating-a-csv-document-collection","text":"Annotating a CSV document collection is similar to annotating a regular document collection, except that a CSV file can support a non-default document schema. When you create the DocReader object for a CSV document collection, use the constructor that specifies a custom document schema, as seen in the following example. Then, use DocReader.next() to traverse the collection. // Directory containing the collection of documents from which to extract information in CSV format File INPUT_DOCS_FILE = new File (\"/path/to/csv/input/collection/on/disk/\"); // Create the operator graph String modulePath = \"/path/to/modules/on/disk\"; TokenizerConfig tokenizer = new TokenizerConfig.Standard (); OperatorGraph og = OperatorGraph.createOG (new String[] { \"module1\", \"module2\" }, modulePath, null, tokenizer); // Use the operator graph to determine the document schema. TupleSchema docSchema = og.getDocumentSchema (); // Open a reader over input document set DocReader docs = new DocReader (INPUT_DOCS_FILE, docSchema, null); while (docs.hasNext ()) { Tuple docTuple = docs.next (); // Execute the operator graph on the current document, generating every single output type that the extractor produces. Map<String, TupleList> results = og.execute (docTuple, null, null); // Process the results as required }","title":"Annotating a CSV document collection"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#annotating-a-json-document-collection","text":"For a JSON document collection, use the static method DocReader.makeDocAndExternalPairsItr() to retrieve an Iterator that returns the document content and any corresponding external view tuples. To use this method, you must build an external view map object that represents all the external views, with corresponding external names and document schemas. // Directory containing the collection of documents from which to extract information in a JSON format. File INPUT_DOCS_FILE = new File (\"/path/to/json/input/collection/on/disk\"); // Create the operator graph String modulePath = \"/path/to/modules/on/disk\"; TokenizerConfig tokenizer = new TokenizerConfig.Standard (); OperatorGraph og = OperatorGraph.createOG (new String[] { \"module1\", \"module2\" }, modulePath, null, tokenizer); Map<Pair<String, String>, TupleSchema> extViewsMap = new HashMap<Pair<String, String>, TupleSchema> (); // Prepare external view map for (String extViewName : og.getExternalViewNames ()) { // Get external name of the external view String extViewExternalName = og.getExternalViewExternalName (extViewName); // Get schema of external view Pair<String, String> extViewNamePair = new Pair<String, String> (extViewName, extViewExternalName); TupleSchema schema = og.getExternalViewSchema (extViewName); extViewsMap.put (extViewNamePair, schema); // iterate over doc tuples with associated external views Iterator<Pair<Tuple, Map<String, TupleList>>> itr = DocReader.makeDocandExternalPairsItr ( INPUT_DOCS_FILE.toURI ().toString (), og.getDocumentSchema (), extViewsMap); while (itr.hasNext ()) { Pair<Tuple, Map<String, TupleList>> docExtViewTup = itr.next (); Tuple docTuple = docExtViewTup.first; Map<String, TupleList> extViewData = docExtViewTup.second; Map<String, TupleList> results = og.execute (docTuple, null, extViewData); // Process the results as required }","title":"Annotating a JSON document collection"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#text-analytics-uri-formats","text":"You can specify the input and output location of your files with Uniform Resource Identifiers (URIs). The following operations are examples of API calls that use URIs: Reading an AQL file for compilation Writing a TAM file Loading modules from a TAM file Loading module metadata Loading external artifact data Compared to other applications, SystemT has a flexible definition of what constitutes a valid URI. You can use a URI that does not contain a file system scheme (schemeless URI), as well as file system schemes of type HDFS. The full HDFS URI (hdfs://namenode.ibm.com:9080/directory/file.tam) has an HDFS scheme and specified authority. The HDFS URI (hdfs:///directory/file.tam) has an HDFS scheme and no specified authority. The local URI (file:///directory/file.tam) has a local file system scheme and no specified authority. The schemeless absolute URI (/directory/file.tam) has a POSIX-compliant absolute file path. The schemeless relative URI (directory/file.tam) has a POSIX-compliant relative file path. For more information about the specifics of the following APIs, see the Javadoc classes and APIs. Java API Supported URI formats CompileAQLParams(...) [constructor] - Local - Schemeless absolute - Schemeless relative CompileAQLParams.setInputModules(...) - Local - Schemeless absolute - Schemeless relative CompileAQLParams.setModulePath(...) - Local - Schemeless absolute - Schemeless relative CompileAQLParams.setOutputURI(...) - Local - Schemeless absolute - Schemeless relative OperatorGraph.createOG(...) - Full HDFS - HDFS - Local - Schemeless absolute - Schemeless relative OperatorGraph.validateOG(...) - Full HDFS - HDFS - Local - Schemeless absolute - Schemeless relative ExternalTypeInfo.addDictionary(...) - Full HDFS - HDFS - Local - Schemeless absolute - Schemeless relative ExternalTypeInfo.addTable(...) - Full HDFS - HDFS - Local - Schemeless absolute - Schemeless relative ModuleMetadataFactory.readMetaData(...) - Full HDFS - HDFS - Local - Schemeless absolute - Schemeless relative ModuleMetadataFactory.readAllMetaData(...) - Local - Schemeless absolute - Schemeless relative Schemeless URIs (formats 5 and 6) are resolved to a schemed URI in a process that is called scheme auto-discovery so that you can reuse a URI in applications without committing to a specific underlying file system scheme: If you are using APIs that do not support a distributed file system (for example, compile APIs), the auto-resolved scheme is always local file system (file://). If the APIs support a distributed file system (for example OperatorGraph.createOG()), the auto-resolved scheme is one of HDFS if installed ( hdfs://), otherwise it is the local file system (file://). To determine whether a DFS is installed, SystemT attempts to read the fs.default.name property from the file core-site.xml in the directory that is specified by the environment variable HADOOP_CONF_DIR. It then sets the scheme to be identical to the scheme of the URI contained in that property. If the read fails for any reason, the scheme is set to local file system. When the scheme is auto-discovered in a relative file path URI (format 6), the URI is resolved to an absolute URI: If the scheme is a distributed file system ( hdfs://), you can assume that the URI is relative to the root of the DFS that is specified. If the scheme is local file system (file://), then you can assume that the URI is relative to the current working directory (usually the directory from which the application was started).","title":"Text Analytics URI formats"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#example-1","text":"In this first example, you can create an operator graph with the module path of application/modules (format 6), using the OperatorGraph.createOG() API. The system checks $HADOOP_CONF_DIR/core-site.xml for the property file fs.default.name, which is set to hdfs://bigserver.widgets.org. The module path is then set to hdfs://bigserver.widgets.org/application/modules.","title":"Example 1"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#example-2","text":"In the same environment, the HADOOP_CONF_DIR environment variable is not set and the directory where SystemT is started is your home directory /home/username. With no distributed file system found, the module path is set to file:///home/username/application/modules.","title":"Example 2"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#using-the-high-level-java-api","text":"","title":"Using the High-level Java API"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#prepare-compiled-text-analytics-modules-tam-files","text":"SystemT's high-level API is designed to compile AQL code and to execute compiled AQL code (TAM modules). For reference, AQL compilation can be completed by using either the high-level API or the low-level API , tams \\ ---demoAqlModule.tam","title":"Prepare Compiled Text Analytics Modules (TAM) files"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#create-configuration-file","text":"SystemT high-level API is designed to integrate application easily, so the input object and output annotation result are defined as Jackson Json node. To construct annotation result, we need to setup annotation core module by using a configuration json file named manifest.json Here is the manifest.json used in this tutorial: { \"annotator\": { \"version\": \"1.0\", \"key\": \"Demo\" }, \"annotatorRuntime\": \"SystemT\", \"version\": \"1.0\", \"acceptedContentTypes\": [ \"text/html\", \"text/plain\" ], \"serializeAnnotatorInfo\": false, \"location\": \"./model\", \"serializeSpan\": \"locationAndText\", \"tokenizer\": \"standard\", \"modulePath\": [ \"tams\" ], \"moduleNames\": [ \"demoAqlModule\" ], \"inputTypes\": null, \"outputTypes\": [ \"ProgrammingLanguageName\", \"ProgrammingLanguageWithVersion\" ], \"externalDictionaries\": null, \"externalTables\": {} } This looks hard to understand at a glance, but not many fields are mandatory to modify for trial. For a complete reference, see Specification of manifest.json . Here's some key fields to use your own custom annotator. Model file path: Set location to the model folder. sh \"location\": \"./model\", Module path: Path to .tam folder path relative to the value in location . sh \"modulePath\": [ \"tams\" ] Module name: Names of AQL modules that you want to execute (coincide with the names of TAM files without .tam extension). sh \"moduleNames\": [ \"demoAqlModule\" ] Output types: Output views that you want to execute (other output views may exist in your TAM files, but they will not be executed unless explicitly included here). sh \"outputTypes\": [ \"ProgrammingLanguageName\", \"ProgrammingLanguageWithVersion\" ] For further details, please refer to Specification of manifest.json Now we have . \u251c\u2500\u2500 model \u2502 \u251c\u2500\u2500 manifest.json \u2502 \u251c\u2500\u2500 tams \u2502 \u2502 \u2514\u2500\u2500 demoAqlModule.tam \u2502 \u2514\u2500\u2500 aql \u2502 \u2514\u2500\u2500 demoAqlModule \u2502 \u2514\u2500\u2500 main.aql","title":"Create Configuration File"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#configuration-for-aql-compilation","text":"Add the following field to specify the location of the AQL files, if you'd like to compile and execute a model including AQL files. Source modules: Path to .aql folder path relative to the value in location . sh \"sourceModules\": [ \"aql/demoAqlModule\" ]","title":"Configuration for AQL Compilation"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#instantiate-and-execute-extractor","text":"The following code takes English text and writes analysis result to console. See API Reference for details of each method. // Annotation service core object (an embeddable library, not a service) AnnotationService as = new AnnotationService(); // Use jackson objectmapper to read manifest.json and dump result ObjectMapper mapper = new ObjectMapper().enable(SerializationFeature.INDENT_OUTPUT); // STEP 1. Load configuration (manifest.json) Path manifestPath = Paths.get(\"model/manifest.json\"); AnnotatorBundleConfig cfg = mapper.readValue (Files.readAllBytes (manifestPath), AnnotatorBundleConfig.class); // STEP 2. Prepare execution parameter // LanguageCode for text String language = \"en\"; ExecuteParams execParams = new ExecuteParams (-1L, language, null, AnnotatorBundleConfig.ContentType.PLAIN.getValue (), null); // Text to process String text = \"I like implementing NLP models in AQL. I can execute AQL from Java 8 and Python 3.7.\"; ObjectNode objNode = mapper.createObjectNode (); objNode.put (\"label\", \"docLabel\"); objNode.put (\"text\", text); // STEP 3. Invoke annotation service core and get result JsonNode outputJson = as.invoke (cfg, objNode, execParams);","title":"Instantiate and Execute Extractor"},{"location":"10-minutes-to-systemt/10-Minutes-to-SystemT-%28Java-API%29/#get-result_1","text":"The analysis result is stored in Jackson JsonNode objects. For the following input text: I like implementing NLP models in AQL. I can execute AQL from Java 8 and Python 3.7. The above code will print to console: { \"annotations\" : { \"ProgrammingLanguageName\" : [ { \"name\" : { \"location\" : { \"begin\" : 34, \"end\" : 37 }, \"text\" : \"AQL\" } }, { \"name\" : { \"location\" : { \"begin\" : 53, \"end\" : 56 }, \"text\" : \"AQL\" } }, { \"name\" : { \"location\" : { \"begin\" : 62, \"end\" : 66 }, \"text\" : \"Java\" } }, { \"name\" : { \"location\" : { \"begin\" : 73, \"end\" : 79 }, \"text\" : \"Python\" } } ], \"ProgrammingLanguageWithVersion\" : [ { \"fullMatch\" : { \"location\" : { \"begin\" : 34, \"end\" : 37 }, \"text\" : \"AQL\" }, \"name\" : { \"location\" : { \"begin\" : 34, \"end\" : 37 }, \"text\" : \"AQL\" }, \"version\" : null }, { \"fullMatch\" : { \"location\" : { \"begin\" : 53, \"end\" : 56 }, \"text\" : \"AQL\" }, \"name\" : { \"location\" : { \"begin\" : 53, \"end\" : 56 }, \"text\" : \"AQL\" }, \"version\" : null }, { \"fullMatch\" : { \"location\" : { \"begin\" : 62, \"end\" : 68 }, \"text\" : \"Java 8\" }, \"name\" : { \"location\" : { \"begin\" : 62, \"end\" : 66 }, \"text\" : \"Java\" }, \"version\" : { \"location\" : { \"begin\" : 67, \"end\" : 68 }, \"text\" : \"8\" } }, { \"fullMatch\" : { \"location\" : { \"begin\" : 73, \"end\" : 83 }, \"text\" : \"Python 3.7\" }, \"name\" : { \"location\" : { \"begin\" : 73, \"end\" : 79 }, \"text\" : \"Python\" }, \"version\" : { \"location\" : { \"begin\" : 80, \"end\" : 83 }, \"text\" : \"3.7\" } } ] }, \"instrumentationInfo\" : { \"annotator\" : { \"version\" : \"1.0\", \"key\" : \"Demo\" }, \"runningTimeMS\" : 4, \"documentSizeChars\" : 92, \"numAnnotationsTotal\" : 8, \"numAnnotationsPerType\" : [ { \"annotationType\" : \"ProgrammingLanguageName\", \"numAnnotations\" : 4 }, { \"annotationType\" : \"ProgrammingLanguageWithVersion\", \"numAnnotations\" : 4 } ], \"interrupted\" : false, \"success\" : true } }","title":"Get Result"}]}